{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet GPT - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used to train a small language model using PyTorch from scratch. The model is inspired by the GPT architecture.\n",
    "\n",
    "\n",
    "#### Hardware\n",
    "- RTX3060 12GB VRAM\n",
    "- AMD Ryzen 7 5800X 8-Core\n",
    "- 32GB RAM\n",
    "- Ubuntu 22.04 LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CACHE_DIR = \"/media/rob/RobsDisk/cache_data_llm\"\n",
    "os.environ['HF_HOME'] = CACHE_DIR\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_DIR, \"datasets\")\n",
    "os.environ['HF_METRICS_CACHE'] = os.path.join(CACHE_DIR, \"metrics\")\n",
    "os.environ['HF_MODULES_CACHE'] = os.path.join(CACHE_DIR, \"modules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import random, math\n",
    "\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "import tiktoken\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Common knowledge datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### English Wikipedia crawled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Wikipedia dataset loaded.\n",
      "dataset size in gb: 18.812774107791483\n",
      "Number of entries: 6407814\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Asherton Independent School District was a public school district based in the community of Asherton, Texas (USA).\n",
      "\n",
      "The community was along the Mexico–United States border, and was southwest of San Antonio.\n",
      "\n",
      "The district existed until 1999, when it was ordered consolidated with Carrizo Springs schools by the Texas Education Agency (TEA) to form the Carrizo Springs Consolidated Independent School District.  The TEA determined that the Asherton ISD had not followed proper procedure in assessing school property taxes, the basis for its closure. TEA officials stated that \"financial instability/insolvency\" was the reason behind the district's dissolution.\n",
      "\n",
      "The Carrizo Springs district moved all students in grades 7-12 into its existing junior high and high schools, but left the Asherton school open to serve grades PreK-6.\n",
      "\n",
      "District enrollment (1988-1999)\n",
      "\n",
      "1988-89 - 419 students\n",
      "1989-90 - 414 students\n",
      "1990-91 - 474 students\n",
      "1991-92 - 444 students\n",
      "1992-93 - 436 students\n",
      "1993-94 - 430 students\n",
      "1994-95 - 401 students\n",
      "1995-96 - 388 students\n",
      "1996-97 - 392 students\n",
      "1997-98 - 365 students\n",
      "1998-99 - 364 students\n",
      "\n",
      "The ethnic composition of Asherton ISD in its final year of operation (1998–99) was 98% Hispanic, 1% White, and 1% African American. Of the 364 students, 330 (90.7%) were considered economically disadvantaged.\n",
      "\n",
      "Student performance\n",
      "Asherton ISD's performance on the Texas Assessment of Academic Skills (TAAS), a state standardized test used from 1991 to 2003, generally met state standards. The elementary and high school campuses consistently received ratings of \"Acceptable\" based on the number of students passing each section of the TAAS test. The district received a separate rating of \"Academically Unacceptable: Special Accreditation Investigation (SAI)\" in its final four years of operation. Unlike an \"Academically Unacceptable\" rating, that was designated for districts with low student test performance, \"Academically Unacceptable: SAI\" ratings were based on problems in governance, finances, testing practice, compliance with federal regulation, and/or administrative management.\n",
      "\n",
      "References\n",
      "\n",
      "Former school districts in Texas\n",
      "School districts in Dimmit County, Texas\n",
      "School districts disestablished in 1999\n",
      "1999 disestablishments in Texas\n"
     ]
    }
   ],
   "source": [
    "# English Wikipedia crawled dataset\n",
    "# path to store the dataset cache: /Volumes/RobertsDisk\n",
    "wiki_en = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split='train', cache_dir=CACHE_DIR) \n",
    "print(\"English Wikipedia dataset loaded.\")\n",
    "print(\"dataset size in gb:\", wiki_en.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(wiki_en))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(wiki_en[random.randint(0, len(wiki_en)-1)]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple stories dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple stories dataset loaded.\n",
      "dataset size in mb: 3030.012650489807\n",
      "Number of entries: 2115696\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Quietly, the ship sailed through the vast emptiness of space. Samuel sat alone, gazing out at the endless dark. The stars were like diamonds, but they brought no comfort. He felt a chill in the air, a reminder of the warmth he had left behind. The hum of the engine filled the silence, but it felt like a lonely song. \n",
      "\n",
      "As he stared out, he caught the smell of old metal mixed with the faint scent of fuel. It made him think of his childhood, of the fresh smell of grass after rain. He longed for the sound of the wind in the trees, for the taste of fruit just picked. Everything felt so far away, hidden among the stars. \n",
      "\n",
      "One night, the ship drew near a glowing nebula. The colors swirled like a painting, vibrant and alive. Samuel's heart leapt. He could almost feel the warmth of the light, like a blanket wrapping around him. He wanted to dive into that beauty, to touch the colors with his hands. \n",
      "\n",
      "But then, the ship's voice spoke again. \"We cannot enter,\" it said softly. Samuel felt his dreams crash down. That colorful nebula was just a wish now, a beautiful thing he could not touch. He looked away, feeling the tears welling up inside. \n",
      "\n",
      "In the darkness of the ship, he remembered his life back on Earth. The sound of laughter, the feel of a hug, the taste of a warm meal shared with family. Each memory was a light in his heart, pushing against the sadness. \n",
      "\n",
      "He realized he had to hold on to those moments. They were the stars in his own sky, guiding him through the dark. Each little thing he missed would help him find his way. \n",
      "\n",
      "Samuel made a promise to himself. He would cherish every memory, keeping them close like treasures. He would not let the cold of space snuff out his spirit. \n",
      "\n",
      "With a deep breath, he looked out at the stars once more. They felt distant, but he knew that somewhere out there, his dreams waited for him. And one day, he would find his way back to the warmth of home.\n"
     ]
    }
   ],
   "source": [
    "# Simple stories dataset\n",
    "stories = load_dataset(\"SimpleStories/SimpleStories\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Simple stories dataset loaded.\")\n",
    "print(\"dataset size in mb:\", stories.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(stories))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(stories[random.randint(0, len(stories)-1)]['story'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FineWeb-Edu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineWeb-Edu is ready.\n",
      "dataset size in gb: 45.730818568728864\n",
      "Number of entries: 9672101\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Over the past 10 years we have been witnessing a significant increase in a number of scientific studies which analyze day-to-day decision-making by using psychological tools and experimental designs. The purpose of this new approach, often called behavioral economics, is to enrich traditional economic conceptions of human decision-making employed by the currently dominant theory of rational choice. This new approach is providing descriptively more fitting accounts of human behavior by showing, for example, that when trying to follow our decisions we, as humans, are constrained by limited will-power.\n",
      "Most of us want to go to the gym more often, stop eating junk food, quit smoking, reduce drinking, and live “better” in general. And so often we fail because our will power is limited, once we focus on one target, we fail even more in others. Those knowing that, try to boost their motivation or limit failing by participating in Christmas clubs or using iPhone apps that will share embarrassing pictures of them with their friends in case they fail to eat healthy. Similar limitations can be observed with respect to our rational decision-making that is “rational” only sporadically. We give higher tips to smiling waitresses with short skirts even in pubs we will never ever visit again (there is really no “economic” reason for that) and we follow our do-not-cheat principles even though no one is watching and we cannot get fined for doing so. And of course, we are generally nice to each other as we are self-interested and in many cases even altruistic.\n",
      "Recently, more and more studies present a variety of decision-making errors and biases under the umbrella of the trio of limited will power, limited self-interest and bounded rationality.\n",
      "Insights obtained from behavioral economics research can be used to design new institutional settings that will help people avoid negative consequences of their systematic errors in decision making, yet maintain the possibility of freedom of choice for those who wish to diverge from the default. This “libertarian paternalistic” approach provides one of the most interesting and practical frameworks for creating incentive structures not only in public policy sphere, but also in management and marketing.\n",
      "The aim of the program is therefore to provide students with a unique opportunity to gain or deepen their knowledge of decision-making and its errors, and behavioral economics implications for policy making and public institutional settings, as well as its practical implications for the realm of organizational management. In order to better understand scientific methods used in research, the participants will design their own experiments under the supervision of the lecturers, conduct them, and present their results to the other participants, who subject them to critical discussions.\n",
      "The knowledge and analytical skills, which the students shall acquire, can be utilized in the areas of policymaking, management, marketing and advertising, consulting, as well as media. Visit ACADEMICS to find out more about the program.\n",
      "The Summer School on Behavioral Economics and Psychology was launched in 2014 and since then brought over 120 outstanding students to Prague. It was organized in cooperation with the Institute for Behavioral and Economic Studies (INBES). The goal of the Institute is to utilize the knowledge of social and organizational psychology, behavioral economics, law and cognitive science when enhancing the functional efficiency of public and private entities.\n",
      "The Prague Summer School on Behavioral Economics and Psychology is driven by our passion to understand the human decision-making better and to share this knowledge with others. Over the past 10 years we have been witnessing a significant increase in a number of scientific studies, which analyze day-to-day decision-making by using psychological tools and experimental designs. Today we know that our decisions are influenced by seemingly irrelevant factors and the general context in which the decisions are taken. We also know that there is no neutral decision architecture, i.e. different contexts are nudging us towards different directions. All this is crucial for us as individuals, companies and last but not least for design of public policies. By launching the summer school we would like to contribute a small bit to the uptake of behavioral science knowledge and its real-word application. We believe this may contribute to better and fairer World.\n",
      "Marek Havrda, Ph.D.\n",
      "Co-founder of Prague Summer Schools\n",
      "“The program was very good: great balance of lecture/work and time to enjoy Prague. I really enjoyed the guest lectures and felt like they added a lot to the course. Thank you for the excellent experience.”\n",
      "“The program was fantastic and lectures were interesting. They made a great job in creating a program that was appropriate for students having different backgrounds. Social activities organized during the free time were excellent. Special thanks to all the great and friendly assistants.”\n",
      "Alumni, Summer School 2015\n",
      "„I really want to thank you for all the work you did this summer; this one week was the shortest, but really unforgettable for me. I met so many amazing people from all over the world. And the program and all the lecturers were so interesting. Thanks to all the staff for being so generous and also for the perfect organization. I’m very grateful for such an amazing experience. Keep going, best wishes to you.“\n",
      "Ekaterine, Georgia, Summer School 2014\n",
      "The classes will be held at the Prague Summer Schools venue (Marianeum, Machova 7, Vinohrady, Prague 2, website). Accommodation will be provided to students in double rooms at the accommodation facilities in the venue or Hotel Ametyst which is located within a walking distance from the venue. Each room is equipped with a shower, WC, satellite TV, Internet connection and telephone. Meals provided by the organizer will include breakfasts served in the hotel and dinners in restaurant located nearby the Summer School venue. As the days are demanding there will be coffee, tea and small snack available free of charge during the breaks between lectures.\n"
     ]
    }
   ],
   "source": [
    "fineweb_edu = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\",  split='train', cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"FineWeb-Edu is ready.\")\n",
    "print(\"dataset size in gb:\", fineweb_edu.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(fineweb_edu))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(fineweb_edu[random.randint(0, len(fineweb_edu)-1)]['text']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenWebText2 dataset loaded.\n",
      "Dataset size in GB: 37.03822539001703\n",
      "Number of entries: 8013769\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "The view inside the Ultra High Vacuum Scanning Thermal Microscope, which was used to measure temperature fluxes at the nanoscale. Image credit: Joseph XuANN ARBOR—When heat travels between two objects that aren’t touching, it flows differently at the smallest scales—distances on the order of the diameter of DNA, or 1/50,000 of a human hair.\n",
      "\n",
      "While researchers have been aware of this for decades, they haven’t understood the process. Heat flow often needs to be prevented or harnessed and the lack of an accurate way to predict it represents a bottleneck in nanotechnology development.\n",
      "\n",
      "Now, in a unique ultra-low vibration lab at the University of Michigan, engineers have measured how heat radiates from one surface to another in a vacuum at distances down to 2 nanometers.\n",
      "\n",
      "While the thermal energy still flows from the warmer place to the colder one, the researchers found it does so 10,000 times faster than it would at the scale of, say, a bonfire and a pair of chilly hands. “Faster” here refers to the speed at which the temperature of one sample changes the temperature of the other—and not the speed at which the heat itself travels. Heat is a form of electromagnetic radiation, so it moves at the speed of light. What’s different at the nanoscale is the efficiency of the process.\n",
      "\n",
      "“We’ve shown, for the first time, the dramatic enhancements of radiative heat fluxes in the extreme near-field,” said Pramod Reddy, associate professor of mechanical engineering and materials science and engineering. “Our experiments and calculations imply that heat flows several orders of magnitude faster in these ultra small gaps.”\n",
      "\n",
      "Reddy and Edgar Meyhofer, a professor of mechanical engineering and biomedical engineering, led the work. A paper on the findings is newly published online in Nature.\n",
      "\n",
      "The findings have applications across nanotechnology. They could advance next-generation information storage such as heat-assisted magnetic recording. They could push forward devices that more directly convert heat into electricity, including heat generated in cars and spacecrafts that is now being wasted. Those are just a few potential uses.\n",
      "\n",
      "The phenomenon the researchers studied is “radiative heat”—the electromagnetic radiation, or light, that all matter above absolute zero emits. It is the emission of the internal energy of matter from movement of particles in matter—movement that only happens above absolute zero.\n",
      "\n",
      "Scientists can explain how this happens at macroscopic distances, dimensions we can readily perceive in the world around us, down to some we can’t see. More than 100 years ago, the German physicist Max Planck wrote the equations that make this possible. His model accurately describes heat transfer across large to relatively small voids, reaching to 10 micrometers at room temperature. But when the gap gets so tight it’s almost not there, the equations break down.\n",
      "\n",
      "In the middle of the last century, the Russian radio physicist Sergei Rytov proposed a new theory called “fluctuational electrodynamics” to describe heat transfer at smaller-than-10-micrometer distances. Since then, research hasn’t always resulted in supporting evidence.\n",
      "\n",
      "“There were experiments in the 1990s or early 2000s that tried to test these ideas further and they found large discrepancies between what theory would predict and what experiments revealed,” Meyhofer said.\n",
      "\n",
      "Because of the sophistication of the U-M lab, the researchers say their findings close the case, and Rytov was right.\n",
      "\n",
      "“Our work, performed in collaboration with colleagues Professor Juan Carlos Cuevas and Professor Francisco García-Vidal at the Universidad Autónoma de Madrid, resolves an important controversy and represents a key contribution to the field of heat transfer,” Reddy said. “These results disprove current dogma in nanoscale heat transfer, which holds that radiative heat transfer in single digit nanometer-sized gaps cannot be explained by existing theory.”\n",
      "\n",
      "The view inside the Ultra High Vacuum Scanning Thermal Microscope, which was used to measure temperature fluxes at the nanoscale. Image credit: Joseph XuThe facility the researchers used is an ultra-low vibration chamber in the G. G. Brown Laboratories, the university’s newly renovated mechanical engineering complex. The chamber—one of several—was custom designed for performing nanoscale experiments so precise that mere footsteps could disturb them if they were done somewhere else. The rooms can withstand vibration from outside, such as traffic, and inside, such as heating and cooling systems. They also limit acoustic noise, temperature and humidity variations, as well as radio frequency and magnetic interference.\n",
      "\n",
      "“Our facility represents the true state of the art,” Meyhofer said. “When creating nanoscale gaps such as those required for our nanoscale heat radiation experiments, the slightest perturbation can ruin an experiment.”\n",
      "\n",
      "In the chamber, the researchers used custom-built “scanning thermal microscopy probes” that allowed them to directly study how fast heat flows between two surfaces of silica, silicon nitride and gold. The researchers chose these materials because they’re commonly used in nanotechnology.\n",
      "\n",
      "For each material, they designated one sample that would be heated to 305 Fahrenheit, and they coated the tip of the probe with the same material, but kept it at a cooler 98 degrees. They slowly moved the sample and the probe together, beginning at 50 nanometers until they were touching, and they measured the temperature of the tip at regular intervals.\n",
      "\n",
      "The cause of the rapid heat transfer, the researchers discovered, is that in nanoscale gaps there can be an overlap of the two sides’ surface and evanescent waves, both of which carry heat.\n",
      "\n",
      "“These waves reach only a small distance into the gap between materials,” said Bai Song, a graduate student in mechanical engineering and one of the lead authors. “And their intensity at the extreme near-field is enormous compared to the electromagnetic waves at larger distances. When these waves from two different devices overlap, that’s when they allow tremendous heat flux.”\n",
      "\n",
      "The paper is titled “Radiative heat transfer in the extreme near field.” It also involved collaborators from Universidad Autónoma de Madrid, Massachusetts Institute of Technology and Donostia International Physics Center. The work was funded by the U.S. Department of Energy Basic Energy Sciences, Army Research Office, National Science Foundation, Spanish Ministry of Economy and Competitiveness, and other organizations.\n",
      "\n",
      "More information:\n"
     ]
    }
   ],
   "source": [
    "# OpenWebText2 dataset\n",
    "owt2 = load_dataset(\"Skylion007/openwebtext\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "print(\"OpenWebText2 dataset loaded.\")\n",
    "print(\"Dataset size in GB:\", owt2.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(owt2))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(owt2[random.randint(0, len(owt2)-1)]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Q&A data to improve the model's ability to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A dataset loaded.\n",
      "dataset size in mb: 46.480509757995605\n",
      "Number of entries: 120959\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "How did the emergence of the Entente Canada-Québec (ECQ) program contribute to the improvement of education systems in Quebec? \n",
      " The ECQ program fostered collaboration and knowledge sharing between Francophone and Anglophone colleges in Quebec, focusing on minority and second language education. This program enabled both linguistic groups to explore innovative approaches to teaching and learning, thereby enhancing outcomes.\n"
     ]
    }
   ],
   "source": [
    "q_a1 = load_dataset(\"agentlans/text-sft-questions-answers-only\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Q&A dataset loaded.\")\n",
    "print(\"dataset size in mb:\", q_a1.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(q_a1))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(q_a1)-1)\n",
    "print(q_a1[index]['question'][:500], \"\\n\", q_a1[index]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit Instruct dataset loaded.\n",
      "dataset size in gb: 0.09901080373674631\n",
      "Number of entries: 84784\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "ELI5: What causes the color fluctuation in older (90’s) videos? (Link inside) So I was browsing YouTube and came across an older video and noticed the color fluctuating a lot. \n",
      "\n",
      "What causes this? [Link](https://youtu.be/VpLQlUa2G0s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " '\\n',\n",
       " 'Image is stored in VHS tapes not as a bunch of red green and blue pixels, but as analog luminance and chrominance. By now you must have read somewhere about how there are other ways to represent color. Chrominance would be Hue and Saturation, luminance would be the Value or Brightness in a HSV system.\\n\\nSince everything in a VHS tape is analog, the VHS player needs analog circuitry to amplify the signals. Amplifiers such as that have a parameter called gain, which refers to by how much they incre')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#euclaise/reddit-instruct\n",
    "reddit_instruct = load_dataset(\"euclaise/reddit-instruct\", split='train', cache_dir=CACHE_DIR)\n",
    "# reddit_instruct = load_dataset(\"Felladrin/ChatML-reddit-instruct-curated\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Reddit Instruct dataset loaded.\")\n",
    "print(\"dataset size in gb:\", reddit_instruct.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(reddit_instruct))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(reddit_instruct)-1)\n",
    "print(reddit_instruct[index]['post_title'][:500], reddit_instruct[index]['post_text'][:500]), \"\\n\", reddit_instruct[index]['comment_text'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca dataset loaded.\n",
      "dataset size in mb: 44.06797695159912\n",
      "Number of entries: 52002\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Name at least 3 European countries. \n",
      " Germany, France, and the United Kingdom.\n"
     ]
    }
   ],
   "source": [
    "# tatsu-lab/alpaca ( for Q&A fine-tuning )\n",
    "alpaca = load_dataset(\"tatsu-lab/alpaca\", split='train')\n",
    "print(\"Alpaca dataset loaded.\")\n",
    "print(\"dataset size in mb:\", alpaca.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(alpaca))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(alpaca)-1)\n",
    "print(alpaca[index]['instruction'][:500], \"\\n\", alpaca[index]['output'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project i use tiktoken for the tokenizer, as it is the same tokenizer used by OpenAI for their models.\n",
    "\n",
    "I use the \"gpt2\" encoding which is a byte pair encoding (BPE) tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"rob-tokenizer\",\n",
    "    pat_str=tokenizer_base._pat_str,\n",
    "    mergeable_ranks=tokenizer_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **tokenizer_base._special_tokens,\n",
    "        \"<|im_start|>\": 50257,\n",
    "        \"<|im_end|>\": 50258,\n",
    "        \"<|pad|>\": 50259,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the byte pair encoding tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2061, 318, 24207, 1616, 2587, 30, 314, 2342, 257, 7684, 286, 1097, 5861, 290, 484, 1561, 546, 275, 32512, 7021, 290, 884, 11, 1312, 373, 11263, 644, 275, 32512, 318, 290, 1312, 18548, 1064, 597, 2562, 7468, 284, 644, 340, 318, 24207, 1616, 318, 655, 262, 1438, 329, 257, 16058, 286, 6147, 13, 554, 262, 29393, 995, 340, 338, 1690, 973, 355, 257, 1790, 1021, 329, 3354, 326, 547, 3235, 1389, 503, 286, 257, 1263, 2512, 286, 2587, 11, 355, 6886, 284, 11721, 3350, 654, 810, 44030, 6147, 318, 19036, 656, 257, 15936, 12070, 503, 286, 9629, 6147, 13, 7080, 3191, 318, 517, 5789, 329, 1588, 17794, 475, 340, 460, 779, 1365, 3081, 286, 21782, 290, 318, 4577, 284, 787, 329, 4833, 17794, 588, 3234, 3354, 13]\n",
      "Decoded text:\n",
      "What is Billet material? I watch a bunch of car videos and they talk about billet blocks and such, i was wondering what billet is and i cant find any easy explanation to what it is Billet is just the name for a chunk of metal. In the automotive world it's often used as a short hand for parts that were machined out of a big block of material, as opposed to cheaper castings where molten metal is poured into a mold pressed out of sheet metal. Machining is more expensive for large quantities but it can use better quality of metals and is easier to make for smaller quantities like race parts.\n",
      "Sample text length in characters: 594\n",
      "Sample text length in tokens: 127\n"
     ]
    }
   ],
   "source": [
    "# test of tokenizer on reddit_instruct\n",
    "sample_text = reddit_instruct[0]['post_title'] + \" \" + reddit_instruct[0]['post_text'] + \" \" + reddit_instruct[0]['comment_text']\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(tokens)\n",
    "print(\"Decoded text:\")\n",
    "print(tokenizer.decode(tokens)) \n",
    "print(f\"Sample text length in characters: {len(sample_text)}\")\n",
    "print(f\"Sample text length in tokens: {len(tokens)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting datasets functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 26209380\n",
      "Finetune dataset size: 257745\n",
      "Example entry from train dataset:\n",
      "{'id': '<urn:uuid:c543181b-bab4-40b9-b0ae-027eebc9705f>', 'url': 'https://codedocs.org/what-is/sqlite', 'title': None, 'text': '|Developer(s)||D. Richard Hipp|\\n|Initial release||17 August 2000|\\n|Stable release||3.35.5 (19 April 2021 )|\\n.sqlite3, .sqlite, .db\\n|Internet media type|\\n|Open format?||yes (Public Domain)|\\nSQLite (//,//) is a relational database management system (RDBMS) contained in a C library. In contrast to many other database management systems, SQLite is not a client–server database engine. Rather, it is embedded into the end program.\\nSQLite generally follows PostgreSQL syntax. SQLite uses a dynamically and weakly typed SQL syntax that does not guarantee the domain integrity. This means that one can, for example, insert a string into a column defined as an integer. SQLite will attempt to convert data between formats where appropriate, the string \"123\" into an integer in this case, but does not guarantee such conversions and will store the data as-is if such a conversion is not possible.\\nSQLite is a popular choice as embedded database software for local/client storage in application software such as web browsers. It is arguably the most widely deployed database engine, as it is used today by several widespread browsers, operating systems, and embedded systems (such as mobile phones), among others. SQLite has bindings to many programming languages.\\nUnlike client–server database management systems, the SQLite engine has no standalone processes with which the application program communicates. Instead, the SQLite library is linked in and thus becomes an integral part of the application program. Linking may be static or dynamic. The application program uses SQLite\\'s functionality through simple function calls, which reduce latency in database access: function calls within a single process are more efficient than inter-process communication.\\nSQLite stores the entire database (definitions, tables, indices, and the data itself) as a single cross-platform file on a host machine. It implements this simple design by locking the entire database file during writing. SQLite read operations can be multitasked, though writes can only be performed sequentially.\\nDue to the server-less design, SQLite applications require less configuration than client–server databases. SQLite is called zero-conf because it does not require service management (such as startup scripts) or access control based on GRANT and passwords. Access control is handled by means of file-system permissions given to the database file itself. Databases in client–server systems use file-system permissions that give access to the database files only to the daemon process.\\nAnother implication of the serverless design is that several processes may not be able to write to the database file. In server-based databases, several writers will all connect to the same daemon, which is able to handle its locks internally. SQLite, on the other hand, has to rely on file-system locks. It has less knowledge of the other processes that are accessing the database at the same time. Therefore, SQLite is not the preferred choice for write-intensive deployments. However, for simple queries with little concurrency, SQLite performance profits from avoiding the overhead of passing its data to another process.\\nSQLite uses PostgreSQL as a reference platform. \"What would PostgreSQL do\" is used to make sense of the SQL standard. One major deviation is that, with the exception of primary keys, SQLite does not enforce type checking; the type of a value is dynamic and not strictly constrained by the schema (although the schema will trigger a conversion when storing, if such a conversion is potentially reversible). SQLite strives to follow Postel\\'s rule.\\nD. Richard Hipp designed SQLite in the spring of 2000 while working for General Dynamics on contract with the United States Navy. Hipp was designing software used for a damage-control system aboard guided-missile destroyers, which originally used HP-UX with an IBM Informix database back-end. SQLite began as a Tcl extension.\\nThe design goals of SQLite were to allow the program to be operated without installing a database management system or requiring a database administrator. Hipp based the syntax and semantics on those of PostgreSQL 6.5. In August 2000, version 1.0 of SQLite was released, with storage based on gdbm (GNU Database Manager). SQLite 2.0 replaced gdbm with a custom B-tree implementation, adding transaction capability. SQLite 3.0, partially funded by America Online, added internationalization, manifest typing, and other major improvements.\\nSQLite implements most of the SQL-92 standard for SQL, but lacks some features. For example, it only partially provides triggers and cannot write to views (however, it provides INSTEAD OF triggers that provide this functionality). While it provides complex queries, it still has limited ALTER TABLE function, as it cannot modify or delete columns. This changed in version 3.25.0 with support for ALTER TABLE RENAME COLUMN and version 3.35.0 with ALTER TABLE DROP COLUMN.\\nSQLite uses an unusual type system for a SQL-compatible DBMS: instead of assigning a type to a column as in most SQL database systems, types are assigned to individual values; in language terms it is dynamically typed. Moreover, it is weakly typed in some of the same ways that Perl is: one can insert a string into an integer column (although SQLite will try to convert the string to an integer first, if the column\\'s preferred type is integer). This adds flexibility to columns, especially when bound to a dynamically typed scripting language. However, the technique is not portable to other SQL products. A common criticism is that SQLite\\'s type system lacks the data integrity mechanism provided by statically typed columns in other products. The SQLite web site describes a \"strict affinity\" mode, but this feature has not yet been added. However, it can be implemented with constraints like\\nTables normally include a hidden rowid index column, which gives faster access. If a database includes an Integer Primary Key column, SQLite will typically optimize it by treating it as an alias for rowid, causing the contents to be stored as a strictly typed 64-bit signed integer and changing its behavior to be somewhat like an auto-incrementing column. Future[when?] versions of SQLite may include a command to introspect whether a column has behavior like that of rowid to differentiate these columns from weakly typed, non-autoincrementing Integer Primary Keys.[failed verification]\\nSeveral computer processes or threads may access the same database concurrently. Several read accesses can be satisfied in parallel. A write access can only be satisfied if no other accesses are currently being serviced. Otherwise, the write access fails with an error code (or can automatically be retried until a configurable timeout expires). This concurrent access situation would change when dealing with temporary tables. This restriction is relaxed in version 3.7 when write-ahead logging (WAL) is turned on, enabling concurrent reads and writes.\\nSQLite version 3.7.4 first saw the addition of the FTS4 (full-text search) module, which features enhancements over the older FTS3 module. FTS4 allows users to perform full-text searches on documents similar to how search engines search webpages. Version 3.8.2 added support for creating tables without rowid, which may provide space and performance improvements.Common table expressions support was added to SQLite in version 3.8.3.\\nAs per version 3.33.0, the maximum supported database size is 281 TB.\\nSQLite\\'s code is hosted with Fossil, a distributed version control system that is itself built upon an SQLite database.\\nA standalone command-line program is provided in SQLite\\'s distribution. It can be used to create a database, define tables, insert and change rows, run queries and manage an SQLite database file. It also serves as an example for writing applications that use the SQLite library.\\nSQLite uses automated regression testing prior to each release. Over 2 million tests are run as part of a release\\'s verification. Starting with the August 10, 2009 release of SQLite 3.6.17, SQLite releases have 100% branch test coverage, one of the components of code coverage. The tests and test harnesses are partially public-domain and partially proprietary.\\nSQLite is included by default in:\\nLanguage bindings to SQLite for a large number of programming languages exist, including:\\nHow do I pronounce the name of the product? I say S-Q-L-ite, like a mineral.\\n[...] ess-kju-ellite [...]\\n[...] sequelite [...]\\nSearching for a record with a specific rowid, or for all records with rowids within a specified range is around twice as fast as a similar search made by specifying any other PRIMARY KEY or indexed value.\\nAdd the \"PRAGMA table_ipk(TABLE)\" command for evaluation purposes.\\nWAL provides more concurrency as readers do not block writers and a writer does not block readers. Reading and writing can proceed concurrently.\\nSometimes you\\'re forced to use a database brand that doesn\\'t support foreign key constraints (for example MySQL\\'s MyISAM storage engine or SQLite prior to version 3.6.19).\\nEdited: 2021-06-18 14:10:29', 'story': None, 'topic': None, 'theme': None, 'style': None, 'feature': None, 'grammar': None, 'persona': None, 'initial_word_type': None, 'initial_letter': None, 'word_count': None, 'character_count': None, 'num_paragraphs': None, 'avg_word_length': None, 'avg_sentence_length': None, 'flesch_reading_ease': None, 'flesch_kincaid_grade': None, 'dale_chall_readability_score': None, 'num_stories_in_completion': None, 'expected_num_stories_in_completion': None, 'generation_id': None, 'model': None, 'dump': 'CC-MAIN-2022-21', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2022-21/segments/1652662531352.50/warc/CC-MAIN-20220520030533-20220520060533-00777.warc.gz', 'language': 'en', 'language_score': 0.8899863362312317, 'token_count': 2115, 'score': 3.515625, 'int_score': 4}\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_train_dataset = concatenate_datasets([  \n",
    "    wiki_en,\n",
    "    stories,\n",
    "    fineweb_edu,\n",
    "    owt2,  \n",
    "])  \n",
    "\n",
    "combined_finetune_dataset = concatenate_datasets([\n",
    "    q_a1,\n",
    "    reddit_instruct,\n",
    "    alpaca,\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_dataset = combined_train_dataset.shuffle(seed=42)\n",
    "finetune_dataset = combined_finetune_dataset.shuffle(seed=42)\n",
    "print(f\"Train dataset size: {len(combined_train_dataset)}\")\n",
    "print(f\"Finetune dataset size: {len(combined_finetune_dataset)}\")\n",
    "\n",
    "# Exemple \n",
    "\n",
    "print(\"Example entry from train dataset:\")\n",
    "index = random.randint(0, len(train_dataset)-1)\n",
    "print(train_dataset[index])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the dataloader from the \"LLMs from scratch\" repository. But adapted for multi-row text arrays.\n",
    "\n",
    "https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/dataloader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Dataset of the combined hugginface entries\n",
    "            tokenizer: the initiatokenizer to process text\n",
    "            max_length: Context window size\n",
    "        \"\"\"\n",
    "        self.data = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.input_tokens = []\n",
    "        self.target_tokens = []\n",
    "\n",
    "        self.pad_token_id = 50259         # <|pad|>\n",
    "        self.bos_token_id = 50257    # <|im_start|>\n",
    "        self.eos_token_id = 50258    # <|im_end|>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Raw text\n",
    "        \n",
    "        # Data format handling\n",
    "        entry = self.data[idx]\n",
    "        if 'text' in entry:\n",
    "            text = entry['text']\n",
    "        elif 'story' in entry:\n",
    "            text = entry['story']\n",
    "        elif 'question' in entry and 'answer' in entry: \n",
    "            text = \"User: \" + entry['question'] + \" Assistant:\" + entry['answer']\n",
    "        elif 'post_title' in entry and 'post_text' in entry and 'comment_text' in entry:\n",
    "            text = \"User: \" + entry['post_title'] + \" Assistant:\" + entry['post_text'] + \" \" + entry['comment_text']\n",
    "        elif 'instruction' in entry and 'output' in entry:\n",
    "            text = \"User: \" + entry['instruction'] + \" Assistant:\" + entry['output']\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data entry format\")\n",
    "        \n",
    "        text = str(text) # Ensure text is a string\n",
    "        #print(text)\n",
    "\n",
    "        # Adding Start and End tokens\n",
    "        text = \"<|im_start|>\" + text + \"<|im_end|>\" \n",
    "\n",
    "        # Tokenization\n",
    "        tokens = self.tokenizer.encode(text, allowed_special=\"all\")\n",
    "\n",
    "        # Truncation\n",
    "        tokens = tokens[:self.max_length] #Data is loost here ( fix later with sliding window )\n",
    "\n",
    "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)  # All tokens except last\n",
    "        labels = torch.tensor(tokens[1:], dtype=torch.long)      # All tokens except first\n",
    "\n",
    "\n",
    "        #Padding \n",
    "        padding_length = self.max_length - len(tokens)\n",
    "        if padding_length > 0:\n",
    "            input_ids = torch.cat([input_ids, torch.full((padding_length,), self.pad_token_id)])\n",
    "            labels = torch.cat([labels, torch.full((padding_length,), -100)])\n",
    "\n",
    "\n",
    "        attention_mask = (input_ids != self.pad_token_id).long() # 1 for real tokens, 0 for padding\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the configuration for the GPT model i am going to train. It is a smaller version of the GPT-2 model. \n",
    "\n",
    "- Context length: 512 tokens\n",
    "- Embedding dimension: 512\n",
    "- Number of attention heads: 8\n",
    "- Number of layers: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50260,\n",
    "    \"context_length\": 512, # max i could fit on my gpu\n",
    "    \"emb_dim\": 512,\n",
    "    \"number_heads\": 8,\n",
    "    \"number_layers\": 8,\n",
    "    \"drop_rate\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test of a entry from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 26209380\n"
     ]
    }
   ],
   "source": [
    "# Empty cuda cache and memory management\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_dataset = GPTDataset(combined_train_dataset, tokenizer, max_length=GPT_CONFIG[\"context_length\"])\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "\n",
    "batch_size = 12\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=2, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Test of a entry from dataloader\n",
      "{'input_ids': tensor([[50257, 18683, 11064,  ...,  2077,  1337,   286],\n",
      "        [50257,  5956,  1285,  ...,   286,  2346,   960],\n",
      "        [50257, 24857, 15428,  ..., 11004,  7401,   329],\n",
      "        ...,\n",
      "        [50257,   464,  1584,  ...,  1232,    13, 24690],\n",
      "        [50257,  2061,   921,  ..., 50259, 50259, 50259],\n",
      "        [50257,  7841,  9057,  ...,   198,   198,   464]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[18683, 11064,  1012,  ...,  1337,   286,   351],\n",
      "        [ 5956,  1285,    11,  ...,  2346,   960,   392],\n",
      "        [24857, 15428,  2626,  ...,  7401,   329,  6079],\n",
      "        ...,\n",
      "        [  464,  1584,  1906,  ...,    13, 24690,   422],\n",
      "        [ 2061,   921, 10664,  ...,  -100,  -100,  -100],\n",
      "        [ 7841,  9057,   308,  ...,   198,   464, 21880]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Test of a entry from dataloader\")\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytroch model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first implementation, i am using the transformer and embedding modules from PyTorch. Later i will try to implement the attention mechanism from scratch for better understanding.\n",
    "\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Gpt model class using transformer library\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Network components \n",
    "        ## Embedding layers\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.positional_encoding = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        ## Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['emb_dim'],\n",
    "            nhead=config['number_heads'],\n",
    "            dim_feedforward=4 * config['emb_dim'],\n",
    "            dropout=config['drop_rate'],\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True # stabilityy \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config['number_layers'])\n",
    "        ## Output layer\n",
    "        self.output_layer = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label_ids=None):   \n",
    "        batch_size, seq_length = input_ids.shape\n",
    "\n",
    "        # Embedding\n",
    "        token_embeddings = self.embedding(input_ids)  \n",
    "        pos_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        position_embeddings = self.positional_encoding(pos_ids)  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        embeddings = token_embeddings + position_embeddings  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        # Prevent attending to future tokens\n",
    "        causal_mask = torch.triu(torch.full((seq_length, seq_length), float('-inf'), device=input_ids.device), diagonal=1)\n",
    "\n",
    "        # voiding to pay attatention padding tokens\n",
    "        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        x = self.transformer(embeddings, mask=causal_mask, src_key_padding_mask=key_padding_mask, is_causal=True)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        # Computing loss \n",
    "        if label_ids is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(logits.view(-1, self.config['vocab_size']), label_ids.view(-1)) # Applies loss to predictions\n",
    "            return logits, loss\n",
    "\n",
    "        return logits, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model instantiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (embedding): Embedding(50260, 512)\n",
      "  (positional_encoding): Embedding(512, 512)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=512, out_features=50260, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18267/1232015742.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Gradient scaler for mixed precision\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTModel(GPT_CONFIG).to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3) # AdamW optimizer\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10) # Cosine annealing learning rate scheduler\n",
    "scaler = torch.cuda.amp.GradScaler()  # Gradient scaler for mixed precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, prompt, max_length=256, device='cpu'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_length)\n",
    "\n",
    "    generated_ids = input_ids\n",
    "    max_length = max_length - input_ids.shape[1]  # Remaining length for generation\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            attention_mask = torch.ones_like(generated_ids)  # All tokens are real (no padding)\n",
    "            logits, _ = model(generated_ids, attention_mask)\n",
    "            next_token_logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # (1, 1)\n",
    "\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)  # Append to sequence\n",
    "\n",
    "            if next_token_id.item() == 50258:  # Stop if eot token is generated\n",
    "                break\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_ids.squeeze().tolist())\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps= 2  # Number of steps to accumulate gradients\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, scheduler, device, num_epochs=3, accumulation_steps = 4, question_interval=500):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        step_count = 0\n",
    "\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):  # bf16 optimized for ampere archi\n",
    "                logits, loss = model(input_ids, attention_mask, labels)\n",
    "                loss = loss / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()*accumulation_steps\n",
    "            progress_bar.set_postfix(loss=loss.item() * accumulation_steps)\n",
    "\n",
    "            # Inference check\n",
    "            step_count += 1\n",
    "            if step_count % question_interval == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    prompt = \"You are an AI being trained. How are you doing?\"\n",
    "                    generated_text = inference(model, tokenizer, prompt, max_length=50, device=device)\n",
    "                    print(f\"\\n[Inference at step {step_count}]\")\n",
    "                    #save generated text to file train_ouput.txt\n",
    "                    with open(\"train_output.txt\", \"a\") as f:\n",
    "                        if generated_text.strip() != \"\":\n",
    "                            f.write(f\"\\n[Inference at step {step_count}]: {generated_text}\\n\")  \n",
    "                            f.write(\"-\"*50 + \"\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"\\n[Inference at step {step_count}]: [No output generated]\\n\")\n",
    "                            f.write(\"-\"*50 + \"\\n\")\n",
    "                model.train()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First training on combined train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 500/2184115 [01:43<144:48:06,  4.19it/s, loss=18.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 1000/2184115 [03:27<142:25:13,  4.26it/s, loss=10.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 1500/2184115 [05:10<142:45:47,  4.25it/s, loss=16.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 1500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 2000/2184115 [06:54<144:04:37,  4.21it/s, loss=7.99]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 2000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 2501/2184115 [08:36<127:34:31,  4.75it/s, loss=7.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 2500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 3001/2184115 [10:12<127:22:04,  4.76it/s, loss=7.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 3000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 3501/2184115 [11:48<127:28:13,  4.75it/s, loss=7.44]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 3500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 4001/2184115 [13:23<127:43:00,  4.74it/s, loss=7.56]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 4000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 4501/2184115 [14:59<127:29:02,  4.75it/s, loss=7.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 4500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 5001/2184115 [16:36<135:30:41,  4.47it/s, loss=7.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 5501/2184115 [18:18<135:10:24,  4.48it/s, loss=7.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 5500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 6000/2184115 [20:00<140:38:47,  4.30it/s, loss=7.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 6000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 6501/2184115 [21:38<134:37:24,  4.49it/s, loss=7.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 6500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 7000/2184115 [23:17<144:23:53,  4.19it/s, loss=7.38]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 7000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 7501/2184115 [24:59<134:29:07,  4.50it/s, loss=7.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 7500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 7833/2184115 [26:05<123:31:57,  4.89it/s, loss=6.98]"
     ]
    }
   ],
   "source": [
    "#empty gpu memory \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    " \n",
    "\n",
    "train_loop(model, train_dataloader, optimizer, scheduler, device, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m torch.cuda.synchronize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/cuda/memory.py:170\u001b[39m, in \u001b[36mempty_cache\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[33;03m`nvidia-smi`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m \u001b[33;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources \n",
    "\n",
    "### Principal references: \n",
    "- https://arxiv.org/abs/2005.14165 (GPT-3 paper)\n",
    "- https://arxiv.org/abs/2002.05709 (Attention is all you need paper)\n",
    "- Build a Large Language Model (from scratch) by Sebastian Raschka\n",
    "\n",
    "### About Padding tokens in Language Modeling\n",
    "- https://arxiv.org/html/2510.01238v1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
