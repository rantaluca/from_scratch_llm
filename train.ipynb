{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet GPT - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used to train a small language model using PyTorch from scratch. The model is inspired by the GPT architecture.\n",
    "\n",
    "\n",
    "#### Hardware\n",
    "- RTX3060 12GB VRAM\n",
    "- AMD Ryzen 7 5800X 8-Core\n",
    "- 32GB RAM\n",
    "- Ubuntu 22.04 LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CACHE_DIR = \"/media/rob/RobsDisk/cache_data_llm\"\n",
    "os.environ['HF_HOME'] = CACHE_DIR\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_DIR, \"datasets\")\n",
    "os.environ['HF_METRICS_CACHE'] = os.path.join(CACHE_DIR, \"metrics\")\n",
    "os.environ['HF_MODULES_CACHE'] = os.path.join(CACHE_DIR, \"modules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/micromamba/envs/mlenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import random, math\n",
    "\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Common knowledge datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### English Wikipedia crawled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Wikipedia dataset loaded.\n",
      "dataset size in gb: 18.812774107791483\n",
      "Number of entries: 6407814\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Nickel Hoffmann (also known as Nikolaus Hofmann, 1536 – 1592) was a German stonemason, sculptor, craftsman, and builder of the Great Masters in the later part of the Gothic Art movement and the Renaissance in central Germany.\n",
      "\n",
      "Life and work\n",
      "Hoffmann was at Mansfeld involved in the mining there and had, at a young age, already reached relative prosperity. The Saxon towns of Torgau, Pirna and Halle were his favorite towns. In Pirna and Halle, he was also recognized as a citizen. He is especially known for his church and town hall buildings.\n",
      "\n",
      "Work in Torgau\n",
      "In 1536, he produced sculptural ornamental work for Wing C of Hartenfels Castle for Torgau. In 1543 and 1544 he worked again on Hartenfels Castle, but this time on wing B.\n",
      "\n",
      "Work in Pirna\n",
      "Hoffmann had his masterpiece in Pirna in, at the latest, 1539 - 1540. In 1540, he became a citizen in Pirna, where he also received orders from the Council. In 1555, during the period in which he mainly worked in Halle, he was a co-signatory of the order of masons of Pirna.\n",
      "\n",
      "Work in Halle\n",
      "From 1542 to 1543, he worked at the Marktkirche Unser Lieben Frauen in Halle. Whether Hoffmann already had operational planning for the work in Torgau has not been established. It is thought that he, with Jakob Hans from the now Czech city of Chomutov as polishing, took over construction management in 1545. In addition, he ran well on his own, with a sculpture in the woman cemetery and delivered a considerable amount of goods for the construction of the church. The church was completed under his direction until 1554th It was equipped with a vault, a gallery was built and the towers were increased. Hoffmann is often known as a foreman for this building.\n",
      "\n",
      "During the construction period in 1550, Hoffman became a citizen of Halle. As in Pirna, he also took orders the Council of the city. Among other things, in 1557, he built the system of Stadtgottesacker and in 1580 the Wagegebäude am Markt. Between 1554 and 1557, he completed the curvature of the Moritzkirche. In 1568, he was responsible for expanding the town hall tower.\n",
      "\n",
      "Other works\n",
      "Hoffmann built the Südpartie City Hall in Merseburg in 1561 and, 1563 from 1566, the town hall in Hof in Upper Franconia. In 1568, he briefly was the oldest foreman at the palace of Augustus Castle under Hieronymus Lotter. In the years 1563-1569 he was able to switch from Paul Widemann the contract for the construction of the choir of St. Mary at Zwickau secure along with his brother Philipp Hoffmann.\n",
      "\n",
      "This was followed by the construction of the town hall in Schweinfurt between 1569 and 1572. This was his main work on the room grouping, façade and detail processing and deemed a feat of German Renaissance art. In 1570 he refereed in Rothenburg with their Town Hall building.\n",
      "\n",
      "Legacy\n",
      "In Hof, there is a street named after Hoffman, Nikolaus Hoffman Street; Nickel Hoffmann street is another street in Halle.\n",
      "\n",
      "External links\n",
      " \n",
      " Spurensuche Nickel Hoffmann Ein Baumeister der ‚Deutschen Renaissance‘ (~ 1515 - 1592) Dissertationsschrift Uni. Marburg (PDF-Data; 4,40 MB)\n",
      "\n",
      "1536 births\n",
      "1592 deaths\n",
      "16th-century German architects\n",
      "Renaissance architects\n",
      "Gothic architects\n",
      "People from Mansfeld-Südharz\n"
     ]
    }
   ],
   "source": [
    "# English Wikipedia crawled dataset\n",
    "# path to store the dataset cache: /Volumes/RobertsDisk\n",
    "wiki_en = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split='train', cache_dir=CACHE_DIR) \n",
    "print(\"English Wikipedia dataset loaded.\")\n",
    "print(\"dataset size in gb:\", wiki_en.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(wiki_en))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(wiki_en[random.randint(0, len(wiki_en)-1)]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple stories dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple stories dataset loaded.\n",
      "dataset size in mb: 3030.012650489807\n",
      "Number of entries: 2115696\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Awash in bright colors, Lily watched the parade pass by. It was a day of celebration, full of joy and excitement. But for Lily, there was only sorrow. Just two weeks ago, she had been with her best friend, planning this very day. Now, she felt the absence like a heavy weight on her chest.\n",
      "\n",
      "Earlier that month, they had danced and laughed, dreaming of the parade. They had spoken of all the fun they would have, but everything changed in a blink. A sudden accident took her friend away, leaving Lily with shattered dreams and empty plans. The power of life can be so fragile, she thought.\n",
      "\n",
      "As the floats moved past her, Lily felt lost in a sea of people. Everyone was celebrating, but she stood still. Memories flashed in her mind, reminding her of happier times. She missed the way her friend would laugh at silly things, how they would share secrets. The joy around her felt like a cruel reminder of what she had lost.\n",
      "\n",
      "Days turned into nights, and the parade day arrived. It was hard to smile, but Lily put on a brave face. She wore the colors they had picked together, trying to feel her friend's spirit beside her. But the fun felt forced, and the laughter felt empty. The power of grief weighed heavily on her heart.\n",
      "\n",
      "In that moment, Lily understood that it was okay to feel sadness amid joy. Life was not always bright and easy. She would carry her friend's memory with her, allowing both sorrow and joy to coexist. As the parade moved on, she lifted her head high, knowing that love endures even in loss.\n"
     ]
    }
   ],
   "source": [
    "# Simple stories dataset\n",
    "stories = load_dataset(\"SimpleStories/SimpleStories\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Simple stories dataset loaded.\")\n",
    "print(\"dataset size in mb:\", stories.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(stories))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(stories[random.randint(0, len(stories)-1)]['story'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FineWeb-Edu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineWeb-Edu is ready.\n",
      "dataset size in gb: 45.730818568728864\n",
      "Number of entries: 9672101\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Ravens Make Interesting Neighbors\n",
      "In ancient times ravens were regarded as birds of evil omen. It is hard to understand this today, for they are among the most playful of all birds. In windy weather they may often be seen performing aerial acrobatics, apparently just for their own amusement.\n",
      "Once when I was sitting at the edge of a cliff overlooking Copper Creek, a few miles from my home, three ravens came to investigate me. One after another, as if playing follow-the-leader, they would swoop down at me, veer off up into the sky, then down again for another swoop. At the low point of each dive they would glide by surprisingly slowly, so that I was able to get a good look at them. They kept coming closer and closer until they were passing within six feet above my head. One doesn’t often get an opportunity to view ravens at such close quarters, since they are intelligent enough to be shy of man.\n",
      "Ravens are big birds, with a wing-span of up to three feet and a length of twenty-six inches. They are entirely black and look much like their cousins the crows, but they are larger and are found in mountain country, whereas crows inhabit lower areas. Also, the raven’s calls are more varied. One peculiar call resembles the sound made by running a stick along a picket fence.\n",
      "The ravens’ playful nature may take a malicious turn, for they will sometimes attack other birds, seemingly just for sport. One day when I was out after huckleberries I saw two ravens checking a red-tailed hawk. The hawk dove through a dense stand of trees, and the ravens veered off rather than passing through the thicket. This gave the hawk time to catch an updraft and he began circling skyward; but the ravens caught the same updraft and came circling up after him.\n",
      "After a while one of the ravens lost interest and flew away, but the other kept after the hawk. The birds circled up and up until they looked so small that they could be distinguished from one another only with difficulty, by the different shapes of their wings.\n",
      "Finally the hawk reached the end of the updraft and the raven caught up with him. Then there began a thrilling aerial dogfight. First the raven would dart at the hawk and try to strike it with his beak; then the hawk in its turn would dart at the raven. Once or twice the birds seemed to succeed in striking one another. Whether they did any damage I don't know, but at last the raven gave up, flew away, and left the hawk soaring in lonely triumph at an immense altitude.\n"
     ]
    }
   ],
   "source": [
    "fineweb_edu = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\",  split='train', cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"FineWeb-Edu is ready.\")\n",
    "print(\"dataset size in gb:\", fineweb_edu.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(fineweb_edu))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(fineweb_edu[random.randint(0, len(fineweb_edu)-1)]['text']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenWebText2 dataset loaded.\n",
      "Dataset size in GB: 37.03822539001703\n",
      "Number of entries: 8013769\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "The Muslim wife of convicted Islamic terrorism-backer Anjem Choudary is being investigated by police after being caught on camera allegedly promoting the Islamic State and blasting “filthy Jews.”\n",
      "\n",
      "UK Express Lifelong welfare dependent Rubana Akhtar, 42, is the leader of the female wing of Choudary’s banned terror group, Al-Muhajiroun. She allegedly held secret study groups where she was filmed by an undercover reporter for Channel 4’s Dispatches saying, in reference to the so called Islamic State caliphate, “the good days have already begun”.\n",
      "\n",
      "Akhtar also accused “filthy Jews” of “audacity and arrogance” and encouraging the “killing of Muslim children and Muslim women”.\n",
      "\n",
      "Choudary (below), who could now face up to 10 years in prison after pledging allegiance to ISIS, tried to use his wife to get out of jail. He claimed she was suffering from cancer and an operation to remove a kidney, making it difficult for her to do “the housework”.\n",
      "\n",
      "He said: “I am a family man. I am 48. I have five children (All supported by welfare from British taxpayers). I have a dear mother that I care for, I am her primary carer. My wife as well is quite sick. She has difficulty doing the housework.”\n"
     ]
    }
   ],
   "source": [
    "# OpenWebText2 dataset\n",
    "owt2 = load_dataset(\"Skylion007/openwebtext\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "print(\"OpenWebText2 dataset loaded.\")\n",
    "print(\"Dataset size in GB:\", owt2.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(owt2))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(owt2[random.randint(0, len(owt2)-1)]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Q&A data to improve the model's ability to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A dataset loaded.\n",
      "dataset size in mb: 46.480509757995605\n",
      "Number of entries: 120959\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "What ingredients are known for their antioxidant properties and how do they help the skin? \n",
      " Ingredients such as vitamin C, green tea extract, resveratrol, and ferulic acid are known for their antioxidant benefits. They help neutralize harmful molecules, strengthen skin defenses, and provide a shield against daily assaults.\n"
     ]
    }
   ],
   "source": [
    "q_a1 = load_dataset(\"agentlans/text-sft-questions-answers-only\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Q&A dataset loaded.\")\n",
    "print(\"dataset size in mb:\", q_a1.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(q_a1))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(q_a1)-1)\n",
    "print(q_a1[index]['question'][:500], \"\\n\", q_a1[index]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit Instruct dataset loaded.\n",
      "dataset size in gb: 0.09901080373674631\n",
      "Number of entries: 84784\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "EILI5: Why would the UK leave the EU and what would be the consequences?  I'm really behind in the subject after being in the US and my French friends seem somewhat biased.... Please, like I'm 5. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " '\\n',\n",
       " 'ELI5: The UK feels the direction the EU is going in is harmful to their economy. The focus on higher taxes and regulation especially with regard to Banks and a proposed Financial Transaction Tax would do a lot of damage to the City of London which is the second most important financial center in the world(1st is Wall st). The City of London is very important to the economy of the UK. The UK also feels that certain countries(France,Germany) are not acting in the best interests of Europe as a whol')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#euclaise/reddit-instruct\n",
    "reddit_instruct = load_dataset(\"euclaise/reddit-instruct\", split='train', cache_dir=CACHE_DIR)\n",
    "# reddit_instruct = load_dataset(\"Felladrin/ChatML-reddit-instruct-curated\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Reddit Instruct dataset loaded.\")\n",
    "print(\"dataset size in gb:\", reddit_instruct.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(reddit_instruct))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(reddit_instruct)-1)\n",
    "print(reddit_instruct[index]['post_title'][:500], reddit_instruct[index]['post_text'][:500]), \"\\n\", reddit_instruct[index]['comment_text'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca dataset loaded.\n",
      "dataset size in mb: 44.06797695159912\n",
      "Number of entries: 52002\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Construct a dialogue between two people discussing the importance of being kind. \n",
      " Person 1: Being kind is so important in life, don't you think?\n",
      "Person 2: Absolutely. Kindness can make a huge difference in how people treat each other. It's important to be kind to everyone, regardless of their circumstances. \n",
      "Person 1: Yeah, I agree. It's shocking what can happen when people don't show kindness to each other. \n",
      "Person 2: Absolutely. Kindness can open doors of connection and understanding that wouldn't have been possible otherwise.\n"
     ]
    }
   ],
   "source": [
    "# tatsu-lab/alpaca ( for Q&A fine-tuning )\n",
    "alpaca = load_dataset(\"tatsu-lab/alpaca\", split='train')\n",
    "print(\"Alpaca dataset loaded.\")\n",
    "print(\"dataset size in mb:\", alpaca.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(alpaca))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(alpaca)-1)\n",
    "print(alpaca[index]['instruction'][:500], \"\\n\", alpaca[index]['output'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project i use tiktoken for the tokenizer, as it is the same tokenizer used by OpenAI for their models.\n",
    "\n",
    "I use the \"gpt2\" encoding which is a byte pair encoding (BPE) tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"rob-tokenizer\",\n",
    "    pat_str=tokenizer_base._pat_str,\n",
    "    mergeable_ranks=tokenizer_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **tokenizer_base._special_tokens,\n",
    "        \"<|im_start|>\": 50257,\n",
    "        \"<|im_end|>\": 50258,\n",
    "        \"<|pad|>\": 50259,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the byte pair encoding tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2061, 318, 24207, 1616, 2587, 30, 314, 2342, 257, 7684, 286, 1097, 5861, 290, 484, 1561, 546, 275, 32512, 7021, 290, 884, 11, 1312, 373, 11263, 644, 275, 32512, 318, 290, 1312, 18548, 1064, 597, 2562, 7468, 284, 644, 340, 318, 24207, 1616, 318, 655, 262, 1438, 329, 257, 16058, 286, 6147, 13, 554, 262, 29393, 995, 340, 338, 1690, 973, 355, 257, 1790, 1021, 329, 3354, 326, 547, 3235, 1389, 503, 286, 257, 1263, 2512, 286, 2587, 11, 355, 6886, 284, 11721, 3350, 654, 810, 44030, 6147, 318, 19036, 656, 257, 15936, 12070, 503, 286, 9629, 6147, 13, 7080, 3191, 318, 517, 5789, 329, 1588, 17794, 475, 340, 460, 779, 1365, 3081, 286, 21782, 290, 318, 4577, 284, 787, 329, 4833, 17794, 588, 3234, 3354, 13]\n",
      "Decoded text:\n",
      "What is Billet material? I watch a bunch of car videos and they talk about billet blocks and such, i was wondering what billet is and i cant find any easy explanation to what it is Billet is just the name for a chunk of metal. In the automotive world it's often used as a short hand for parts that were machined out of a big block of material, as opposed to cheaper castings where molten metal is poured into a mold pressed out of sheet metal. Machining is more expensive for large quantities but it can use better quality of metals and is easier to make for smaller quantities like race parts.\n",
      "Sample text length in characters: 594\n",
      "Sample text length in tokens: 127\n"
     ]
    }
   ],
   "source": [
    "# test of tokenizer on reddit_instruct\n",
    "sample_text = reddit_instruct[0]['post_title'] + \" \" + reddit_instruct[0]['post_text'] + \" \" + reddit_instruct[0]['comment_text']\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(tokens)\n",
    "print(\"Decoded text:\")\n",
    "print(tokenizer.decode(tokens)) \n",
    "print(f\"Sample text length in characters: {len(sample_text)}\")\n",
    "print(f\"Sample text length in tokens: {len(tokens)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting datasets functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 26209380\n",
      "Finetune dataset size: 257745\n",
      "Example entry from train dataset:\n",
      "{'id': None, 'url': None, 'title': None, 'text': \"Support/Questions\\n\\nrFactor support is handled by ISI. rFactor 2 support is handled by Studio 397. Please try to select the correct address below:\\n\\nrFactor: support@rfactor.net\\n\\nrFactor 2: support@studio-397.com\\n\\nFrequently Asked Questions\\n\\n- Where can I purchase rFactor and rFactor 2?\\n\\nBoth titles are available on Steam.\\n\\n- Where do I download/install rFactor and rFactor 2?\\n\\nBoth titles are downloaded through the Steam client after you own them there. You need to install their software, select rFactor or rFactor 2 in your library and follow on-screen instructions to install it.\\n\\n- I already purchased rFactor or rFactor 2 from ISI, can I transfer to Steam?\\n\\nIn most cases, yes. Please complete this form once for each product you need a key for. Your purchase will be manually verified and we'll either send you a key or ask for more information from you as needed.\\n\\n- I purchased an rFactor DVD, can I transfer to Steam?\\n\\nIt's unlikely. Please email support@rfactor.net and we will check what version you have.\\n\\n- I purchased an rFactor DVD and it says 'insert correct DVD'?\\n\\nYour DVD is being run on a version of Windows it was never designed for. Return it to the store.\\n\\n- Can I get a refund for my purchase on Steam?\\n\\nRefunds are handled by Steam, and in accordance with their policy.\\n\\n- Can the Steam version of rFactor be installed multiple times?\\n\\nrFactor 2 does not need this. With rFactor, the easiest way to avoid mod conflicts in doing this is to install rFactor to a new Steam library folder, and make a backup of that steamapps/common/rfactor folder. Install your mod(s) and then you can move rFactor folders in and out of that location to activate or deactivate them. However it should be noted that some mods do include unnecessary files that WILL break your installation entirely, forcing you to reinstall. Nothing we can do about that. If you can contact the mod creator and inform them, perhaps they can remove the problematic files.\\n\\n- I installed a mod and now rFactor says it needs to activate?\\n\\nThe mod replaced Steam files with non-Steam files and broke your installation. You'll have to remove that mod. We no longer even have a non-Steam version.\\n\\n- How do I install mods?\\n\\nWith rFactor 2, use the Steam workshop where possible and just click subscribe on the item. With rFactor, we use loose files and a manual process where you simply drop the correct files into the correct folder. Most mods have a readme.txt file to help you with that.\\n\\n- How do I create mods?\\n\\nWith rFactor 2 download and see guides on the Studio 397 Web site. With rFactor, you can download some of the tools here: MAS Tool, AIWCAM Editor, 3DSMax Plugins, Older Tools. A 3D object creator is required along with a painting tool capable of DDS creation.\\n\\n- Can I paint my cars?\\n\\nYes. We use DDS format, which requires the plugin for Photoshop from NVIDIA. Download templates.\\n\\n- rFactor doesn't detect my wheel?\\n\\nYou may have to manually bind controls for a wheel where they are not detected. Click an action in the settings, do the action with your controller.\\n\\n- rFactor / rFactor 2 doesn't launch?\\n\\nEither you have a bad mod (usually rF1 issue only), bad plugin, bad UI (rF1), are out of RAM, or are missing runtimes our software requires in order to run.\\n\\nWith rFactor 2, wherever you installed Steam look at the following paths for the files and install them all: steam/steamapps/common/rfactor 2/_CommonRedist/\\n\\n- Is the rFactor ISI matchmaker down?\\n\\nrFactor is now Steam-only. Server listings are available in the Steam client. Hosts need to be using the Steam version for it to show there. IP direct connection is always available, and most leagues use that.\\n\\n- How do I find the Steam server listings?\\n\\nOpen the Steam software. Go to the VIEW menu, then servers. Filter by game: rFactor.\\n\\n- How do I launch the Steam dedicated server?\\n\\nOpen the Steam software. Right-click rFactor in your games library list and you will see a dedicated server option.\\n\\n- Why are Studio 397 developing rFactor 2 instead of ISI?\\n\\nISI were beginning to wind down development on rF2 to focus on other projects. Studio 397 was formed to specifically give it back that focus and push it forwards.\\n\\n- Can I use rFactor or rFactor 2 commercially?\\n\\nNot without permission or commercial licensing. Contact ISI or Studio 397 to make sure you're operating legally.\\n\\n- Is rFactor Pro available to retail customers?\\n\\nNo. Never has been. Pro is a controlled product for auto manufacturers, race car engineers and racing teams.\\n\\n- Can I extract data (standings, telemetry, etc) from rFactor?\\n\\nYes. Our Internals Plugin system does this. Download (version 3). Instructions PDF, Plugin example.\", 'story': None, 'topic': None, 'theme': None, 'style': None, 'feature': None, 'grammar': None, 'persona': None, 'initial_word_type': None, 'initial_letter': None, 'word_count': None, 'character_count': None, 'num_paragraphs': None, 'avg_word_length': None, 'avg_sentence_length': None, 'flesch_reading_ease': None, 'flesch_kincaid_grade': None, 'dale_chall_readability_score': None, 'num_stories_in_completion': None, 'expected_num_stories_in_completion': None, 'generation_id': None, 'model': None, 'dump': None, 'file_path': None, 'language': None, 'language_score': None, 'token_count': None, 'score': None, 'int_score': None}\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_train_dataset = concatenate_datasets([  \n",
    "    wiki_en,\n",
    "    stories,\n",
    "    fineweb_edu,\n",
    "    owt2,  \n",
    "])  \n",
    "\n",
    "combined_finetune_dataset = concatenate_datasets([\n",
    "    q_a1,\n",
    "    reddit_instruct,\n",
    "    alpaca,\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_dataset = combined_train_dataset.shuffle(seed=42)\n",
    "finetune_dataset = combined_finetune_dataset.shuffle(seed=42)\n",
    "print(f\"Train dataset size: {len(combined_train_dataset)}\")\n",
    "print(f\"Finetune dataset size: {len(combined_finetune_dataset)}\")\n",
    "\n",
    "# Exemple \n",
    "\n",
    "print(\"Example entry from train dataset:\")\n",
    "index = random.randint(0, len(train_dataset)-1)\n",
    "print(train_dataset[index])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the dataloader from the \"LLMs from scratch\" repository. But adapted for multi-row text arrays.\n",
    "\n",
    "https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/dataloader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Dataset of the combined hugginface entries\n",
    "            tokenizer: the initiatokenizer to process text\n",
    "            max_length: Context window size\n",
    "        \"\"\"\n",
    "        self.data = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.input_tokens = []\n",
    "        self.target_tokens = []\n",
    "\n",
    "        self.pad_token_id = 50259         # <|pad|>\n",
    "        self.bos_token_id = 50257    # <|im_start|>\n",
    "        self.eos_token_id = 50258    # <|im_end|>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Raw text\n",
    "        \n",
    "        # Data format handling\n",
    "        entry = self.data[idx]\n",
    "        if 'text' in entry:\n",
    "            text = entry['text']\n",
    "        elif 'story' in entry:\n",
    "            text = entry['story']\n",
    "        elif 'question' in entry and 'answer' in entry: \n",
    "            text = \"User: \" + entry['question'] + \" Assistant:\" + entry['answer']\n",
    "        elif 'post_title' in entry and 'post_text' in entry and 'comment_text' in entry:\n",
    "            text = \"User: \" + entry['post_title'] + \" Assistant:\" + entry['post_text'] + \" \" + entry['comment_text']\n",
    "        elif 'instruction' in entry and 'output' in entry:\n",
    "            text = \"User: \" + entry['instruction'] + \" Assistant:\" + entry['output']\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data entry format\")\n",
    "        \n",
    "        text = str(text) # Ensure text is a string\n",
    "        #print(text)\n",
    "\n",
    "        # Adding Start and End tokens\n",
    "        text = \"<|im_start|>\" + text + \"<|im_end|>\" \n",
    "\n",
    "        # Tokenization\n",
    "        tokens = self.tokenizer.encode(text, allowed_special=\"all\")\n",
    "\n",
    "        # Truncation\n",
    "        tokens = tokens[:self.max_length] #Data is loost here ( fix later with sliding window )\n",
    "\n",
    "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)  # All tokens except last\n",
    "        labels = torch.tensor(tokens[1:], dtype=torch.long)      # All tokens except first\n",
    "\n",
    "\n",
    "        #Padding \n",
    "        padding_length = self.max_length - len(tokens)\n",
    "        if padding_length > 0:\n",
    "            input_ids = torch.cat([input_ids, torch.full((padding_length,), self.pad_token_id)])\n",
    "            labels = torch.cat([labels, torch.full((padding_length,), -100)])\n",
    "\n",
    "\n",
    "        attention_mask = (input_ids != self.pad_token_id).long() # 1 for real tokens, 0 for padding\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the configuration for the GPT model i am going to train. It is a smaller version of the GPT-2 model. \n",
    "\n",
    "- Context length: 512 tokens\n",
    "- Embedding dimension: 512\n",
    "- Number of attention heads: 8\n",
    "- Number of layers: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50260,\n",
    "    \"context_length\": 512, # max i could fit on my gpu\n",
    "    \"emb_dim\": 512,\n",
    "    \"number_heads\": 8,\n",
    "    \"number_layers\": 8,\n",
    "    \"drop_rate\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test of a entry from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 26209380\n"
     ]
    }
   ],
   "source": [
    "# Empty cuda cache and memory management\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_dataset = GPTDataset(combined_train_dataset, tokenizer, max_length=GPT_CONFIG[\"context_length\"])\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "\n",
    "batch_size = 12\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=2, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Test of a entry from dataloader\n",
      "{'input_ids': tensor([[50257,  2484,   789,  ...,   340,    13, 24006],\n",
      "        [50257, 14202, 50259,  ..., 50259, 50259, 50259],\n",
      "        [50257,    12, 36829,  ..., 50259, 50259, 50259],\n",
      "        ...,\n",
      "        [50257,  2061,   717,  ...,   428,   198,  5235],\n",
      "        [50257, 14202, 50259,  ..., 50259, 50259, 50259],\n",
      "        [50257, 13603,   272,  ..., 42116,   422,   262]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 2484,   789, 35375,  ...,    13, 24006,   329],\n",
      "        [14202, 50258,  -100,  ...,  -100,  -100,  -100],\n",
      "        [   12, 36829,  6763,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [ 2061,   717,  3181,  ...,   198,  5235, 16877],\n",
      "        [14202, 50258,  -100,  ...,  -100,  -100,  -100],\n",
      "        [13603,   272, 10322,  ...,   422,   262, 21165]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Test of a entry from dataloader\")\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytroch model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first implementation, i am using the transformer and embedding modules from PyTorch. Later i will try to implement the attention mechanism from scratch for better understanding.\n",
    "\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Gpt model class using transformer library\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Network components \n",
    "        ## Embedding layers\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.positional_encoding = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        ## Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['emb_dim'],\n",
    "            nhead=config['number_heads'],\n",
    "            dim_feedforward=4 * config['emb_dim'],\n",
    "            dropout=config['drop_rate'],\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True # stabilityy \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config['number_layers'])\n",
    "        ## Output layer\n",
    "        self.output_layer = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label_ids=None):   \n",
    "        batch_size, seq_length = input_ids.shape\n",
    "\n",
    "        # Embedding\n",
    "        token_embeddings = self.embedding(input_ids)  \n",
    "        pos_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        position_embeddings = self.positional_encoding(pos_ids)  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        embeddings = token_embeddings + position_embeddings  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        # Prevent attending to future tokens\n",
    "        causal_mask = torch.triu(torch.full((seq_length, seq_length), float('-inf'), device=input_ids.device), diagonal=1)\n",
    "\n",
    "        # voiding to pay attatention padding tokens\n",
    "        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        x = self.transformer(embeddings, mask=causal_mask, src_key_padding_mask=key_padding_mask, is_causal=True)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        # Computing loss \n",
    "        if label_ids is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(logits.view(-1, self.config['vocab_size']), label_ids.view(-1)) # Applies loss to predictions\n",
    "            return logits, loss\n",
    "\n",
    "        return logits, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model instantiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m device = \u001b[43mtorch\u001b[49m.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m model = GPTModel(GPT_CONFIG).to(device)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTModel(GPT_CONFIG).to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10) # Cosine annealing learning rate scheduler\n",
    "scaler = torch.cuda.amp.GradScaler()  # Gradient scaler for mixed precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, prompt, max_length=256, device='cpu'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_length)\n",
    "\n",
    "    generated_ids = input_ids\n",
    "    max_length = max_length - input_ids.shape[1]  # Remaining length for generation\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            attention_mask = torch.ones_like(generated_ids)  # All tokens are real (no padding)\n",
    "            logits, _ = model(generated_ids, attention_mask)\n",
    "            next_token_logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # (1, 1)\n",
    "\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)  # Append to sequence\n",
    "\n",
    "            if next_token_id.item() == 50258:  # Stop if eot token is generated\n",
    "                break\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_ids.squeeze().tolist())\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps= 1  # Number of steps to accumulate gradients\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, scheduler, device, num_epochs=3, accumulation_steps = 4, question_interval=500, saving_interval=5000):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        step_count = 0\n",
    "\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):  # bf16 optimized for ampere archi\n",
    "                logits, loss = model(input_ids, attention_mask, labels)\n",
    "                loss = loss / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()*accumulation_steps\n",
    "            progress_bar.set_postfix(loss=loss.item() * accumulation_steps)\n",
    "\n",
    "            # Inference check\n",
    "            step_count += 1\n",
    "            if step_count % question_interval == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    prompt = \"You are an AI being trained. How are you doing?\"\n",
    "                    generated_text = inference(model, tokenizer, prompt, max_length=50, device=device)\n",
    "                    print(f\"\\n[Inference at step {step_count}]\")\n",
    "                    #save generated text to file train_ouput.txt\n",
    "                    with open(\"outputs/train_output.txt\", \"a\") as f:\n",
    "                        if generated_text.strip() != \"\":\n",
    "                            f.write(f\"\\n[Inference at step {step_count}]: {generated_text}\\n\")  \n",
    "                            f.write(\"-\"*50 + \"\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"\\n[Inference at step {step_count}]: [No output generated]\\n\")\n",
    "                            f.write(\"-\"*50 + \"\\n\")\n",
    "                model.train()\n",
    "\n",
    "            if step_count % saving_interval == 0:\n",
    "                # Save model checkpoint\n",
    "                date_hour = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                # Create models directory if it doesn't exist\n",
    "                Path(f\"models/{date_hour}\").mkdir(parents=True, exist_ok=True)\n",
    "                checkpoint_path = f\"models/{date_hour}/weights_step{step_count}.pt\"\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"\\nModel checkpoint saved at step {step_count} to {checkpoint_path}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First training on combined train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/2184115 [00:00<?, ?it/s]/home/rob/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Epoch 1/1:   0%|          | 501/2184115 [01:45<146:10:01,  4.15it/s, loss=7.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 1000/2184115 [03:25<138:05:52,  4.39it/s, loss=6.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 1501/2184115 [05:08<136:23:39,  4.45it/s, loss=6.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 1500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 2000/2184115 [06:52<144:55:01,  4.18it/s, loss=6.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 2000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 2500/2184115 [08:36<144:40:57,  4.19it/s, loss=6.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 2500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 3000/2184115 [10:20<143:54:17,  4.21it/s, loss=6.37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 3000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 3501/2184115 [12:03<127:32:01,  4.75it/s, loss=6.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 3500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 4001/2184115 [13:39<126:52:30,  4.77it/s, loss=6.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 4000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 4501/2184115 [15:14<127:12:28,  4.76it/s, loss=6.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 4500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 4999/2184115 [16:49<122:17:21,  4.95it/s, loss=6.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 5000]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models/20260202_181015 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m torch.cuda.empty_cache()\n\u001b[32m      3\u001b[39m torch.cuda.synchronize()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(model, dataloader, optimizer, scheduler, device, num_epochs, accumulation_steps, question_interval, saving_interval)\u001b[39m\n\u001b[32m     49\u001b[39m         date_hour = datetime.now().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m         checkpoint_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_hour\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/weights_step\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mModel checkpoint saved at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m avg_loss = epoch_loss / \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/serialization.py:651\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    648\u001b[39m _check_save_filelike(f)\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    652\u001b[39m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[32m    653\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/serialization.py:525\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    524\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/serialization.py:496\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    494\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(torch._C.PyTorchFileWriter(\u001b[38;5;28mself\u001b[39m.file_stream))\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Parent directory models/20260202_181015 does not exist."
     ]
    }
   ],
   "source": [
    "#empty gpu memory \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    " \n",
    "\n",
    "train_loop(model, train_dataloader, optimizer, scheduler, device, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m torch.cuda.synchronize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/cuda/memory.py:170\u001b[39m, in \u001b[36mempty_cache\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[33;03m`nvidia-smi`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m \u001b[33;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources \n",
    "\n",
    "### Principal references: \n",
    "- https://arxiv.org/abs/2005.14165 (GPT-3 paper)\n",
    "- https://arxiv.org/abs/2002.05709 (Attention is all you need paper)\n",
    "- Build a Large Language Model (from scratch) by Sebastian Raschka\n",
    "\n",
    "### About Padding tokens in Language Modeling\n",
    "- https://arxiv.org/html/2510.01238v1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
