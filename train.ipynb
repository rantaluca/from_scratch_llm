{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet GPT - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used to train a small language model using PyTorch from scratch. The model is inspired by the GPT architecture.\n",
    "\n",
    "\n",
    "#### Hardware\n",
    "- RTX3060 12GB VRAM\n",
    "- AMD Ryzen 7 5800X 8-Core\n",
    "- 32GB RAM\n",
    "- Ubuntu 22.04 LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CACHE_DIR = \"/media/rob/RobsDisk/cache_data_llm\"\n",
    "os.environ['HF_HOME'] = CACHE_DIR\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_DIR, \"datasets\")\n",
    "os.environ['HF_METRICS_CACHE'] = os.path.join(CACHE_DIR, \"metrics\")\n",
    "os.environ['HF_MODULES_CACHE'] = os.path.join(CACHE_DIR, \"modules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random, math\n",
    "\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "import tiktoken\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Common knowledge datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### English Wikipedia crawled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Wikipedia dataset loaded.\n",
      "dataset size in gb: 18.812774107791483\n",
      "Number of entries: 6407814\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Wakefield Scott Stornetta (born June 1959) is an American physicist and scientific researcher. His 1991 paper \"How to Time-Stamp a Digital Document”, co-authored with Stuart Haber, won the 1992 Discover Award for Computer Software and is considered to be one of the most important papers in the development of cryptocurrencies.\n",
      "\n",
      "Stornetta is currently a fellow at the Creative Destruction Lab, a science and technology-based startup accelerator at the Rotman School of Management at the University of Toronto. He is also a founding partner and chief scientist of Yugen Partners, a blockchain-focused venture capital firm that counsels investors on blockchain startup opportunities and governments on blockchain policy, as well as the director of the board of advisors for the American Blockchain PAC.\n",
      "\n",
      "Career \n",
      "In 1989, Stornetta began working as a scientific researcher at Bell Communications Research (Bellcore), where he met Stuart Haber, his future scientific partner and collaborator.\n",
      "\n",
      "In 1994, Stornetta and Haber co-founded Surety Technologies, a spinoff of Bellcore. In 1995, Surety’s offering constituted the first commercial deployment of a blockchain and is currently the oldest running blockchain.\n",
      "\n",
      "In 2019, Stornetta and Haber delivered the keynote address at the Becker Friedman Institute for Research in Economics at the University of Chicago Cryptocurrencies and Blockchains Conference. He also spoke in both the Distinguished Lecture Series at Virginia Tech University and the Decentralized Learning Series at the University of Nicosia.\n",
      "\n",
      "Contributions \n",
      "Stornetta and Stuart Haber are the most cited authors in Satoshi Nakamoto’s original Bitcoin white paper; of the eight citations, three reference their work.\n",
      "\n",
      "Their 1991 paper \"How to Time-Stamp a Digital Document” is where they first describe a digital hierarchy system called \"Blockchain\". In this study, Stornetta and Haber sought to create mechanisms to create digital time stamps, offering a solution for maintaining the integrity of digital records and ensuring that they could not be modified or manipulated. \n",
      "\n",
      "In 1992, Stornetta, Haber, and Dave Bayer incorporated Merkle trees into their design, improving its efficiency by allowing many document certificates to be collected into one block.\n",
      "\n",
      "Stornetta is a co-author of \"Central Bank Digital Currencies and a Euro for the Future\", a report published in June 2021 by the EU Blockchain Observatory and Forum. The report addresses the latest trends and developments of digital currencies and discusses the future of blockchain in Europe and the rest of the world.\n",
      "\n",
      "Personal life \n",
      "Stornetta is a member of The Church of Jesus Christ of Latter-day Saints.\n",
      "\n",
      "He describes himself as a libertarian.\n",
      "\n",
      "References \n",
      "\n",
      "1959 births\n",
      "Living people\n",
      "20th-century American physicists\n",
      "University of Toronto people\n"
     ]
    }
   ],
   "source": [
    "# English Wikipedia crawled dataset\n",
    "# path to store the dataset cache: /Volumes/RobertsDisk\n",
    "wiki_en = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split='train', cache_dir=CACHE_DIR) \n",
    "print(\"English Wikipedia dataset loaded.\")\n",
    "print(\"dataset size in gb:\", wiki_en.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(wiki_en))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(wiki_en[random.randint(0, len(wiki_en)-1)]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple stories dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple stories dataset loaded.\n",
      "dataset size in mb: 3030.012650489807\n",
      "Number of entries: 2115696\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Yonder in a bright land, there was a wise old man. He had seen many things and learned many lessons. The village around him was full of simple folk. They often gathered by the big tree to hear him speak. One day, he began a tale of a blue dragon that lived in the mountains. \"You see,\" he said, \"this dragon was not what it seemed.\" \n",
      "\n",
      "The blue dragon looked fierce, with shiny scales and sharp teeth. But deep inside, it was kind and gentle. Many villagers feared it. They thought it would eat them if they got too close. But one day, a brave girl decided to visit the dragon. She thought, \"Maybe it is not bad.\" As she walked to the mountain, she felt nervous but also excited. \n",
      "\n",
      "When she reached the cave, she found the dragon weeping. The girl asked, \"Why do you cry, great dragon?\" The dragon replied, \"I am lonely. Everyone fears me for my looks.\" The girl listened closely and said, \"You do not have to be alone.\" Slowly, they talked and laughed. The dragon shared its dreams of flying high in the sky with her. \n",
      "\n",
      "Days passed, and the girl told her friends about the dragon. They were not sure at first, but they decided to visit. When they saw the dragon, they were scared. \"Why is this dragon blue?\" they asked. But the girl said, \"It is not the color that matters. It is what is inside.\" \n",
      "\n",
      "The dragon, hearing her words, smiled. It opened its heart, and the villagers felt warm. They began to understand that the blue dragon was a friend, not a foe. They invited the dragon to the village, where it danced and played with the children. \n",
      "\n",
      "With each laugh, the fear faded. They all learned that what seems scary can be kind. And the wise old man smiled as he saw this change. He spoke softly, \"Remember, my young ones, it is what is in the heart that counts.\" \n",
      "\n",
      "Now, in the village, the blue dragon is loved. It soars through the sky, showing everyone that true beauty lies within. The wise man nodded, knowing his story had touched their hearts.\n"
     ]
    }
   ],
   "source": [
    "# Simple stories dataset\n",
    "stories = load_dataset(\"SimpleStories/SimpleStories\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Simple stories dataset loaded.\")\n",
    "print(\"dataset size in mb:\", stories.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(stories))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(stories[random.randint(0, len(stories)-1)]['story'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FineWeb-Edu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineWeb-Edu is ready.\n",
      "dataset size in gb: 45.730818568728864\n",
      "Number of entries: 9672101\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Curcuma longa, turmeric in English and haridra in Sanskrit (meaning the special bond with the divine Lord) has been honored for centuries as both a medicine and food, and has recently surged in popularity within the alternative health and holistic nutrition communities in the West.\n",
      "How it grows\n",
      "The turmeric plant grows to about three feet in height and produces both a flower and a rhizome, a horizontal stem that grows underground. India has been growing it since ancient times and is currently the largest producer together with other countries like China, Pakistan, Jamaica, Peru, Thailand and Taiwan.\n",
      "This is thanks to “curcumin”, a compound found in turmeric which is a healing substance known to have the following health benefits:\n",
      "• Promotes skin health\n",
      "• Regulates blood sugar\n",
      "• Reduces internal inflammation\n",
      "• Balances cholesterol\n",
      "• Boosts brain function\n",
      "• Enhances detoxification\n",
      "A rich history\n",
      "Turmeric may be the latest superfood but it has been embedded in Indian culture for thousands of years and it still plays a prominent role in the country’s cuisines, Ayurvedic medicines, and in many Hindu traditional rituals. There is no other spice so diverse in its use.\n",
      "Because of its yellow-orange coloring, turmeric was associated with the Sun in Hindu mythology and recognized as sacred and auspicious. It is the color of the solar plexus chakra, which is considered the energy center of the body relating to the metabolic and digestive systems.\n",
      "The spice has even been used in Indian textiles: The robes of the Hindu monks, for example, were traditionally colored with a natural yellow dye made from turmeric.\n",
      "easy everyday uses\n",
      "Almost all Indian dishes have turmeric but did you know that it’s been used to add yellow into American cheese, prepared mustard, butter, yellow cake mix, pickles, popcorn and other products?\n",
      "These days, turmeric’s claim to fame is Golden Milk. But I love using the spice in so many other dishes:\n",
      "- Add a pinch to your salad dressing\n",
      "- Create a refreshing turmeric-honey lemonade\n",
      "- Boost your bone broth with a hint of turmeric\n",
      "- The golden milk can enrich your breakfast oatmeal\n",
      "- Add a pinch to green juices and smoothies\n",
      "- Spice up your grains\n",
      "- Roasted vegetables can get a splash of color\n",
      "- Marinades take to the spice well, too!\n",
      "Used in ritual\n",
      "Some Indian wedding traditions have several rituals that honor the spice. There’s one in which a string is dyed yellow with turmeric paste and the groom ties this around the bride’s neck. The necklace is called a Mangal Sutra and indicates that the woman is married. The necklace is also made in silver and gold for the married woman to wear it always.\n",
      "The Haldi Ceremony\n",
      "My favorite ritual showcasing this spice is a pre-wedding celebration called the Haldi ceremony. In different regions and communities it is also known by other names – Gaye Halud, Tel Baan, Halad Chadawat , Manjha and Pithi.\n",
      "The Haldi ceremony is typically held the day before or on the morning of the wedding. Close family and friends gather at the homes of both the groom and bride respectively.\n",
      "A simple, special paste called Uptan or Ubtan is made using turmeric powder, sandalwood powder, Bengal Gram flour and rosewater. Sometimes milk or yogurt is used – the recipe is often adapted by individual family members or local communities.\n",
      "The gathered family and friends apply the paste on the bride-to-be and groom’s face, neck, hands, arms, legs and feet. The bride and groom also smear a tiny bit of paste on their unmarried friends and siblings.\n",
      "Just like the throwing of the bouquet, it is said that whoever gets touched by this sacred paste will soon find their own beloved partners.\n",
      "The reason behind the ceremony\n",
      "There are several important reasons for this particular ceremony:\n",
      "- The turmeric application helps to ward off evil spirits and protect the bride and groom from any bad omens that might harm them as a couple. This is also why the bride and groom are usually not allowed to leave their home until the actual wedding ceremony.\n",
      "- The yellow color is considered very auspicious and sacred as the Sun energy signifies purification of the heart and body. For the bride, it serves as a reminder of the goddess within her. There is a symbolic cleansing of the past when the paste is washed off. This marks the beginning of a new life together as a couple.\n",
      "- Ancient texts (and even modern studies) state that this golden spice has powerful antioxidant, anti inflammatory and antiseptic properties that help cleanse the body. The application of the turmeric paste helps in removing the dead skin cells and revealing the youthful and rejuvenated skin. Think of it like a spa day before the grand celebration.\n",
      "- The active compound curcumin is known to act as a natural remedy for anxiety and headaches. This helps to soothe the bride and groom’s nerves and to relax before the big day.\n",
      "- Turmeric is a symbol of blessing for the couple to, as Spock would say, “live long and prosper.” According to Hindu customs, all the married ladies of the house apply the paste to the bride and the groom and bless them with a long and strong relationship, good health and many children. The ritual is accompanied by music, singing, dancing and of course, lots of good food.\n",
      "Here is a simple uptan recipe—\n",
      "This recipe will make a generous amount for the bride or groom.\n",
      "• ⅓ cup sandalwood powder\n",
      "• ⅓ cup turmeric powder\n",
      "• 1 cup Gram flour ( known as besan at the South asian market)\n",
      "• 1 ½ cups or more rosewater\n",
      "Mix all the ingredients together until it forms a smooth paste (you may want to add a bit more or less rosewater, depending on how the paste forms). The paste can be made 1-2 hours before use. Keep it covered so that it does not dry up (and if it does, just add a bit more rosewater). Once applied to the body make sure you wash off within 15-20 minutes, otherwise your skin will stain!\n",
      "A word of caution:\n",
      "The spice will impart its yellow color to every dish that you add it to (not to mention your hands and body). Follow my grandma’s rule: add just enough to see it but not taste it!\n",
      "This post was contributed by our friend Vinita Jacinto.\n"
     ]
    }
   ],
   "source": [
    "fineweb_edu = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\",  split='train', cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"FineWeb-Edu is ready.\")\n",
    "print(\"dataset size in gb:\", fineweb_edu.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(fineweb_edu))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(fineweb_edu[random.randint(0, len(fineweb_edu)-1)]['text']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenWebText2 dataset loaded.\n",
      "Dataset size in GB: 37.03822539001703\n",
      "Number of entries: 8013769\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Jobs at Adelaide's Coca-Cola factory in doubt as company cancels shifts, union says\n",
      "\n",
      "Posted\n",
      "\n",
      "Workers at Adelaide's Coca-Cola production factory have had their shifts cancelled and a meeting has been called for Wednesday morning leaving about 150 jobs in doubt, the union has said.\n",
      "\n",
      "Staff have been told to attend a meeting at the Port Road business at 8:00am for an announcement, which will coincide with the release of the company's 2016 full-year financial results.\n",
      "\n",
      "The union, United Voice, said it understood some casual workers at the plant were stood down today.\n",
      "\n",
      "United Voice spokesperson Carolyn Smart said it had not been advised of any other job losses.\n",
      "\n",
      "\"We would be extremely disappointed if workers were advised of any job losses tomorrow, especially given the consultation clauses contained in their enterprise agreement,\" Ms Smart said.\n",
      "\n",
      "\"Workers have had no advance warning of any changes or closures. United Voice has not been consulted about any changes either.\n",
      "\n",
      "\"If it is the case, it is a sad reflection on multinational corporations and the distain they've shown for their workers.\"\n",
      "\n",
      "The company has been contacted for comment.\n",
      "\n",
      "The South Australian Government said it had also contacted the company to find out about tomorrow's meeting, but had not had a response.\n",
      "\n",
      "It said it will make sure it provides whatever support is needed for workers.\n",
      "\n",
      "Topics: unemployment, community-and-society, company-news, thebarton-5031, adelaide-5000, sa\n"
     ]
    }
   ],
   "source": [
    "# OpenWebText2 dataset\n",
    "owt2 = load_dataset(\"Skylion007/openwebtext\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "print(\"OpenWebText2 dataset loaded.\")\n",
    "print(\"Dataset size in GB:\", owt2.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(owt2))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(owt2[random.randint(0, len(owt2)-1)]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Q&A data to improve the model's ability to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A dataset loaded.\n",
      "dataset size in mb: 46.480509757995605\n",
      "Number of entries: 120959\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Why is durability a critical factor in product design and marketing? \n",
      " Durability is crucial because it ensures a product can withstand wear and stress, meets user satisfaction criteria, and leads to repeat purchases and a positive brand image.\n"
     ]
    }
   ],
   "source": [
    "q_a1 = load_dataset(\"agentlans/text-sft-questions-answers-only\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Q&A dataset loaded.\")\n",
    "print(\"dataset size in mb:\", q_a1.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(q_a1))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(q_a1)-1)\n",
    "print(q_a1[index]['question'][:500], \"\\n\", q_a1[index]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit Instruct dataset loaded.\n",
      "dataset size in gb: 0.09901080373674631\n",
      "Number of entries: 84784\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "[Magic: The Gathering] What exactly is \"The Roil\" on Zendikar? \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " '\\n',\n",
       " \"It's a traveling volitality of extreme climate and geographical upheaval that moves across the plane as a result of its unique mana. Weather rapidly changes, land rises and falls, and oceans fill and dry in its wake. It's the primary reason why Zendikar remains so wild and untamed; you can't even reliably map the place for long, let alone settle it in any but the most fleeting sense. \\n\\nIt's also why Zendikar is such a great prison for the Eldrazi.\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#euclaise/reddit-instruct\n",
    "reddit_instruct = load_dataset(\"euclaise/reddit-instruct\", split='train', cache_dir=CACHE_DIR)\n",
    "# reddit_instruct = load_dataset(\"Felladrin/ChatML-reddit-instruct-curated\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Reddit Instruct dataset loaded.\")\n",
    "print(\"dataset size in gb:\", reddit_instruct.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(reddit_instruct))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(reddit_instruct)-1)\n",
    "print(reddit_instruct[index]['post_title'][:500], reddit_instruct[index]['post_text'][:500]), \"\\n\", reddit_instruct[index]['comment_text'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca dataset loaded.\n",
      "dataset size in mb: 44.06797695159912\n",
      "Number of entries: 52002\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Give me five examples of artificial intelligence technologies and how they are being used in the world today. \n",
      " 1. Machine Learning: a branch of AI that enables systems to learn from data and improve their capabilities without being explicitly programmed. Machine Learning is used to analyse data, recognize patterns, and make predictions.\n",
      "\n",
      "2. Natural Language Processing: a branch of AI that focuses on understanding and generating human languages. NLP is used by many businesses to automate customer service and automated chat bots.\n",
      "\n",
      "3. Computer Vision: a branch of AI which enables systems to interpret, analy\n"
     ]
    }
   ],
   "source": [
    "# tatsu-lab/alpaca ( for Q&A fine-tuning )\n",
    "alpaca = load_dataset(\"tatsu-lab/alpaca\", split='train')\n",
    "print(\"Alpaca dataset loaded.\")\n",
    "print(\"dataset size in mb:\", alpaca.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(alpaca))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(alpaca)-1)\n",
    "print(alpaca[index]['instruction'][:500], \"\\n\", alpaca[index]['output'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project i use tiktoken for the tokenizer, as it is the same tokenizer used by OpenAI for their models.\n",
    "\n",
    "I use the \"gpt2\" encoding which is a byte pair encoding (BPE) tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"rob-tokenizer\",\n",
    "    pat_str=tokenizer_base._pat_str,\n",
    "    mergeable_ranks=tokenizer_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **tokenizer_base._special_tokens,\n",
    "        \"<|im_start|>\": 50257,\n",
    "        \"<|im_end|>\": 50258,\n",
    "        \"<|pad|>\": 50259,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the byte pair encoding tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2061, 318, 24207, 1616, 2587, 30, 314, 2342, 257, 7684, 286, 1097, 5861, 290, 484, 1561, 546, 275, 32512, 7021, 290, 884, 11, 1312, 373, 11263, 644, 275, 32512, 318, 290, 1312, 18548, 1064, 597, 2562, 7468, 284, 644, 340, 318, 24207, 1616, 318, 655, 262, 1438, 329, 257, 16058, 286, 6147, 13, 554, 262, 29393, 995, 340, 338, 1690, 973, 355, 257, 1790, 1021, 329, 3354, 326, 547, 3235, 1389, 503, 286, 257, 1263, 2512, 286, 2587, 11, 355, 6886, 284, 11721, 3350, 654, 810, 44030, 6147, 318, 19036, 656, 257, 15936, 12070, 503, 286, 9629, 6147, 13, 7080, 3191, 318, 517, 5789, 329, 1588, 17794, 475, 340, 460, 779, 1365, 3081, 286, 21782, 290, 318, 4577, 284, 787, 329, 4833, 17794, 588, 3234, 3354, 13]\n",
      "Decoded text:\n",
      "What is Billet material? I watch a bunch of car videos and they talk about billet blocks and such, i was wondering what billet is and i cant find any easy explanation to what it is Billet is just the name for a chunk of metal. In the automotive world it's often used as a short hand for parts that were machined out of a big block of material, as opposed to cheaper castings where molten metal is poured into a mold pressed out of sheet metal. Machining is more expensive for large quantities but it can use better quality of metals and is easier to make for smaller quantities like race parts.\n",
      "Sample text length in characters: 594\n",
      "Sample text length in tokens: 127\n"
     ]
    }
   ],
   "source": [
    "# test of tokenizer on reddit_instruct\n",
    "sample_text = reddit_instruct[0]['post_title'] + \" \" + reddit_instruct[0]['post_text'] + \" \" + reddit_instruct[0]['comment_text']\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(tokens)\n",
    "print(\"Decoded text:\")\n",
    "print(tokenizer.decode(tokens)) \n",
    "print(f\"Sample text length in characters: {len(sample_text)}\")\n",
    "print(f\"Sample text length in tokens: {len(tokens)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting datasets functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 26209380\n",
      "Finetune dataset size: 257745\n",
      "Example entry from train dataset:\n",
      "{'id': None, 'url': None, 'title': None, 'text': \"btcx\\n\\nSr. Member\\n\\nOffline\\n\\nActivity: 302\\n\\nMerit: 253\\n\\nVIPSr. MemberActivity: 302Merit: 253 [ANN] OGRR.COM adds 20,000 users, HIRING Full-Time PHP Developer, Designer November 13, 2012, 10:55:03 PM\\n\\nLast edit: November 17, 2012, 03:52:42 AM by btcx #1\\n\\nCurrently, we're in search of a full-time Senior Web Developer to continue expanding and refining the features on the forum, and help grow it to one of the largest communities on the Internet.\\n\\nRequirements:\\n\\nLinux, Apache, MySQL, PHP\\n\\nJavascript, Node.js\\n\\nCSS3, HTML5\\n\\n~$50/hr negotiable, equity possibilities\\n\\nAlso, we're looking for someone reliable with some CSS skills to create a few alternative themes for the forums.\\n\\nIf this sounds like something you might be interested in, PM me here or over at Ogrr.com has recently completed its merger with mmoexchange.net, adding 20,000 new users to our Bitcoin trade forums. We have a few other acquisition deals in the pipeline that will introduce thousands more to Bitcoin once those mergers are complete.Currently, we're in search of a full-time Senior Web Developer to continue expanding and refining the features on the forum, and help grow it to one of the largest communities on the Internet.Requirements:Linux, Apache, MySQL, PHPJavascript, Node.jsCSS3, HTML5~$50/hr negotiable, equity possibilitiesAlso, we're looking for someone reliable with some CSS skills to create a few alternative themes for the forums.If this sounds like something you might be interested in, PM me here or over at Ogrr.com Bitcoin, Ethereum, Litecoin, Namecoin, Dogecoin, Ripple, Stellar, US dollar, euro, British pound, Canadian dollar and Japanese yen exchange: https://www.kraken.com\", 'story': None, 'topic': None, 'theme': None, 'style': None, 'feature': None, 'grammar': None, 'persona': None, 'initial_word_type': None, 'initial_letter': None, 'word_count': None, 'character_count': None, 'num_paragraphs': None, 'avg_word_length': None, 'avg_sentence_length': None, 'flesch_reading_ease': None, 'flesch_kincaid_grade': None, 'dale_chall_readability_score': None, 'num_stories_in_completion': None, 'expected_num_stories_in_completion': None, 'generation_id': None, 'model': None, 'dump': None, 'file_path': None, 'language': None, 'language_score': None, 'token_count': None, 'score': None, 'int_score': None}\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_train_dataset = concatenate_datasets([  \n",
    "    wiki_en,\n",
    "    stories,\n",
    "    fineweb_edu,\n",
    "    owt2,  \n",
    "])  \n",
    "\n",
    "combined_finetune_dataset = concatenate_datasets([\n",
    "    q_a1,\n",
    "    reddit_instruct,\n",
    "    alpaca,\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_dataset = combined_train_dataset.shuffle(seed=42)\n",
    "finetune_dataset = combined_finetune_dataset.shuffle(seed=42)\n",
    "print(f\"Train dataset size: {len(combined_train_dataset)}\")\n",
    "print(f\"Finetune dataset size: {len(combined_finetune_dataset)}\")\n",
    "\n",
    "# Exemple \n",
    "\n",
    "print(\"Example entry from train dataset:\")\n",
    "index = random.randint(0, len(train_dataset)-1)\n",
    "print(train_dataset[index])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the dataloader from the \"LLMs from scratch\" repository. But adapted for multi-row text arrays.\n",
    "\n",
    "https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/dataloader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Dataset of the combined hugginface entries\n",
    "            tokenizer: the initiatokenizer to process text\n",
    "            max_length: Context window size\n",
    "        \"\"\"\n",
    "        self.data = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.input_tokens = []\n",
    "        self.target_tokens = []\n",
    "\n",
    "        self.pad_token_id = 50259         # <|pad|>\n",
    "        self.bos_token_id = 50257    # <|im_start|>\n",
    "        self.eos_token_id = 50258    # <|im_end|>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Raw text\n",
    "        \n",
    "        # Data format handling\n",
    "        entry = self.data[idx]\n",
    "        if 'text' in entry:\n",
    "            text = entry['text']\n",
    "        elif 'story' in entry:\n",
    "            text = entry['story']\n",
    "        elif 'question' in entry and 'answer' in entry: \n",
    "            text = \"User: \" + entry['question'] + \" Assistant:\" + entry['answer']\n",
    "        elif 'post_title' in entry and 'post_text' in entry and 'comment_text' in entry:\n",
    "            text = \"User: \" + entry['post_title'] + \" Assistant:\" + entry['post_text'] + \" \" + entry['comment_text']\n",
    "        elif 'instruction' in entry and 'output' in entry:\n",
    "            text = \"User: \" + entry['instruction'] + \" Assistant:\" + entry['output']\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data entry format\")\n",
    "        \n",
    "        text = str(text) # Ensure text is a string\n",
    "        #print(text)\n",
    "\n",
    "        # Adding Start and End tokens\n",
    "        text = \"<|im_start|>\" + text + \"<|im_end|>\" \n",
    "\n",
    "        # Tokenization\n",
    "        tokens = self.tokenizer.encode(text, allowed_special=\"all\")\n",
    "\n",
    "        # Truncation\n",
    "        tokens = tokens[:self.max_length] #Data is loost here ( fix later with sliding window )\n",
    "\n",
    "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)  # All tokens except last\n",
    "        labels = torch.tensor(tokens[1:], dtype=torch.long)      # All tokens except first\n",
    "\n",
    "\n",
    "        #Padding \n",
    "        padding_length = self.max_length - len(tokens)\n",
    "        if padding_length > 0:\n",
    "            input_ids = torch.cat([input_ids, torch.full((padding_length,), self.pad_token_id)])\n",
    "            labels = torch.cat([labels, torch.full((padding_length,), -100)])\n",
    "\n",
    "\n",
    "        attention_mask = (input_ids != self.pad_token_id).long() # 1 for real tokens, 0 for padding\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 26209380\n"
     ]
    }
   ],
   "source": [
    "# Empty cuda cache and memory management\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_dataset = GPTDataset(combined_train_dataset, tokenizer, max_length=512)\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "\n",
    "batch_size = 10\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test of a entry from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Test of a entry from dataloader\n",
      "{'input_ids': tensor([[50257,  1525, 17704,  ...,  1099,   290, 21825],\n",
      "        [50257, 38202, 50173,  ..., 50259, 50259, 50259],\n",
      "        [50257, 14202, 50259,  ..., 50259, 50259, 50259],\n",
      "        ...,\n",
      "        [50257,  5159,  6634,  ..., 50259, 50259, 50259],\n",
      "        [50257,  5886,  2063,  ...,  1751,  2299, 16856],\n",
      "        [50257, 31160, 18024,  ...,  1440,  5852,    13]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 1525, 17704,   367,  ...,   290, 21825,   416],\n",
      "        [38202, 50173,  2634,  ...,  -100,  -100,  -100],\n",
      "        [14202, 50258,  -100,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [ 5159,  6634,  3203,  ...,  -100,  -100,  -100],\n",
      "        [ 5886,  2063,   286,  ...,  2299, 16856,  1243],\n",
      "        [31160, 18024,   349,  ...,  5852,    13,   383]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Test of a entry from dataloader\")\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the configuration for the GPT model i am going to train. It is a smaller version of the GPT-2 model. \n",
    "\n",
    "- Context length: 512 tokens\n",
    "- Embedding dimension: 512\n",
    "- Number of attention heads: 8\n",
    "- Number of layers: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50260,\n",
    "    \"context_length\": 512,\n",
    "    \"emb_dim\": 512,\n",
    "    \"number_heads\": 8,\n",
    "    \"number_layers\": 8,\n",
    "    \"drop_rate\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytroch model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first implementation, i am using the transformer and embedding modules from PyTorch. Later i will try to implement the attention mechanism from scratch for better understanding.\n",
    "\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Gpt model class using transformer library\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Network components \n",
    "        ## Embedding layers\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.positional_encoding = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        ## Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['emb_dim'],\n",
    "            nhead=config['number_heads'],\n",
    "            dim_feedforward=4 * config['emb_dim'],\n",
    "            dropout=config['drop_rate'],\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True # stabilityy \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config['number_layers'])\n",
    "        ## Output layer\n",
    "        self.output_layer = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label_ids=None):   \n",
    "        batch_size, seq_length = input_ids.shape\n",
    "\n",
    "        # Embedding\n",
    "        token_embeddings = self.embedding(input_ids)  \n",
    "        pos_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        position_embeddings = self.positional_encoding(pos_ids)  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        embeddings = token_embeddings + position_embeddings  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        # Prevent attending to future tokens\n",
    "        causal_mask = torch.triu(torch.full((seq_length, seq_length), float('-inf'), device=input_ids.device), diagonal=1)\n",
    "\n",
    "        # voiding to pay attatention padding tokens\n",
    "        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        x = self.transformer(embeddings, mask=causal_mask, src_key_padding_mask=key_padding_mask, is_causal=True)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        # Computing loss \n",
    "        if label_ids is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(logits.view(-1, self.config['vocab_size']), label_ids.view(-1)) # Applies loss to predictions\n",
    "            return logits, loss\n",
    "\n",
    "        return logits, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model instantiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (embedding): Embedding(50260, 512)\n",
      "  (positional_encoding): Embedding(512, 512)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=512, out_features=50260, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTModel(GPT_CONFIG).to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4) # AdamW optimizer\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10) # Cosine annealing learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, prompt, max_length=512, device='cpu'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_length)\n",
    "\n",
    "    generated_ids = input_ids\n",
    "    max_length = max_length - input_ids.shape[1]  # Remaining length for generation\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            attention_mask = torch.ones_like(generated_ids)  # All tokens are real (no padding)\n",
    "            logits, _ = model(generated_ids, attention_mask)\n",
    "            next_token_logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # (1, 1)\n",
    "\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)  # Append to sequence\n",
    "\n",
    "            if next_token_id.item() == 100265:  # Stop if <|im_end|> token is generated\n",
    "                break\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_ids.squeeze().tolist())\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, dataloader, optimizer, scheduler, device, num_epochs=3, question_interval=500):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        step_count = 0\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Step scheduler every batch\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            # Inference check\n",
    "            step_count += 1\n",
    "            if step_count % question_interval == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    prompt = \"You are an AI being trained. How are you doing? Please answer briefly.\"\n",
    "                    generated_text = inference(model, tokenizer, prompt, max_length=50, device=device)\n",
    "                    print(f\"\\n[Inference at step {step_count}]\")\n",
    "                model.train()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First training on combined train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/3276173 [00:00<?, ?it/s]/home/rob/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Epoch 1/3:   0%|          | 500/3276173 [02:47<340:48:45,  2.67it/s, loss=6.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 500]: You are an AI being trained. How are you doing? Please answer briefly.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 1000/3276173 [05:34<343:31:09,  2.65it/s, loss=6.58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 1000]: You are an AI being trained. How are you doing? Please answer briefly.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 1396/3276173 [07:46<303:56:14,  2.99it/s, loss=6.4] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m torch.cuda.empty_cache()\n\u001b[32m      3\u001b[39m torch.cuda.synchronize()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(model, dataloader, optimizer, scheduler, device, num_epochs, question_interval)\u001b[39m\n\u001b[32m     16\u001b[39m optimizer.step()\n\u001b[32m     17\u001b[39m scheduler.step()  \u001b[38;5;66;03m# Step scheduler every batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m epoch_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m progress_bar.set_postfix(loss=loss.item())\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Inference check\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#empty gpu memory \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    " \n",
    "\n",
    "train_loop(model, train_dataloader, optimizer, scheduler, device, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources \n",
    "\n",
    "### Principal references: \n",
    "- https://arxiv.org/abs/2005.14165 (GPT-3 paper)\n",
    "- https://arxiv.org/abs/2002.05709 (Attention is all you need paper)\n",
    "- Build a Large Language Model (from scratch) by Sebastian Raschka\n",
    "\n",
    "### About Padding tokens in Language Modeling\n",
    "- https://arxiv.org/html/2510.01238v1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
