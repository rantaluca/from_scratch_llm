{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet GPT - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used to train a small language model using PyTorch from scratch. The model is inspired by the GPT architecture.\n",
    "\n",
    "\n",
    "#### Hardware\n",
    "- RTX3060 12GB VRAM\n",
    "- AMD Ryzen 7 5800X 8-Core\n",
    "- 32GB RAM\n",
    "- Ubuntu 22.04 LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#CACHE_DIR = \"/media/rob/RobertsDisk/data_cache\"\n",
    "CACHE_DIR = \"/home/rob/projet_gpt/cache_huggingface\"\n",
    "os.environ['HF_HOME'] = CACHE_DIR\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_DIR, \"datasets\")\n",
    "os.environ['HF_METRICS_CACHE'] = os.path.join(CACHE_DIR, \"metrics\")\n",
    "os.environ['HF_MODULES_CACHE'] = os.path.join(CACHE_DIR, \"modules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random, math\n",
    "\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "import tiktoken\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Common knowledge datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### English Wikipedia crawled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Wikipedia dataset loaded.\n",
      "dataset size in gb: 18.812774107791483\n",
      "Number of entries: 6407814\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Hernán Ismael Galíndez (30 March 1987) is an Argentine-Ecuadorian professional footballer who plays as a goalkeeper for Ecuadorian Serie A club Aucas.\n",
      "\n",
      "Born in Argentina, he began his career in Argentina with Rosario Central before settling in Ecuador with Universidad Católica, where he made over 300 appearances in a nine-year spell. In 2022, he joined Universidad de Chile but left the club after six months, citing harassment from the club's fanbase. He returned to Ecuador with Aucas, helping th\n"
     ]
    }
   ],
   "source": [
    "# English Wikipedia crawled dataset\n",
    "# path to store the dataset cache: /Volumes/RobertsDisk\n",
    "wiki_en = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split='train', cache_dir=CACHE_DIR) \n",
    "print(\"English Wikipedia dataset loaded.\")\n",
    "print(\"dataset size in gb:\", wiki_en.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(wiki_en))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(wiki_en[random.randint(0, len(wiki_en)-1)]['text'][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple stories dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 2115696/2115696 [00:05<00:00, 396051.19 examples/s]\n",
      "Generating test split: 100%|██████████| 21371/21371 [00:00<00:00, 424350.81 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple stories dataset loaded.\n",
      "dataset size in mb: 3030.012650489807\n",
      "Number of entries: 2115696\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Ovens crackled as Rita prepared for the festival. She had received a letter from her friend, explaining how their town celebrated by baking bread for the community. The letter made Rita think of her own town's traditions. She remembered how food brought people together and decided to host a baking event. \n",
      "\n",
      "Rita invited neighbors to join her in making bread. They laughed and shared recipes, blending flavors from their cultures. As the bread baked, the smell wafted through the streets, drawing peo\n"
     ]
    }
   ],
   "source": [
    "# Simple stories dataset\n",
    "stories = load_dataset(\"SimpleStories/SimpleStories\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Simple stories dataset loaded.\")\n",
    "print(\"dataset size in mb:\", stories.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(stories))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(stories[random.randint(0, len(stories)-1)]['story'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FineWeb-Edu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 9672101/9672101 [02:42<00:00, 59365.68 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineWeb-Edu is ready.\n",
      "dataset size in gb: 45.730818568728864\n",
      "Number of entries: 9672101\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "“The typical American diet is too high in calories, saturated fat, sodium, and added sugars, and does not have enough fruits, vegetables, whole grains, calcium, and fiber. Such that contributes to some of the leading causes of death and increases the risk of lots of fatal diseases“– Paul Ebeling\n",
      "The risks associated with poor eating habits have long been known, and yet a new study revealed just how detrimental the dangers of a poor diet can actually be to our long-term health.\n",
      "In a recent study \n"
     ]
    }
   ],
   "source": [
    "fineweb_edu = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\",  split='train', cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"FineWeb-Edu is ready.\")\n",
    "print(\"dataset size in gb:\", fineweb_edu.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(fineweb_edu))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(fineweb_edu[random.randint(0, len(fineweb_edu)-1)]['text'][:500]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Q&A data to improve the model's ability to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 120959/120959 [00:00<00:00, 802562.08 examples/s]\n",
      "Generating validation split: 100%|██████████| 30240/30240 [00:00<00:00, 1534742.85 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A dataset loaded.\n",
      "dataset size in mb: 46.480509757995605\n",
      "Number of entries: 120959\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "What was the first song Marlboro created, and what was its Portuguese equivalent of? \n",
      " Marlboro's first song was 'Melô da Mulher Feia', a Portuguese version of 2 Live Crew's 'Do Wah Diddy', which achieved significant success on the radio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q_a1 = load_dataset(\"agentlans/text-sft-questions-answers-only\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Q&A dataset loaded.\")\n",
    "print(\"dataset size in mb:\", q_a1.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(q_a1))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(q_a1)-1)\n",
    "print(q_a1[index]['question'][:500], \"\\n\", q_a1[index]['answer'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 84784/84784 [00:00<00:00, 538655.93 examples/s]\n",
      "Generating test split: 100%|██████████| 2000/2000 [00:00<00:00, 243684.87 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit Instruct dataset loaded.\n",
      "dataset size in gb: 0.09901080373674631\n",
      "Number of entries: 84784\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "If I put two things in the microwave instead of one, are they both going to be heated slower? The scenario is the following: I have a microwave that operates at 1000 W and I have two cups of water.\n",
      "\n",
      "I noticed that for the same amount of time spent in the microwave, a single cup of water will get warmer if it's in there alone. If I put both cups in there, I also habe to spend more time.\n",
      "\n",
      "So is the power being \"shared\" by the cups, and if so, do they share this power based on volume / mass / position?\n",
      "\n",
      "Do both cups of water receive 500 W each?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " '\\n',\n",
       " 'A fixed amount of radiation is bouncing around in there. Not all of it will manage to be absorbed by the item. \\n\\nSo there is some loss and some of it transfers to the heat you want. So multiple items are indeed sharing the same amount of energy. But the exact proportions of heating, how much loss are a complex matter of the shapes of these items, their position the exact design of your microwave and things like that. It probably won’t work out that 2 cups takes exactly twice as long as 1 cup.\\n\\nB')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#euclaise/reddit-instruct\n",
    "reddit_instruct = load_dataset(\"euclaise/reddit-instruct\", split='train', cache_dir=CACHE_DIR)\n",
    "# reddit_instruct = load_dataset(\"Felladrin/ChatML-reddit-instruct-curated\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Reddit Instruct dataset loaded.\")\n",
    "print(\"dataset size in gb:\", reddit_instruct.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(reddit_instruct))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(reddit_instruct)-1)\n",
    "print(reddit_instruct[index]['post_title'][:500], reddit_instruct[index]['post_text'][:500]), \"\\n\", reddit_instruct[index]['comment_text'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 52002/52002 [00:00<00:00, 1103159.06 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca dataset loaded.\n",
      "dataset size in mb: 44.06797695159912\n",
      "Number of entries: 52002\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Generate an HTML code for a 3Cols table that also has a header. Output the code directly. \n",
      " <table>\n",
      "  <tr>\n",
      "    <th>Header 1</th>\n",
      "    <th>Header 2</th>\n",
      "    <th>Header 3</th>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>Column 1</td>\n",
      "   <td>Column 2</td>\n",
      "   <td>Column 3</td>\n",
      "  </tr>\n",
      "</table>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tatsu-lab/alpaca ( for Q&A fine-tuning )\n",
    "alpaca = load_dataset(\"tatsu-lab/alpaca\", split='train')\n",
    "print(\"Alpaca dataset loaded.\")\n",
    "print(\"dataset size in mb:\", alpaca.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(alpaca))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(alpaca)-1)\n",
    "print(alpaca[index]['instruction'][:500], \"\\n\", alpaca[index]['output'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project i use tiktoken for the tokenizer, as it is the same tokenizer used by OpenAI for their models.\n",
    "\n",
    "I use the \"gpt2\" encoding which is a byte pair encoding (BPE) tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"rob-tokenizer\",\n",
    "    pat_str=tokenizer_base._pat_str,\n",
    "    mergeable_ranks=tokenizer_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **tokenizer_base._special_tokens,\n",
    "        \"<|im_start|>\": 100264,\n",
    "        \"<|im_end|>\": 100265,\n",
    "        \"<|pad|>\": 0,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the byte pair encoding tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2061, 318, 24207, 1616, 2587, 30, 314, 2342, 257, 7684, 286, 1097, 5861, 290, 484, 1561, 546, 275, 32512, 7021, 290, 884, 11, 1312, 373, 11263, 644, 275, 32512, 318, 290, 1312, 18548, 1064, 597, 2562, 7468, 284, 644, 340, 318, 24207, 1616, 318, 655, 262, 1438, 329, 257, 16058, 286, 6147, 13, 554, 262, 29393, 995, 340, 338, 1690, 973, 355, 257, 1790, 1021, 329, 3354, 326, 547, 3235, 1389, 503, 286, 257, 1263, 2512, 286, 2587, 11, 355, 6886, 284, 11721, 3350, 654, 810, 44030, 6147, 318, 19036, 656, 257, 15936, 12070, 503, 286, 9629, 6147, 13, 7080, 3191, 318, 517, 5789, 329, 1588, 17794, 475, 340, 460, 779, 1365, 3081, 286, 21782, 290, 318, 4577, 284, 787, 329, 4833, 17794, 588, 3234, 3354, 13]\n",
      "Decoded text:\n",
      "What is Billet material? I watch a bunch of car videos and they talk about billet blocks and such, i was wondering what billet is and i cant find any easy explanation to what it is Billet is just the name for a chunk of metal. In the automotive world it's often used as a short hand for parts that were machined out of a big block of material, as opposed to cheaper castings where molten metal is poured into a mold pressed out of sheet metal. Machining is more expensive for large quantities but it can use better quality of metals and is easier to make for smaller quantities like race parts.\n",
      "Sample text length in characters: 594\n",
      "Sample text length in tokens: 127\n"
     ]
    }
   ],
   "source": [
    "# test of tokenizer on reddit_instruct\n",
    "sample_text = reddit_instruct[0]['post_title'] + \" \" + reddit_instruct[0]['post_text'] + \" \" + reddit_instruct[0]['comment_text']\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(tokens)\n",
    "print(\"Decoded text:\")\n",
    "print(tokenizer.decode(tokens)) \n",
    "print(f\"Sample text length in characters: {len(sample_text)}\")\n",
    "print(f\"Sample text length in tokens: {len(tokens)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting datasets functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_plain_text(batch):\n",
    "    # Handle both 'text' and 'story' \n",
    "    keys = batch.keys()\n",
    "    if 'text' in keys:\n",
    "        sources = batch['text']\n",
    "    elif 'story' in keys:\n",
    "        sources = batch['story']\n",
    "    else:\n",
    "        # Fallback if neither exists \n",
    "        return {'text': [''] * len(list(batch.values())[0])}\n",
    "        \n",
    "    return {'text': [f\"<|im_start|>\\n{t}\\n<|im_end|>\" for t in sources]}\n",
    "\n",
    "def format_qa(batch):\n",
    "    return {'text': [f\"<|im_start|>\\nQuestion: {q}\\nAnswer: {a}\\n<|im_end|>\" \n",
    "                     for q, a in zip(batch['question'], batch['answer'])]}\n",
    "\n",
    "def format_reddit(batch):\n",
    "    return {'text': [f\"<|im_start|>\\nQuestion: {pt}\\nAnswer: {ptext}\\nAnswer: {ct}\\n<|im_end|>\" \n",
    "                     for pt, ptext, ct in zip(batch['post_title'], batch['post_text'], batch['comment_text'])]}\n",
    "\n",
    "def format_alpaca(batch):\n",
    "    return {'text': [f\"<|im_start|>\\nQuestion: {inst}\\nAnswer: {out}\\n<|im_end|>\" \n",
    "                     for inst, out in zip(batch['instruction'], batch['output'])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting Wiki...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6407814/6407814 [01:23<00:00, 76374.26 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting Stories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2115696/2115696 [00:09<00:00, 212877.96 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting FineWeb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9672101/9672101 [03:17<00:00, 48906.32 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting QA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120959/120959 [00:00<00:00, 461960.44 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting Reddit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 84784/84784 [00:00<00:00, 197938.66 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting Alpaca...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 52002/52002 [00:00<00:00, 319196.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Formatting Wiki...\")\n",
    "wiki_formatted = wiki_en.map(format_plain_text, remove_columns=wiki_en.column_names, batched=True)\n",
    "\n",
    "print(\"Formatting Stories...\")\n",
    "stories_formatted = stories.map(format_plain_text, remove_columns=stories.column_names, batched=True)\n",
    "\n",
    "print(\"Formatting FineWeb...\")\n",
    "fineweb_formatted = fineweb_edu.map(format_plain_text, remove_columns=fineweb_edu.column_names, batched=True)\n",
    "\n",
    "print(\"Formatting QA...\")\n",
    "qa_formatted = q_a1.map(format_qa, remove_columns=q_a1.column_names, batched=True)\n",
    "\n",
    "print(\"Formatting Reddit...\")\n",
    "reddit_formatted = reddit_instruct.map(format_reddit, remove_columns=reddit_instruct.column_names, batched=True)\n",
    "\n",
    "print(\"Formatting Alpaca...\")\n",
    "alpaca_formatted = alpaca.map(format_alpaca, remove_columns=alpaca.column_names, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size: 18453356\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_hf_dataset = concatenate_datasets([\n",
    "    wiki_formatted, \n",
    "    stories_formatted, \n",
    "    fineweb_formatted, \n",
    "    qa_formatted, \n",
    "    reddit_formatted, \n",
    "    alpaca_formatted\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "combined_hf_dataset = combined_hf_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Combined dataset size: {len(combined_hf_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hf_dataset: The Hugging Face dataset object (from Part 1)\n",
    "            tokenizer: The tokenizer to process text\n",
    "            max_length: Context window size\n",
    "        \"\"\"\n",
    "        self.data = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.pad_token_id = 0       # <|pad|>\n",
    "        self.eos_token_id = 100265  # <|im_end|>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Raw text\n",
    "        text = self.data[idx]['text']\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = self.tokenizer.encode(text, allowed_special=\"all\")\n",
    "\n",
    "        # Truncation and padding\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "            tokens[-1] = self.eos_token_id  # ensure last token is eos\n",
    "\n",
    "\n",
    "\n",
    "        input_ids = torch.tensor(tokens, dtype=torch.long)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "\n",
    "        #Padding \n",
    "        padding_length = self.max_length - input_ids.size(0)\n",
    "        if padding_length > 0:\n",
    "            # Create padding tensors\n",
    "            pad_ids = torch.full((padding_length,), self.pad_token_id, dtype=torch.long)\n",
    "            pad_mask = torch.zeros((padding_length,), dtype=torch.long) # 0 = ignore\n",
    "            \n",
    "            # Concatenate\n",
    "            input_ids = torch.cat([input_ids, pad_ids])\n",
    "            attention_mask = torch.cat([attention_mask, pad_mask])\n",
    "\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        #adding the ignore index for padding tokens\n",
    "        if padding_length > 0:\n",
    "            # We know the padding is at the end\n",
    "            labels[-padding_length:] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources \n",
    "\n",
    "### Principal references: \n",
    "- https://arxiv.org/abs/2005.14165 (GPT-3 paper)\n",
    "- https://arxiv.org/abs/2002.05709 (Attention is all you need paper)\n",
    "- Build a Large Language Model (from scratch) by Sebastian Raschka\n",
    "\n",
    "### About Padding tokens in Language Modeling\n",
    "- https://arxiv.org/html/2510.01238v1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
