{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet GPT - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used to train a small language model using PyTorch from scratch. The model is inspired by the GPT architecture.\n",
    "\n",
    "\n",
    "#### Hardware\n",
    "- RTX3060 12GB VRAM\n",
    "- AMD Ryzen 7 5800X 8-Core\n",
    "- 32GB RAM\n",
    "- Ubuntu 22.04 LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CACHE_DIR = \"/media/rob/RobsDisk/cache_data_llm\"\n",
    "os.environ['HF_HOME'] = CACHE_DIR\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_DIR, \"datasets\")\n",
    "os.environ['HF_METRICS_CACHE'] = os.path.join(CACHE_DIR, \"metrics\")\n",
    "os.environ['HF_MODULES_CACHE'] = os.path.join(CACHE_DIR, \"modules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/micromamba/envs/mlenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random, math\n",
    "\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "import tiktoken\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Common knowledge datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### English Wikipedia crawled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Wikipedia dataset loaded.\n",
      "dataset size in gb: 18.812774107791483\n",
      "Number of entries: 6407814\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Michael Edward Krukow (born January 21, 1952), nicknamed \"Kruk\", is an American former professional baseball player and sportscaster. As a starting pitcher, he played in Major League Baseball (MLB) for the Chicago Cubs, Philadelphia Phillies, and San Francisco Giants. He has been a television and radio broadcaster for the Giants since 1990, and is one half of the popular \"Kruk and Kuip\" duo, alongside his friend and former teammate Duane Kuiper. He was an All-Star in 1986.\n",
      "\n",
      "Early life\n",
      "Krukow was born in Long Beach, California, and attended San Gabriel High School in San Gabriel, California, where he played as a catcher. Krukow was a fan of the Los Angeles Dodgers, the Giants' archrival, and attended many games at Dodger Stadium with his father. He was drafted as a catcher by the California Angels in the 32nd round of the 1970 Major League Baseball Draft, but did not sign.\n",
      "\n",
      "College career\n",
      "Krukow became a pitcher and played college baseball for the Cal Poly Mustangs in San Luis Obispo, California. Though his collegiate eligibility was cut short, he still holds the school record for career earned run average at 1.94, and is tied for most shutouts in a season with five.\n",
      "\n",
      "Professional career\n",
      "\n",
      "Draft, minor leagues, and Chicago Cubs (1976–1981)\n",
      "The Chicago Cubs selected Krukow in the eighth round of the 1973 MLB draft.  He first appeared for the Cubs in 1976, and joined the starting rotation in 1977; he would remain with the team for four more seasons.\n",
      "\n",
      "Philadelphia Phillies (1982)\n",
      "He was traded from the Cubs to the Phillies for Keith Moreland, Dickie Noles and Dan Larson on December 8, 1981. \n",
      "\n",
      "For the Phillies, the right-handed starter was second only to Steve Carlton in wins, posting a 13–11 record and an impressive 3.12 ERA.\n",
      "\n",
      "San Francisco Giants (1983–1989)\n",
      "He was dealt along with Mark Davis and minor-league outfielder C.L. Penigar from the Phillies to the Giants for Joe Morgan and Al Holland on December 14, 1982. The trade helped Philadelphia win the National League pennant in , but it also gave San Francisco two pitching arms that would become a big part of the Giants' success in the late 1980s.\n",
      "\n",
      "Although known as a starter, Krukow earned his only career save on August 31, 1984, pitching to just one batter (the Phillies' Sixto Lezcano), inducing a game-ending groundout, therefore preserving a 6–5 Giant victory.\n",
      "\n",
      "Krukow's best campaign was in  when he became the first Giants pitcher since Ron Bryant in 1973 to win at least 20 games in a season with a 20–9 record and a 3.05 ERA. Krukow finished third in that year's NL Cy Young Award behind Mike Scott and Fernando Valenzuela. Krukow was selected to the National League All-Star team that season. He received the Willie Mac Award in 1985 and 1986 for his spirit and leadership. In , Krukow helped lead the Giants to their first division championship in 16 years.\n",
      "\n",
      "Krukow's 17 no decisions were the most among MLB starting pitchers in 1987, as well as being the most ever by a Giants starter dating back to at least 1908. He made the only postseason appearance of his career in Game-4 of the 1987 National League Championship Series. Krukow was the winning pitcher in a nine-inning complete game, allowing two runs on nine hits, as the Giants beat the St. Louis Cardinals, 4–2. It was the Cardinals, however, that took the series in seven games to reach the World Series.\n",
      "\n",
      "Krukow went 4–3 with a 3.98 ERA during a 1989 campaign which was cut short on June 30 when he underwent arthroscopic surgery to repair a rotator cuff tear in his pitching shoulder after spending parts of three seasons on the injured list for what was believed to be bursitis. He was ninth in Giants franchise history with 66 wins and sixth with 802 strikeouts at the time of his retirement as an active player on March 19, 1990 due to recurring shoulder problems. Krukow posted a 124–117 record with a 3.90 ERA in 369 games during his 14-season MLB career.\n",
      "\n",
      "Broadcasting career\n",
      "After his playing career, Krukow became a radio and television sportscaster. Krukow began broadcasting as an occasional color analyst for KNBR radio in  and became a full-time broadcaster in . He is a seven-time Emmy award winner. \"Kruk,\" who was named as the starting right-handed pitcher to the 1980s Giants All-Decade Team in a vote by Bay Area media in 1999, is noted for his deep knowledge of the game and tremendous sense of humor. He is known for his detailed scouting reports on umpires' strike zones.\n",
      "\n",
      "Part of the San Francisco Giants broadcasting team, Krukow is half of the duo dubbed \"Kruk and Kuip,\" (pronounced \"Kruke\" and \"Kipe\") along with partner Duane Kuiper, a former Giants teammate. Krukow and Kuiper tape a game-day commentary (\"Kruk and Kuip on baseball\") for KNBR radio as part of the Giants' pre-game radio coverage. Notably, although Krukow was a pitcher and Kuiper was a position player, Krukow has five career home runs, four more than Kuiper (who managed only one in his career despite having over 3,000 at-bats).\n",
      "\n",
      "Krukow has a few \"Kruktionary\" catchphrases, including: \"Grab some pine, meat\"; \"Just another, ha ha ha ha, laugher!\" (after a nail-biter win); and \"I wanna get that!\", the last of which is associated with a product endorsement.\n",
      "\n",
      "Video games\n",
      "Krukow and Kuiper can be heard as the commentators in Electronic Arts video games MVP Baseball 2003, 2004 and 2005. They include Krukow's familiar \"grab some pine, meat\" quote.\n",
      "\n",
      "Personal life \n",
      "\n",
      "Until 2014, Krukow and his wife Jennifer resided in San Luis Obispo, California, but they moved to Reno, Nevada to be closer to their grandchildren though Krukow stays in San Francisco during the season. They have five adult children, Jarek, Baker, Tessa, Chase and Weston. Mike Krukow is a talented musician, and proficient in the guitar, the mandolin, the banjo, and the ukulele.\n",
      "\n",
      "In July 2014, Krukow revealed he was suffering from inclusion body myositis (IBM). His condition was known to the Giants and many of his fellow broadcasters, but he kept the condition a secret from the general public until then. Krukow first noticed that he was having problems about 10 years earlier, when he had lost about  off his golf drive. According to sportswriter Steve Fainaru, Krukow \"blew it off... for years\", but \"secretly feared he had amyotrophic lateral sclerosis, Lou Gehrig's disease\". Finally, in 2011, he saw the Giants' team neurologist, who referred him to a neuromuscular specialist who in turn diagnosed him with IBM. The disease, which mainly affects the quadriceps and hand muscles, is not life-threatening, but now requires him to use a cane; eventually, Krukow will have to use a walker and/or a scooter. Because of increasing hand weakness that limits his ability to play stringed instruments, he has recently taken up the drums, which require a different set of muscular movements. Krukow plans to continue broadcasting for the foreseeable future, but in 2017, he announced that he would reduce his schedule to 120 games a season working road games only west of Denver, except for postseason games.\n",
      "\n",
      "For the 2020 season, NBC Sports Bay Area announced that it would experiment with having Krukow comment from the network's San Francisco studio rather than on-site (promoted as \"SplitKast\") for 22 NL West road games. However, since no broadcasters were allowed to travel to opposing ballparks due to the COVID-19 pandemic, Krukow and Kuiper ended up broadcasting each Giants game from Oracle Park.\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      " The Buddy System: Long-lasting friendship between Krukow, Kuiper comes across the airwaves\n",
      " San Francisco Giants Broadcast Team on the Comcast SportNet Bay Area site\n",
      "\n",
      "National League All-Stars\n",
      "Cal Poly Mustangs baseball players\n",
      "Chicago Cubs players\n",
      "Philadelphia Phillies players\n",
      "San Francisco Giants announcers\n",
      "San Francisco Giants players\n",
      "Major League Baseball pitchers\n",
      "Major League Baseball broadcasters\n",
      "Baseball players from Long Beach, California\n",
      "1952 births\n",
      "Living people\n",
      "Gulf Coast Cubs players\n",
      "Midland Cubs players\n",
      "Wichita Aeros players\n",
      "Phoenix Firebirds players\n"
     ]
    }
   ],
   "source": [
    "# English Wikipedia crawled dataset\n",
    "# path to store the dataset cache: /Volumes/RobertsDisk\n",
    "wiki_en = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split='train', cache_dir=CACHE_DIR) \n",
    "print(\"English Wikipedia dataset loaded.\")\n",
    "print(\"dataset size in gb:\", wiki_en.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(wiki_en))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(wiki_en[random.randint(0, len(wiki_en)-1)]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple stories dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple stories dataset loaded.\n",
      "dataset size in mb: 3030.012650489807\n",
      "Number of entries: 2115696\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Without warning, a star fell from the sky and landed near me. It was small, but it glowed with a light that made shadows dance. I, a master of mischief, decided to capture it. The star twinkled in the grass, and I could feel its magic pulsing. My plan was to use it to spread chaos across the land.\n",
      "\n",
      "In the night, I snuck up to the star and grabbed it. The moment I touched it, I felt a strange pull. It whispered to me, promising to grant wishes. But deep down, I knew that the wishes would bring sadness. My heart raced at the thought of taking happiness from others.\n",
      "\n",
      "As I wandered through the village, I listened to their hopes. They wished for peace, for joy, and for love. I could twist those wishes into shadows, pulling them into my darkness. But with each wish I stole, I felt a little less like myself. The star seemed to glow dimmer with every wish I took.\n",
      "\n",
      "At dawn, the villagers gathered, their faces filled with worry. They looked up at the sky, searching for the light I had taken. Suddenly, the star shimmered in my hand, and I saw the girl with the golden hair again. She stood there, determined. With her light, she reached out and touched the star. In that moment, I felt the warmth of true magic. My heart began to change, and I understood that helping others was far stronger than harming them.\n"
     ]
    }
   ],
   "source": [
    "# Simple stories dataset\n",
    "stories = load_dataset(\"SimpleStories/SimpleStories\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Simple stories dataset loaded.\")\n",
    "print(\"dataset size in mb:\", stories.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(stories))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(stories[random.randint(0, len(stories)-1)]['story'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FineWeb-Edu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineWeb-Edu is ready.\n",
      "dataset size in gb: 45.730818568728864\n",
      "Number of entries: 9672101\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "- Bitcoin is different than any currency you’ve used before, so it's very important to understand some key points.\n",
      "- Unlike government issued money, that can be inflated at will,\n",
      "- The supply of bitcoin is mathematically limited to twenty one million bitcoins\n",
      "- Bitcoins are impossible to be counterfeited or inflated.\n",
      "- Use them to send or receive any amount of money, with anyone, anywhere in the world, at very low cost.\n",
      "- Bitcoin payments are impossible to be blocked, and bitcoin wallets can’t be frozen.\n",
      "- Unless the entire world's internet is turned off, the Bitcoin network is unstoppable and cannot be censored.\n",
      "- Learn even MORE!\n",
      "BitCoins - Be Informed\n"
     ]
    }
   ],
   "source": [
    "fineweb_edu = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\",  split='train', cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"FineWeb-Edu is ready.\")\n",
    "print(\"dataset size in gb:\", fineweb_edu.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(fineweb_edu))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(fineweb_edu[random.randint(0, len(fineweb_edu)-1)]['text']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenWebText2 dataset loaded.\n",
      "Dataset size in GB: 37.03822539001703\n",
      "Number of entries: 8013769\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "MEMORANDUM FOR: The President\n",
      "\n",
      "FROM: Veteran Intelligence Professionals for Sanity (VIPS)\n",
      "\n",
      "SUBJECT: Veteran Intelligence Professionals Challenge CIA’s “Rebuttal” on Torture\n",
      "\n",
      "Former CIA leaders responsible for allowing torture to become part of the 21st Century legacy of the CIA are trying to rehabilitate their tarnished reputations with the release of a new book, Rebuttal: The CIA Responds to the Senate Intelligence Committee’s Study of Its Detention and Interrogation Program. They are pushing the lie that the only allegations against them are from a partisan report issued by Democrats from the Senate Intelligence Committee.\n",
      "\n",
      "We recall the answer of General John Kimmons, the former Deputy Director of Operations for the Joint Chiefs of Staff, who was asked if good intelligence could be obtained from abusive practices. He replied: “I am absolutely convinced the answer to your first question is no. No good intelligence is going to come from abusive practices. I think history tells us that. I think the empirical evidence of the last five years, hard years, tell us that.”\n",
      "\n",
      "But the allegation that the CIA leaders were negligent and guilty was not the work of an isolated group of partisan Democrat Senators. The Senate Intelligence report on torture enjoyed bipartisan support. Senator John McCain, for example, whose own encounter with torture in North Vietnamese prisons scarred him physically and emotionally, embraced and endorsed the work of Senator Feinstein. It was only a small group of intransigent Republicans, led by Saxby Chambliss, who obstructed the work of the Senate Intel Committee.\n",
      "\n",
      "Indeed, some of us witnessed firsthand during the administration of President George W. Bush that the Senate Select Committee on Intelligence and the House Permanent Select Committee on Intelligence were virtually paralyzed from conducting any meaningful oversight of the CIA and the U.S. Intelligence Community by the Republican members of these committees. Instead, they pursued the clear objective of protecting the Bush administration from any criticism for engaging in torture during the “War on Terror.”\n",
      "\n",
      "It is curious that our former colleagues stridently denounce the work of the Senate Intelligence Committee but are mute with respect to an equally damning report from the CIA’s own inspector general, John Helgerson, in 2004.\n",
      "\n",
      "Helgerson’s report, “ Counterterrorism Detention and Interrogation Activities (September 2001-October 2003) ,” was published on May 7, 2004, and classified Top Secret. That report alone is damning of the CIA leadership and it is important to remind all about the specifics of those conclusions. According to the CIA’s own inspector general:\n",
      "\n",
      "–The Agency’s detention and interrogation of terrorists has provided intelligence that has enabled the identification and apprehension of other terrorists and warned of terrorist plots planned in the United States and around the world. . . . The effectiveness of particular interrogation techniques in eliciting information that might not otherwise have been obtained cannot be so easily measured however.\n",
      "\n",
      "–In addition, some Agency officials are aware of interrogation activities that were outside or beyond the scope of the written DOJ opinion. Officers are concerned that future public revelation of the CTC Program is inevitable and will seriously damage Agency officers’ personal reputations, as well as the reputation and effectiveness of the Agency itself.\n",
      "\n",
      "–By distinction the Agency-especially in the early months of the Program-failed to provide adequate staffing, guidance, and support to those involved with the detention and interrogation of detainees . . .\n",
      "\n",
      "–The Agency failed to issue in a timely manner comprehensive written guidelines for detention and interrogation activities. . . .Such written guidance as does exist . . . is inadequate.\n",
      "\n",
      "–During the interrogation of two detainees, the waterboard was used in a manner inconsistent with the written DOJ legal opinion of 1 August 2002.\n",
      "\n",
      "–Agency officers report that reliance on analytical assessments that were unsupported by credible intelligence may have resulted in the application of EITs without justification.\n",
      "\n",
      "The CIA’s Inspector General makes it very clear that there was a failure by the CIA leaders, who include Director of Central Intelligence George Tenet, Deputy Director of Central Intelligence John McLaughlin, Counter Terrorism Center Chief Cofer Black, Counter Terrorism Center Chief Jose Rodriguez and the Director Directorate of Operations James L. Pavitt. Lack of proper guidance and oversight created fertile soil for subsequent abuses and these men were guilty of failing to properly do their jobs.\n",
      "\n",
      "We do not have to rely solely on the report of the CIA’s Inspector General. In addition, the Report by the Senate Armed Services Committee on Detainee Treatment reached the same conclusions about the origins, evils, harm to U.S. policy and intelligence collection of “enhanced interrogation,” a euphemism for “torture” first used by Nazi Germany during World War II.\n",
      "\n",
      "Indeed, all independent analyses of the enhanced interrogation program have concluded it constituted torture, was ineffective, and contrary to all American laws, ideals, and intelligence practices. [Background here, here and here.] We also have the testimony and record of Ali Soufan, an Arabic-speaking FBI Agent, who was involved with several interrogations before torture was used and who achieved substantive results without violating international law.\n",
      "\n",
      "The sworn testimony of FBI Agent Ali Soufan, who is the only U.S. Government employee to testify under oath on these matters, completely contradicts the authors of Rebuttal:\n",
      "\n",
      "“In the middle of my interrogation of the high-ranking terrorist Abu Zubaydah at a black-site prison 12 years ago, my intelligence work wasn’t just cut short for so-called enhanced interrogation techniques to begin. After I left the black site, those who took over left, too – for 47 days. For personal time and to ‘confer with headquarters’.\n",
      "\n",
      "“For nearly the entire summer of 2002, Abu Zubaydah was kept in isolation. That was valuable lost time, and that doesn’t square with claims about the ‘ticking bomb scenarios’ that were the basis for America’s enhanced interrogation program, or with the commitment to getting life-saving, actionable intelligence from valuable detainees. The techniques were justified by those who said Zubaydah ‘stopped all cooperation’ around the time my fellow FBI agent and I left. If Zubaydah was in isolation the whole time, that’s not really a surprise.\n",
      "\n",
      "“One of the hardest things we struggled to make sense of, back then, was why U.S. officials were authorizing harsh techniques when our interrogations were working and their harsh techniques weren’t. The answer, as the long-awaited Senate Intelligence Committee Report now makes clear, is that the architects of the program were taking credit for our success, from the unmasking of Khalid Sheikh Mohammed as the mastermind of 9/11 to the uncovering of the ‘dirty bomber’ Jose Padilla. The claims made by government officials for years about the efficacy of ‘enhanced interrogation’, in secret memos and in public, are false. ‘Enhanced interrogation’ doesn’t work.”\n",
      "\n",
      "The former CIA officers who have collaborated on this latest attempt to whitewash the historical record that they embraced and facilitated torture by Americans, are counting on the laziness of the press and the American public. As long as no one takes time to actually read the extensively footnoted and documented report by the Senate Intelligence Committee, then it is easy to buy into the fantasy that the CIA officers are simply victims of a political vendetta.\n",
      "\n",
      "These officers are also counting on a segment of the American people – repeatedly identified in polling results – that continues to believe torture works. Such people have no proof that it works (because there is none that it works consistently and effectively), they simply believe it instinctively or because of people such as this book’s authors’ arguments to that effect.\n",
      "\n",
      "That is why it is so important that the truth be told and this book and its arguments be debunked. Americans must learn the realities of torture – that it rarely if ever works, that it dehumanizes the torturer as well as the tortured, that it increases the numbers and hostility of our opponents while providing no benefit, and that it seriously diminishes America’s reputation in the world and thus its power. Torture is wrong and the men who wrote this book are wrong.\n",
      "\n",
      "The book, Rebuttal, is a new incarnation of the lie extolling the efficacy of torture. In the immediate aftermath of the attacks on Sept. 11, 2001, a time of perceived crisis and palpable fear, the leaders of the CIA decided to ignore international and domestic law. They chose to discard the moral foundations of our Republic and, using the same justifications that authoritarian regimes have employed for attacking enemies, and embarked willingly on a course of action that embraced practices that in earlier times the United States had condemned and punished as a violation of U.S. laws and fundamental human rights.\n",
      "\n",
      "As former intelligence officers, we are compelled by conscience to denounce the actions and words of our former colleagues. In their minds they have found a way to rationalize and justify torture. We say there is no excuse; there is no justification. The heart of good intelligence work — whether collection or analysis — is based in the pursuit of truth, not the fabrication of a lie.\n",
      "\n",
      "It is to this end that we reiterate that no threat, no matter how grave, should serve to justify inhuman behavior and immoral conduct or torture conducted by Americans.\n",
      "\n",
      "For the Steering Group, Veteran Intelligence Professionals for Sanity (VIPS)\n",
      "\n",
      "Fulton Armstrong, National Intelligence Officer for Latin America (ret.)\n",
      "\n",
      "William Binney, former Technical Director, World Geopolitical & Military Analysis, NSA; co-founder, SIGINT Automation Research Center (ret.)\n",
      "\n",
      "Tony Camerino, former Air Force and Air Force Reserves, a senior interrogator in Iraq and author of How to Break a Terrorist under pseudonym Matthew Alexander\n",
      "\n",
      "Thomas Drake, former Senior Executive, NSA\n",
      "\n",
      "Daniel Ellsberg, former State Department and Defense Department Official (VIPS Associate)\n",
      "\n",
      "Philip Giraldi, CIA, Operations Officer (ret.)\n",
      "\n",
      "Matthew Hoh, former Capt., USMC, Iraq & Foreign Service Officer, Afghanistan (associate VIPS)\n",
      "\n",
      "Larry C Johnson, CIA & State Department (ret.)\n",
      "\n",
      "Michael S. Kearns, Captain, USAF Intelligence Agency (Retired), ex Master SERE Instructor\n",
      "\n",
      "John Kiriakou, Former CIA Counterterrorism Officer\n",
      "\n",
      "Karen Kwiatkowski, Lt. Col., US Air Force (ret.)\n",
      "\n",
      "Edward Loomis, NSA, Cryptologic Computer Scientist (ret.)\n",
      "\n",
      "David MacMichael, National Intelligence Council (ret.)\n",
      "\n",
      "James Marcinkowski, Attorney, former CIA Operations Officer\n",
      "\n",
      "Ray McGovern, former US Army infantry/intelligence officer & CIA analyst (ret.)\n",
      "\n",
      "Elizabeth Murray, Deputy National Intelligence Officer for Middle East (ret.)\n",
      "\n",
      "Todd Pierce, MAJ, US Army Judge Advocate (ret.)\n",
      "\n",
      "Scott Ritter, former Maj., USMC, former UN Weapon Inspector, Iraq\n",
      "\n",
      "Coleen Rowley, Division Counsel & Special Agent, FBI (ret.)\n",
      "\n",
      "Ali Soufan, former FBI Special Agent\n",
      "\n",
      "Peter Van Buren, U.S. Department of State, Foreign Service Officer (ret.) (associate VIPS)\n",
      "\n",
      "Lawrence Wilkerson, Colonel (USA, ret.), Distinguished Visiting Professor, College of William and Mary\n",
      "\n",
      "Valerie Plame Wilson, CIA Operations Officer (ret.)\n",
      "\n",
      "Ann Wright, U.S. Army Reserve Colonel (ret) and former U.S. Diplomat\n"
     ]
    }
   ],
   "source": [
    "# OpenWebText2 dataset\n",
    "owt2 = load_dataset(\"Skylion007/openwebtext\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "print(\"OpenWebText2 dataset loaded.\")\n",
    "print(\"Dataset size in GB:\", owt2.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(owt2))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(owt2[random.randint(0, len(owt2)-1)]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Q&A data to improve the model's ability to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A dataset loaded.\n",
      "dataset size in mb: 46.480509757995605\n",
      "Number of entries: 120959\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "What was the outcome of the British Cabinet's discussions on a draft declaration concerning Palestine in 1917? \n",
      " The discussions led to the creation of Balfour's declaration, drafting of which involved Rothschild, Weizmann, and other key figures within the British and Zionist groups, culminating in a final version in 1917.\n"
     ]
    }
   ],
   "source": [
    "q_a1 = load_dataset(\"agentlans/text-sft-questions-answers-only\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Q&A dataset loaded.\")\n",
    "print(\"dataset size in mb:\", q_a1.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(q_a1))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(q_a1)-1)\n",
    "print(q_a1[index]['question'][:500], \"\\n\", q_a1[index]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit Instruct dataset loaded.\n",
      "dataset size in gb: 0.09901080373674631\n",
      "Number of entries: 84784\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Why isn't HPV vaccine recommended for adults past a certain age? I was recently looking over the [CDC recommended immunization schedule for adults](https://www.cdc.gov/vaccines/schedules/downloads/adult/adult-combined-schedule.pdf) and I noticed a couple of oddities. Most notably is the HPV vaccine. Why isn't this recommended for males past age 21 and for females past age 26? Is there a biological reason that people become less susceptible as they age? On a related note, why is 1957 the cutoff birth year for MMR vaccine?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " '\\n',\n",
       " 'No, there’s no biological reason. They don’t recommend the vaccine past a certain age because they assume that by that age that both men and women have had enough sex where they have already been exposed to the virus and wouldn’t benefit from the vaccine. \\n\\nAlso, there’s a cutoff age for the MMR vaccine because the original vaccine came out in ‘63, meaning people who were kids before that year were most likely exposed to measles’s and have life long immunity due to the antibodies already being p')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#euclaise/reddit-instruct\n",
    "reddit_instruct = load_dataset(\"euclaise/reddit-instruct\", split='train', cache_dir=CACHE_DIR)\n",
    "# reddit_instruct = load_dataset(\"Felladrin/ChatML-reddit-instruct-curated\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Reddit Instruct dataset loaded.\")\n",
    "print(\"dataset size in gb:\", reddit_instruct.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(reddit_instruct))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(reddit_instruct)-1)\n",
    "print(reddit_instruct[index]['post_title'][:500], reddit_instruct[index]['post_text'][:500]), \"\\n\", reddit_instruct[index]['comment_text'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca dataset loaded.\n",
      "dataset size in mb: 44.06797695159912\n",
      "Number of entries: 52002\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Rewrite the sentence to reveal the metaphor. \n",
      " The sun was shining brightly in the clear blue sky, as bright as a golden coin.\n"
     ]
    }
   ],
   "source": [
    "# tatsu-lab/alpaca ( for Q&A fine-tuning )\n",
    "alpaca = load_dataset(\"tatsu-lab/alpaca\", split='train')\n",
    "print(\"Alpaca dataset loaded.\")\n",
    "print(\"dataset size in mb:\", alpaca.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(alpaca))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(alpaca)-1)\n",
    "print(alpaca[index]['instruction'][:500], \"\\n\", alpaca[index]['output'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project i use tiktoken for the tokenizer, as it is the same tokenizer used by OpenAI for their models.\n",
    "\n",
    "I use the \"gpt2\" encoding which is a byte pair encoding (BPE) tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"rob-tokenizer\",\n",
    "    pat_str=tokenizer_base._pat_str,\n",
    "    mergeable_ranks=tokenizer_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **tokenizer_base._special_tokens,\n",
    "        \"<|im_start|>\": 100264,\n",
    "        \"<|im_end|>\": 100265,\n",
    "        \"<|pad|>\": 0,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the byte pair encoding tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2061, 318, 24207, 1616, 2587, 30, 314, 2342, 257, 7684, 286, 1097, 5861, 290, 484, 1561, 546, 275, 32512, 7021, 290, 884, 11, 1312, 373, 11263, 644, 275, 32512, 318, 290, 1312, 18548, 1064, 597, 2562, 7468, 284, 644, 340, 318, 24207, 1616, 318, 655, 262, 1438, 329, 257, 16058, 286, 6147, 13, 554, 262, 29393, 995, 340, 338, 1690, 973, 355, 257, 1790, 1021, 329, 3354, 326, 547, 3235, 1389, 503, 286, 257, 1263, 2512, 286, 2587, 11, 355, 6886, 284, 11721, 3350, 654, 810, 44030, 6147, 318, 19036, 656, 257, 15936, 12070, 503, 286, 9629, 6147, 13, 7080, 3191, 318, 517, 5789, 329, 1588, 17794, 475, 340, 460, 779, 1365, 3081, 286, 21782, 290, 318, 4577, 284, 787, 329, 4833, 17794, 588, 3234, 3354, 13]\n",
      "Decoded text:\n",
      "What is Billet material? I watch a bunch of car videos and they talk about billet blocks and such, i was wondering what billet is and i cant find any easy explanation to what it is Billet is just the name for a chunk of metal. In the automotive world it's often used as a short hand for parts that were machined out of a big block of material, as opposed to cheaper castings where molten metal is poured into a mold pressed out of sheet metal. Machining is more expensive for large quantities but it can use better quality of metals and is easier to make for smaller quantities like race parts.\n",
      "Sample text length in characters: 594\n",
      "Sample text length in tokens: 127\n"
     ]
    }
   ],
   "source": [
    "# test of tokenizer on reddit_instruct\n",
    "sample_text = reddit_instruct[0]['post_title'] + \" \" + reddit_instruct[0]['post_text'] + \" \" + reddit_instruct[0]['comment_text']\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(tokens)\n",
    "print(\"Decoded text:\")\n",
    "print(tokenizer.decode(tokens)) \n",
    "print(f\"Sample text length in characters: {len(sample_text)}\")\n",
    "print(f\"Sample text length in tokens: {len(tokens)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting datasets functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 26209380\n",
      "Finetune dataset size: 257745\n",
      "Example entry from train dataset:\n",
      "{'id': '51408024', 'url': 'https://en.wikipedia.org/wiki/Josef%20Sch%C3%A4ffer', 'title': 'Josef Schäffer', 'text': 'Josef Schäffer (born July 2, 1891 in Moravia) was an Austrian track and field athlete who competed in the 1912 Summer Olympics. He competed in the decathlon, shot put, discus throw and two-handed discus throw. He finished tenth in the decathlon, throwing the second-furthest in the discus on his way to his score of 6568.585. In the shot put, he finished thirteenth. In the discus throw, he only managed to come twenty-ninth in the regular discus throw, but came sixteenth in the two-handed discus.\\n\\nSee also \\n Austria at the 1912 Summer Olympics\\n\\nReferences\\n\\nExternal links\\n \\n\\n1891 births\\nAustrian decathletes\\nAustrian shot putters\\nAustrian male discus throwers\\nOlympic athletes for Austria\\nAthletes (track and field) at the 1912 Summer Olympics\\nYear of death missing\\nOlympic decathletes', 'story': None, 'topic': None, 'theme': None, 'style': None, 'feature': None, 'grammar': None, 'persona': None, 'initial_word_type': None, 'initial_letter': None, 'word_count': None, 'character_count': None, 'num_paragraphs': None, 'avg_word_length': None, 'avg_sentence_length': None, 'flesch_reading_ease': None, 'flesch_kincaid_grade': None, 'dale_chall_readability_score': None, 'num_stories_in_completion': None, 'expected_num_stories_in_completion': None, 'generation_id': None, 'model': None, 'dump': None, 'file_path': None, 'language': None, 'language_score': None, 'token_count': None, 'score': None, 'int_score': None}\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_train_dataset = concatenate_datasets([  \n",
    "    wiki_en,\n",
    "    stories,\n",
    "    fineweb_edu,\n",
    "    owt2,  \n",
    "])  \n",
    "\n",
    "combined_finetune_dataset = concatenate_datasets([\n",
    "    q_a1,\n",
    "    reddit_instruct,\n",
    "    alpaca,\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_dataset = combined_train_dataset.shuffle(seed=42)\n",
    "finetune_dataset = combined_finetune_dataset.shuffle(seed=42)\n",
    "print(f\"Train dataset size: {len(combined_train_dataset)}\")\n",
    "print(f\"Finetune dataset size: {len(combined_finetune_dataset)}\")\n",
    "\n",
    "# Exemple \n",
    "\n",
    "print(\"Example entry from train dataset:\")\n",
    "index = random.randint(0, len(train_dataset)-1)\n",
    "print(train_dataset[index])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the dataloader from the \"LLMs from scratch\" repository. But adapted for multi-row text arrays.\n",
    "\n",
    "https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/dataloader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Dataset of the combined hugginface entries\n",
    "            tokenizer: the initiatokenizer to process text\n",
    "            max_length: Context window size\n",
    "        \"\"\"\n",
    "        self.data = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.input_tokens = []\n",
    "        self.target_tokens = []\n",
    "\n",
    "        self.pad_token_id = 0         # <|pad|>\n",
    "        self.bos_token_id = 100264    # <|im_start|>\n",
    "        self.eos_token_id = 100265    # <|im_end|>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Raw text\n",
    "        \n",
    "        # Data format handling\n",
    "        entry = self.data[idx]\n",
    "        if 'text' in entry:\n",
    "            text = entry['text']\n",
    "        elif 'story' in entry:\n",
    "            text = entry['story']\n",
    "        elif 'question' in entry and 'answer' in entry: \n",
    "            text = \"User: \" + entry['question'] + \" Assistant:\" + entry['answer']\n",
    "        elif 'post_title' in entry and 'post_text' in entry and 'comment_text' in entry:\n",
    "            text = \"User: \" + entry['post_title'] + \" Assistant:\" + entry['post_text'] + \" \" + entry['comment_text']\n",
    "        elif 'instruction' in entry and 'output' in entry:\n",
    "            text = \"User: \" + entry['instruction'] + \" Assistant:\" + entry['output']\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data entry format\")\n",
    "        \n",
    "        text = str(text) # Ensure text is a string\n",
    "        #print(text)\n",
    "\n",
    "        # Adding Start and End tokens\n",
    "        text = \"<|im_start|>\" + text + \"<|im_end|>\" \n",
    "\n",
    "        # Tokenization\n",
    "        tokens = self.tokenizer.encode(text, allowed_special=\"all\")\n",
    "\n",
    "        # Truncation\n",
    "        tokens = tokens[:self.max_length] #Data is loost here ( fix later with sliding window )\n",
    "\n",
    "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)  # All tokens except last\n",
    "        labels = torch.tensor(tokens[1:], dtype=torch.long)      # All tokens except first\n",
    "\n",
    "\n",
    "        #Padding \n",
    "        padding_length = self.max_length - len(tokens)\n",
    "        if padding_length > 0:\n",
    "            input_ids = torch.cat([input_ids, torch.full((padding_length,), self.pad_token_id)])\n",
    "            labels = torch.cat([labels, torch.full((padding_length,), -100)])\n",
    "\n",
    "\n",
    "        attention_mask = (input_ids != self.pad_token_id).long() # 1 for real tokens, 0 for padding\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 26209380\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GPTDataset(combined_train_dataset, tokenizer, max_length=512)\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test of a entry from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Test of a entry from dataloader\n",
      "{'input_ids': tensor([[100264,  20588,    642,  ...,      0,      0,      0],\n",
      "        [100264,    818,    428,  ...,      0,      0,      0],\n",
      "        [100264,   1722,   8536,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [100264,    818,  36864,  ...,      0,      0,      0],\n",
      "        [100264,  14478,   5451,  ...,      0,      0,      0],\n",
      "        [100264,  14040,    271,  ...,    198,  23675,   4796]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[20588,   642,    11,  ...,  -100,  -100,  -100],\n",
      "        [  818,   428,  4326,  ...,  -100,  -100,  -100],\n",
      "        [ 1722,  8536,   287,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [  818, 36864, 29803,  ...,  -100,  -100,  -100],\n",
      "        [14478,  5451,   318,  ...,  -100,  -100,  -100],\n",
      "        [14040,   271,  1973,  ..., 23675,  4796,  7381]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Test of a entry from dataloader\")\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 512,\n",
    "    \"emb_dim\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 8,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources \n",
    "\n",
    "### Principal references: \n",
    "- https://arxiv.org/abs/2005.14165 (GPT-3 paper)\n",
    "- https://arxiv.org/abs/2002.05709 (Attention is all you need paper)\n",
    "- Build a Large Language Model (from scratch) by Sebastian Raschka\n",
    "\n",
    "### About Padding tokens in Language Modeling\n",
    "- https://arxiv.org/html/2510.01238v1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
