{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet GPT - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used to train a small language model using PyTorch from scratch. The model is inspired by the GPT architecture.\n",
    "\n",
    "\n",
    "#### Hardware\n",
    "- RTX3060 12GB VRAM\n",
    "- AMD Ryzen 7 5800X 8-Core\n",
    "- 32GB RAM\n",
    "- Ubuntu 22.04 LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CACHE_DIR = \"/media/rob/RobsDisk/cache_data_llm\"\n",
    "os.environ['HF_HOME'] = CACHE_DIR\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_DIR, \"datasets\")\n",
    "os.environ['HF_METRICS_CACHE'] = os.path.join(CACHE_DIR, \"metrics\")\n",
    "os.environ['HF_MODULES_CACHE'] = os.path.join(CACHE_DIR, \"modules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import random, math\n",
    "\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Common knowledge datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### English Wikipedia crawled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Wikipedia dataset loaded.\n",
      "dataset size in gb: 18.812774107791483\n",
      "Number of entries: 6407814\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "is a Japanese anime television series that adapted several Agatha Christie stories about Hercule Poirot and Miss Marple. A new character named Maybelle West, Miss Marple's great-niece, who becomes Poirot's junior assistant, is used to connect the two detectives.\n",
      "\n",
      "The series was broadcast from 4 July 2004 to 15 May 2005 on NHK, and continues to be shown in re-runs on NHK and other networks in Japan. The series was adapted as manga under the same title, which was released in 2004 and 2005.\n",
      "\n",
      "Adaptation \n",
      "\n",
      "The TV series is a generally faithful adaptation of the original stories given the time constraints (typically one 25-minute episode for a short story, four episodes for a novel). Despite being a modern Japanese adaptation, the original (mainly English) locations and time period are retained. The most obvious story change is the addition of Mabel West and her pet duck, Oliver. However, apart from her soliloquies, most of her lines are taken from the incidental dialogue of other characters in the original stories, so her presence does not materially alter the plot development. \n",
      "\n",
      "Other changes include Inspector Japp becoming \"Inspector Sharpe\", possibly due to the derogatory implications attached to the word \"jap\". The significance of some details which rely on English idioms is changed, for example when a dying uncle taps his eye in Strange Jest, this was originally a reference to the saying All my eye and Betty Martin, but in the anime it becomes a reference to a stamp in which an angel appears to be winking (however the clue still achieves the same end).\n",
      "\n",
      "Characters \n",
      "\n",
      "Hercule Poirot: \n",
      "Jane Marple: \n",
      "Mabel West: \n",
      "Oliver West: \n",
      "Miss Lemon: \n",
      "Hastings: \n",
      "Inspector Sharpe:\n",
      "\n",
      "Media\n",
      "\n",
      "Anime \n",
      "\n",
      "The anime was produced by Oriental Light and Magic for NHK. The series was directed by Naohito Takahashi with music by Toshiyuki Watanabe and character designs by Sayuri Ichiishi. The opening theme is \"Lucky Girl ni Hanataba wo\" and the ending theme is \"Wasurenaide,\" both performed by Tatsuro Yamashita. The 39 episodes were initially broadcast in Japan on NHK stations from 4 July 2004 to 15 May 2005. The series was released on seven DVDs.\n",
      "\n",
      "DVD \n",
      "Region 2, NTSC,\n",
      "Language: Japanese,\n",
      "subtitle: No\n",
      "\n",
      "Manga \n",
      "Three volumes of the manga adaptation of the television series were released in 2004 and 2005 by NHK Publishing, the publishing division of NHK. The adaptations were written by  (volumes 1 and 3) and  (volume 2), and illustrated by .\n",
      "\n",
      "References\n",
      "\n",
      "External links \n",
      " Official site at NHK \n",
      " \n",
      " Series listing at Aga Search \n",
      "\n",
      "2004 anime television series debuts\n",
      "2004 manga\n",
      "Detective anime and manga\n",
      "Detective television series\n",
      "Films based on works by Agatha Christie\n",
      "NHK original programming\n",
      "Odex\n",
      "OLM, Inc.\n"
     ]
    }
   ],
   "source": [
    "# English Wikipedia crawled dataset\n",
    "# path to store the dataset cache: /Volumes/RobertsDisk\n",
    "wiki_en = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split='train', cache_dir=CACHE_DIR) \n",
    "print(\"English Wikipedia dataset loaded.\")\n",
    "print(\"dataset size in gb:\", wiki_en.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(wiki_en))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(wiki_en[random.randint(0, len(wiki_en)-1)]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple stories dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple stories dataset loaded.\n",
      "dataset size in mb: 3030.012650489807\n",
      "Number of entries: 2115696\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "A radiant star shone brightly in the night sky over a sleepy town. Each night, people would look up and make wishes. One day, a jealous star saw the shining star and decided to cover it with dark clouds. The people became sad, unable to see their favorite star. They gathered and spoke of their loss. \n",
      "\n",
      "Then, a brave child shouted, \"We will not let one cloud take our joy!\" The townsfolk began to sing, their voices rising high. The clouds slowly parted, revealing the star. It sparkled brighter than ever. The jealous star learned that love and hope can break even the darkest clouds. The town celebrated their star, and their wishes flowed freely once again.\n"
     ]
    }
   ],
   "source": [
    "# Simple stories dataset\n",
    "stories = load_dataset(\"SimpleStories/SimpleStories\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Simple stories dataset loaded.\")\n",
    "print(\"dataset size in mb:\", stories.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(stories))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(stories[random.randint(0, len(stories)-1)]['story'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FineWeb-Edu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineWeb-Edu is ready.\n",
      "dataset size in gb: 45.730818568728864\n",
      "Number of entries: 9672101\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "June 4, 2013 — The chemical secrets of a concrete Roman breakwater that has spent the last 2,000 years submerged in the Mediterranean Sea have been uncovered by an international team of researchers led by Paulo Monteiro of the U.S. Department of Energy's Lawrence Berkeley National Laboratory (Berkeley Lab), a professor of civil and environmental engineering at the University of California, Berkeley.\n",
      "Analysis of samples provided by team member Marie Jackson pinpointed why the best Roman concrete was superior to most modern concrete in durability, why its manufacture was less environmentally damaging -- and how these improvements could be adopted in the modern world.\n",
      "\"It's not that modern concrete isn't good -- it's so good we use 19 billion tons of it a year,\" says Monteiro. \"The problem is that manufacturing Portland cement accounts for seven percent of the carbon dioxide that industry puts into the air.\"\n",
      "Portland cement is the source of the \"glue\" that holds most modern concrete together. But making it releases carbon from burning fuel, needed to heat a mix of limestone and clays to 1,450 degrees Celsius (2,642 degrees Fahrenheit) -- and from the heated limestone (calcium carbonate) itself. Monteiro's team found that the Romans, by contrast, used much less lime and made it from limestone baked at 900˚ C (1,652˚ F) or lower, requiring far less fuel than Portland cement.\n",
      "Cutting greenhouse gas emissions is one powerful incentive for finding a better way to provide the concrete the world needs; another is the need for stronger, longer-lasting buildings, bridges, and other structures.\n",
      "\"In the middle 20th century, concrete structures were designed to last 50 years, and a lot of them are on borrowed time,\" Monteiro says. \"Now we design buildings to last 100 to 120 years.\" Yet Roman harbor installations have survived 2,000 years of chemical attack and wave action underwater.\n",
      "How the Romans did it\n",
      "The Romans made concrete by mixing lime and volcanic rock. For underwater structures, lime and volcanic ash were mixed to form mortar, and this mortar and volcanic tuff were packed into wooden forms. The seawater instantly triggered a hot chemical reaction. The lime was hydrated -- incorporating water molecules into its structure -- and reacted with the ash to cement the whole mixture together.\n",
      "Pozzuoli Bay defines the northwestern region of the Bay of Naples. The concrete sample examined at the Advanced Light Source by Berkeley researchers, BAI.06.03, is from the harbor of Baiae, one of many ancient underwater sites in the region. Black lines indicate caldera rims, and red areas are volcanic craters. (Click on image for best resolution.)\n",
      "Descriptions of volcanic ash have survived from ancient times. First Vitruvius, an engineer for the Emperor Augustus, and later Pliny the Elder recorded that the best maritime concrete was made with ash from volcanic regions of the Gulf of Naples (Pliny died in the eruption of Mt. Vesuvius that buried Pompeii), especially from sites near today's seaside town of Pozzuoli. Ash with similar mineral characteristics, called pozzolan, is found in many parts of the world.\n",
      "Using beamlines 18.104.22.168, 22.214.171.124, 12.2.2 and 12.3.2 at Berkeley Lab's Advanced Light Source (ALS), along with other experimental facilities at UC Berkeley, the King Abdullah University of Science and Technology in Saudi Arabia, and the BESSY synchrotron in Germany, Monteiro and his colleagues investigated maritime concrete from Pozzuoli Bay. They found that Roman concrete differs from the modern kind in several essential ways.\n",
      "One is the kind of glue that binds the concrete's components together. In concrete made with Portland cement this is a compound of calcium, silicates, and hydrates (C-S-H). Roman concrete produces a significantly different compound, with added aluminum and less silicon. The resulting calcium-aluminum-silicate-hydrate (C-A-S-H) is an exceptionally stable binder.\n",
      "At ALS beamlines 126.96.36.199 and 188.8.131.52, x-ray spectroscopy showed that the specific way the aluminum substitutes for silicon in the C-A-S-H may be the key to the cohesion and stability of the seawater concrete.\n",
      "Another striking contribution of the Monteiro team concerns the hydration products in concrete. In theory, C-S-H in concrete made with Portland cement resembles a combination of naturally occurring layered minerals, called tobermorite and jennite. Unfortunately these ideal crystalline structures are nowhere to be found in conventional modern concrete.\n",
      "Tobermorite does occur in the mortar of ancient seawater concrete, however. High-pressure x-ray diffraction experiments at ALS beamline 12.2.2 measured its mechanical properties and, for the first time, clarified the role of aluminum in its crystal lattice. Al-tobermorite (Al for aluminum) has a greater stiffness than poorly crystalline C-A-S-H and provides a model for concrete strength and durability in the future.\n",
      "Finally, microscopic studies at ALS beamline 12.3.2 identified the other minerals in the Roman samples. Integration of the results from the various beamlines revealed the minerals' potential applications for high-performance concretes, including the encapsulation of hazardous wastes.\n",
      "Lessons for the future\n",
      "Environmentally friendly modern concretes already include volcanic ash or fly ash from coal-burning power plants as partial substitutes for Portland cement, with good results. These blended cements also produce C-A-S-H, but their long-term performance could not be determined until the Monteiro team analyzed Roman concrete.\n",
      "Their analyses showed that the Roman recipe needed less than 10 percent lime by weight, made at two-thirds or less the temperature required by Portland cement. Lime reacting with aluminum-rich pozzolan ash and seawater formed highly stable C‑A-S-H and Al-tobermorite, insuring strength and longevity. Both the materials and the way the Romans used them hold lessons for the future.\n",
      "\"For us, pozzolan is important for its practical applications,\" says Monteiro. \"It could replace 40 percent of the world's demand for Portland cement. And there are sources of pozzolan all over the world. Saudi Arabia doesn't have any fly ash, but it has mountains of pozzolan.\"\n",
      "Stronger, longer-lasting modern concrete, made with less fuel and less release of carbon into the atmosphere, may be the legacy of a deeper understanding of how the Romans made their incomparable concrete.\n",
      "This work was supported by King Abdullah University of Science and Technology, the Loeb Classical Library Foundation at Harvard University, and DOE's Office of Science, which also supports the Advanced Light Source. Samples of Roman maritime concrete were provided by Marie Jackson and by the ROMACONS drilling program, sponsored by CTG Italcementi of Bergamo, Italy.\n",
      "Tell us what you think of Chemistry 2011 -- we welcome both positive and negative comments. Have any problems using the site? Questions?\n"
     ]
    }
   ],
   "source": [
    "fineweb_edu = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\",  split='train', cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"FineWeb-Edu is ready.\")\n",
    "print(\"dataset size in gb:\", fineweb_edu.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(fineweb_edu))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(fineweb_edu[random.randint(0, len(fineweb_edu)-1)]['text']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenWebText2 dataset loaded.\n",
      "Dataset size in GB: 37.03822539001703\n",
      "Number of entries: 8013769\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "THE skipper and navigation officer of the cargo ship Rena, which smashed into a reef, causing New Zealand's worst maritime environmental disaster, have both been jailed for seven months.\n",
      "\n",
      "Captain Mauro Balomaga, 44, and navigation officer Leonil Relon, 37, both Filipinos, were today sentenced in Tauranga District Court, in the North Island, on a raft of charges laid after the 236-metre, 37,000-tonne cargo ship struck Astrolabe Reef off Tauranga in the early hours of October 5 last year.\n",
      "\n",
      "It spilled about 360 tonnes of heavy fuel oil into the sea, which washed up on local beaches and killed wildlife.\n",
      "\n",
      "Containers were washed overboard and clean-up crews are still picking up debris from the wreck, which broke apart in January after being pounded by heavy seas.\n",
      "\n",
      "The disaster, which sparked a massive anti-pollution response, ruined the summer for many in the Bay of Plenty as beaches were spoiled and tourism suffered.\n",
      "\n",
      "Both men, whose name suppression was lifted on Friday, had earlier pleaded guilty to wilfully attempting to pervert the course of justice by altering the ship's documents after it was grounded.\n",
      "\n",
      "They had been facing maximum penalties of seven years in prison and a $300,000 fine. They were not fined.\n",
      "\n",
      "The court was told crew members were trying to get into Tauranga Harbour as quickly as they could and broke the \"basic principles of navigation\".\n",
      "\n",
      "In trying to take a shortcut they changed course but the diversion was not noted on the chart and no effort was made to project the ship's course.\n",
      "\n",
      "About 10 minutes before the ship grounded, the Astrolabe Reef appeared on the radar, but Balomaga believed it was a small vessel or a false echo, and he took no evasive action.\n",
      "\n",
      "Crown prosecutor Rob Ronayne said each man's actions displayed \"incompetence verging on recklessness\".\n",
      "\n",
      "After the grounding Balomaga instructed Relon to alter the charts, computer and GPS log to make it appear the Rena was planning to pass clear of the reef.\n",
      "\n",
      "Maritime New Zealand director Keith Manch, in a statement welcoming the sentences, said that the two officers had caused genuine confusion for investigators trying to piece together events.\n",
      "\n",
      "\"It is vital that when these types of events do take place, we can find out how and why they have happened to help prevent such an event happening again,\" he said.\n",
      "\n",
      "Mr Manch said the sentencing was a milestone in the continuing clean-up effort, which earlier this year was estimated to have cost $NZ130 million ($101.18 million).\n",
      "\n",
      "More than 800 of the 1368 containers aboard the ship have so far been recovered and most of the 1700 tonnes of heavy fuel pumped off.\n",
      "\n",
      "The ship's owner, Greece-based Daina Shipping, has also been charged with discharging harmful substances and its case has been put off until July.\n",
      "\n",
      "Originally published as NZ ship disaster crew get 7 months' jail\n"
     ]
    }
   ],
   "source": [
    "# OpenWebText2 dataset\n",
    "owt2 = load_dataset(\"Skylion007/openwebtext\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "print(\"OpenWebText2 dataset loaded.\")\n",
    "print(\"Dataset size in GB:\", owt2.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(owt2))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(owt2[random.randint(0, len(owt2)-1)]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Q&A data to improve the model's ability to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A dataset loaded.\n",
      "dataset size in mb: 46.480509757995605\n",
      "Number of entries: 120959\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "How can organizations benchmark themselves against the practices of CESAI? \n",
      " Organizations can benchmark themselves against the practices of CESAI by analyzing their marketing and business strategies, their word of mouth advertising strategy, and their customer-focused culture. This allows them to identify areas for improvement and alignment with the guidelines and philosophies of CESAI.\n"
     ]
    }
   ],
   "source": [
    "q_a1 = load_dataset(\"agentlans/text-sft-questions-answers-only\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Q&A dataset loaded.\")\n",
    "print(\"dataset size in mb:\", q_a1.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(q_a1))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(q_a1)-1)\n",
    "print(q_a1[index]['question'][:500], \"\\n\", q_a1[index]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit Instruct dataset loaded.\n",
      "dataset size in gb: 0.09901080373674631\n",
      "Number of entries: 84784\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "[40K] So, Horrus stabs the Emperor, the Emperor kills him.. What are the events immediately proceeding this? He's standing there in a battlefield bleeding and what not I guess, Horrus is dead, so his men just sorta... stop fighting? So who suddenly took charge and got him back to the throne (which I assume was nowhere near the battlefield, being at the very bottom of a palace the size of Europe...) Who started setting up contingency plans in the case that at that moment he *died*.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " '\\n',\n",
       " \"They're on board Horus's battleship, the Emperor has just wiped Horus's soul from existence and is lying, nearly dead, in the midst of horrible carnage. The bodies of Ollanius Pius and Sanguinus lie near him, and the Emperor weeps for instant of sanity he saw in his son's eyes before he killed him.\\n\\nRogal Dorn enters, having split away from the Emperor shortly after they teleported aboard the battleship. The Emperor, realizing that he is dying, and knowing that he needs to be alive to guide the \")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#euclaise/reddit-instruct\n",
    "reddit_instruct = load_dataset(\"euclaise/reddit-instruct\", split='train', cache_dir=CACHE_DIR)\n",
    "# reddit_instruct = load_dataset(\"Felladrin/ChatML-reddit-instruct-curated\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Reddit Instruct dataset loaded.\")\n",
    "print(\"dataset size in gb:\", reddit_instruct.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(reddit_instruct))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(reddit_instruct)-1)\n",
    "print(reddit_instruct[index]['post_title'][:500], reddit_instruct[index]['post_text'][:500]), \"\\n\", reddit_instruct[index]['comment_text'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca dataset loaded.\n",
      "dataset size in mb: 44.06797695159912\n",
      "Number of entries: 52002\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Summarize an article titled \"Advantages of Owning a Pet\" \n",
      " Owning a pet can bring a multitude of benefits to an individual, both physically and mentally. From physical health benefits such as reduced cholesterol levels to mental wellbeing improvements such as increased levels of happiness, the advantages of owning a pet are significant.\n"
     ]
    }
   ],
   "source": [
    "# tatsu-lab/alpaca ( for Q&A fine-tuning )\n",
    "alpaca = load_dataset(\"tatsu-lab/alpaca\", split='train')\n",
    "print(\"Alpaca dataset loaded.\")\n",
    "print(\"dataset size in mb:\", alpaca.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(alpaca))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(alpaca)-1)\n",
    "print(alpaca[index]['instruction'][:500], \"\\n\", alpaca[index]['output'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project i use tiktoken for the tokenizer, as it is the same tokenizer used by OpenAI for their models.\n",
    "\n",
    "I use the \"gpt2\" encoding which is a byte pair encoding (BPE) tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"rob-tokenizer\",\n",
    "    pat_str=tokenizer_base._pat_str,\n",
    "    mergeable_ranks=tokenizer_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **tokenizer_base._special_tokens,\n",
    "        \"<|im_start|>\": 50257,\n",
    "        \"<|im_end|>\": 50258,\n",
    "        \"<|pad|>\": 50259,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the byte pair encoding tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2061, 318, 24207, 1616, 2587, 30, 314, 2342, 257, 7684, 286, 1097, 5861, 290, 484, 1561, 546, 275, 32512, 7021, 290, 884, 11, 1312, 373, 11263, 644, 275, 32512, 318, 290, 1312, 18548, 1064, 597, 2562, 7468, 284, 644, 340, 318, 24207, 1616, 318, 655, 262, 1438, 329, 257, 16058, 286, 6147, 13, 554, 262, 29393, 995, 340, 338, 1690, 973, 355, 257, 1790, 1021, 329, 3354, 326, 547, 3235, 1389, 503, 286, 257, 1263, 2512, 286, 2587, 11, 355, 6886, 284, 11721, 3350, 654, 810, 44030, 6147, 318, 19036, 656, 257, 15936, 12070, 503, 286, 9629, 6147, 13, 7080, 3191, 318, 517, 5789, 329, 1588, 17794, 475, 340, 460, 779, 1365, 3081, 286, 21782, 290, 318, 4577, 284, 787, 329, 4833, 17794, 588, 3234, 3354, 13]\n",
      "Decoded text:\n",
      "What is Billet material? I watch a bunch of car videos and they talk about billet blocks and such, i was wondering what billet is and i cant find any easy explanation to what it is Billet is just the name for a chunk of metal. In the automotive world it's often used as a short hand for parts that were machined out of a big block of material, as opposed to cheaper castings where molten metal is poured into a mold pressed out of sheet metal. Machining is more expensive for large quantities but it can use better quality of metals and is easier to make for smaller quantities like race parts.\n",
      "Sample text length in characters: 594\n",
      "Sample text length in tokens: 127\n"
     ]
    }
   ],
   "source": [
    "# test of tokenizer on reddit_instruct\n",
    "sample_text = reddit_instruct[0]['post_title'] + \" \" + reddit_instruct[0]['post_text'] + \" \" + reddit_instruct[0]['comment_text']\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(tokens)\n",
    "print(\"Decoded text:\")\n",
    "print(tokenizer.decode(tokens)) \n",
    "print(f\"Sample text length in characters: {len(sample_text)}\")\n",
    "print(f\"Sample text length in tokens: {len(tokens)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting datasets functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 26209380\n",
      "Finetune dataset size: 257745\n",
      "Example entry from train dataset:\n",
      "{'id': None, 'url': None, 'title': None, 'text': 'Something about garbage trucks are simply beastly. Well, maybe a few things — their size, the stench, and the way they totally crush that garbage. So, electric garbage trucks bring some nice beastly power to the electric vehicle community. And I hope it explains why I got so excited when I saw news about such garbage trucks (and keep hearing their crushing power in my head).\\n\\nWord on the street is that Motiv Power Systems will soon provide the first U.S. all-electric Class 8 refuse truck. And it has just announced some of its vendor partners. “The garbage truck body will be a Loadmaster 20 cubic yard Excel-S series, provided by RNOW Inc under municipal contract with the City of Chicago. The chassis, manufactured by Crane Carrier, will be furnished by Cumberland Service Center another City of Chicago contracted dealer,” a press release regarding the new agreements states.\\n\\nThe first of these electric trucks will be put into service in Chicago. “Last November, Motiv was granted a contract up to $13.4 million to provide up to 20 EV refuse trucks for the City of Chicago. The scalability and flexibility of the Motiv electric Powertrain Control System (ePCS) made the company the most cost-effective choice for the exclusive 5-year contract.”\\n\\nOne of the interesting things about the Motiv garbage trucks is that they use “off the shelf” technology, and yet are the only trucks of their kind.\\n\\n“Currently the only technology of its kind in the trucking market, the ePCS uses off-the-shelf batteries and motors, which can be mixed and matched to fit the exact size and duty cycle of the electric truck needed. The ePCS can handle EV trucks from medium-duty to heavy-duty, weighing 15,000 lbs to 52,000 lbs. Research suggests the ePCS design approach cuts operating costs by 50 percent over an eight-year period. With its medium-duty pilot shuttle, Motiv reduced operating cost from 80 cents per mile ($0.80/mi) to 10 cents per mile ($0.10/mi).”\\n\\nI guess there just wasn’t the entrepreneurial drive to develop electric garbage trucks as there was to develop electric sports cars. I wonder why….', 'story': None, 'topic': None, 'theme': None, 'style': None, 'feature': None, 'grammar': None, 'persona': None, 'initial_word_type': None, 'initial_letter': None, 'word_count': None, 'character_count': None, 'num_paragraphs': None, 'avg_word_length': None, 'avg_sentence_length': None, 'flesch_reading_ease': None, 'flesch_kincaid_grade': None, 'dale_chall_readability_score': None, 'num_stories_in_completion': None, 'expected_num_stories_in_completion': None, 'generation_id': None, 'model': None, 'dump': None, 'file_path': None, 'language': None, 'language_score': None, 'token_count': None, 'score': None, 'int_score': None}\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_train_dataset = concatenate_datasets([  \n",
    "    wiki_en,\n",
    "    stories,\n",
    "    fineweb_edu,\n",
    "    owt2,  \n",
    "])  \n",
    "\n",
    "combined_finetune_dataset = concatenate_datasets([\n",
    "    q_a1,\n",
    "    reddit_instruct,\n",
    "    alpaca,\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_dataset = combined_train_dataset.shuffle(seed=42)\n",
    "finetune_dataset = combined_finetune_dataset.shuffle(seed=42)\n",
    "print(f\"Train dataset size: {len(combined_train_dataset)}\")\n",
    "print(f\"Finetune dataset size: {len(combined_finetune_dataset)}\")\n",
    "\n",
    "# Exemple \n",
    "\n",
    "print(\"Example entry from train dataset:\")\n",
    "index = random.randint(0, len(train_dataset)-1)\n",
    "print(train_dataset[index])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the dataloader from the \"LLMs from scratch\" repository. But adapted for multi-row text arrays.\n",
    "\n",
    "https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/dataloader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Dataset of the combined hugginface entries\n",
    "            tokenizer: the initiatokenizer to process text\n",
    "            max_length: Context window size\n",
    "        \"\"\"\n",
    "        self.data = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.input_tokens = []\n",
    "        self.target_tokens = []\n",
    "\n",
    "        self.pad_token_id = 50259         # <|pad|>\n",
    "        self.bos_token_id = 50257    # <|im_start|>\n",
    "        self.eos_token_id = 50258    # <|im_end|>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Raw text\n",
    "        \n",
    "        # Data format handling\n",
    "        entry = self.data[idx]\n",
    "        if 'text' in entry:\n",
    "            text = entry['text']\n",
    "        elif 'story' in entry:\n",
    "            text = entry['story']\n",
    "        elif 'question' in entry and 'answer' in entry: \n",
    "            text = \"User: \" + entry['question'] + \" Assistant:\" + entry['answer']\n",
    "        elif 'post_title' in entry and 'post_text' in entry and 'comment_text' in entry:\n",
    "            text = \"User: \" + entry['post_title'] + \" Assistant:\" + entry['post_text'] + \" \" + entry['comment_text']\n",
    "        elif 'instruction' in entry and 'output' in entry:\n",
    "            text = \"User: \" + entry['instruction'] + \" Assistant:\" + entry['output']\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data entry format\")\n",
    "        \n",
    "        text = str(text) # Ensure text is a string\n",
    "        #print(text)\n",
    "\n",
    "        # Adding Start and End tokens\n",
    "        text = \"<|im_start|>\" + text + \"<|im_end|>\" \n",
    "\n",
    "        # Tokenization\n",
    "        tokens = self.tokenizer.encode(text, allowed_special=\"all\")\n",
    "\n",
    "        # Truncation\n",
    "        tokens = tokens[:self.max_length] #Data is loost here ( fix later with sliding window )\n",
    "\n",
    "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)  # All tokens except last\n",
    "        labels = torch.tensor(tokens[1:], dtype=torch.long)      # All tokens except first\n",
    "\n",
    "\n",
    "        #Padding \n",
    "        padding_length = self.max_length - len(tokens)\n",
    "        if padding_length > 0:\n",
    "            input_ids = torch.cat([input_ids, torch.full((padding_length,), self.pad_token_id)])\n",
    "            labels = torch.cat([labels, torch.full((padding_length,), -100)])\n",
    "\n",
    "\n",
    "        attention_mask = (input_ids != self.pad_token_id).long() # 1 for real tokens, 0 for padding\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the configuration for the GPT model i am going to train. It is a smaller version of the GPT-2 model. \n",
    "\n",
    "- Context length: 512 tokens\n",
    "- Embedding dimension: 512\n",
    "- Number of attention heads: 8\n",
    "- Number of layers: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50260,\n",
    "    \"context_length\": 512, # max i could fit on my gpu\n",
    "    \"emb_dim\": 512,\n",
    "    \"number_heads\": 8,\n",
    "    \"number_layers\": 8,\n",
    "    \"drop_rate\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test of a entry from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 26209380\n"
     ]
    }
   ],
   "source": [
    "# Empty cuda cache and memory management\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_dataset = GPTDataset(combined_train_dataset, tokenizer, max_length=GPT_CONFIG[\"context_length\"])\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "\n",
    "batch_size = 12\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=2, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Test of a entry from dataloader\n",
      "{'input_ids': tensor([[50257,  2484,   789,  ...,   340,    13, 24006],\n",
      "        [50257, 14202, 50259,  ..., 50259, 50259, 50259],\n",
      "        [50257,    12, 36829,  ..., 50259, 50259, 50259],\n",
      "        ...,\n",
      "        [50257,  2061,   717,  ...,   428,   198,  5235],\n",
      "        [50257, 14202, 50259,  ..., 50259, 50259, 50259],\n",
      "        [50257, 13603,   272,  ..., 42116,   422,   262]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 2484,   789, 35375,  ...,    13, 24006,   329],\n",
      "        [14202, 50258,  -100,  ...,  -100,  -100,  -100],\n",
      "        [   12, 36829,  6763,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [ 2061,   717,  3181,  ...,   198,  5235, 16877],\n",
      "        [14202, 50258,  -100,  ...,  -100,  -100,  -100],\n",
      "        [13603,   272, 10322,  ...,   422,   262, 21165]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Test of a entry from dataloader\")\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytroch model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first implementation, i am using the transformer and embedding modules from PyTorch. Later i will try to implement the attention mechanism from scratch for better understanding.\n",
    "\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Gpt model class using transformer library\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Network components \n",
    "        ## Embedding layers\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.positional_encoding = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        ## Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['emb_dim'],\n",
    "            nhead=config['number_heads'],\n",
    "            dim_feedforward=4 * config['emb_dim'],\n",
    "            dropout=config['drop_rate'],\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True # stabilityy \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config['number_layers'])\n",
    "        ## Output layer\n",
    "        self.output_layer = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label_ids=None):   \n",
    "        batch_size, seq_length = input_ids.shape\n",
    "\n",
    "        # Embedding\n",
    "        token_embeddings = self.embedding(input_ids)  \n",
    "        pos_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        position_embeddings = self.positional_encoding(pos_ids)  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        embeddings = token_embeddings + position_embeddings  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        # Prevent attending to future tokens\n",
    "        causal_mask = torch.triu(torch.full((seq_length, seq_length), float('-inf'), device=input_ids.device), diagonal=1)\n",
    "\n",
    "        # voiding to pay attatention padding tokens\n",
    "        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        x = self.transformer(embeddings, mask=causal_mask, src_key_padding_mask=key_padding_mask, is_causal=True)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        # Computing loss \n",
    "        if label_ids is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(logits.view(-1, self.config['vocab_size']), label_ids.view(-1)) # Applies loss to predictions\n",
    "            return logits, loss\n",
    "\n",
    "        return logits, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model instantiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (embedding): Embedding(50260, 512)\n",
      "  (positional_encoding): Embedding(512, 512)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=512, out_features=50260, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9603/2657172018.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Gradient scaler for mixed precision\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTModel(GPT_CONFIG).to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10) # Cosine annealing learning rate scheduler\n",
    "scaler = torch.cuda.amp.GradScaler()  # Gradient scaler for mixed precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of parameters calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 76947456\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)    \n",
    "\n",
    "count_params = count_parameters(model)  \n",
    "print(f\"Number of trainable parameters: {count_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, prompt, max_length=256, device='cpu'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_length)\n",
    "\n",
    "    generated_ids = input_ids\n",
    "    max_length = max_length - input_ids.shape[1]  # Remaining length for generation\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            attention_mask = torch.ones_like(generated_ids)  # All tokens are real (no padding)\n",
    "            logits, _ = model(generated_ids, attention_mask)\n",
    "            next_token_logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # (1, 1)\n",
    "\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)  # Append to sequence\n",
    "\n",
    "            if next_token_id.item() == 50258:  # Stop if eot token is generated\n",
    "                break\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_ids.squeeze().tolist())\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps= 1  # Number of steps to accumulate gradients\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, scheduler, device, num_epochs=3, accumulation_steps = 4, question_interval=500, saving_interval=5000):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        step_count = 0\n",
    "\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):  # bf16 optimized for ampere archi\n",
    "                logits, loss = model(input_ids, attention_mask, labels)\n",
    "                loss = loss / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()*accumulation_steps\n",
    "            progress_bar.set_postfix(loss=loss.item() * accumulation_steps)\n",
    "\n",
    "            # Inference check\n",
    "            step_count += 1\n",
    "            if step_count % question_interval == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    prompt = \"You are an AI being trained. How are you doing?\"\n",
    "                    generated_text = inference(model, tokenizer, prompt, max_length=50, device=device)\n",
    "                    print(f\"\\n[Inference at step {step_count}]\")\n",
    "                    #save generated text to file train_ouput.txt\n",
    "                    with open(\"outputs/train_output.txt\", \"a\") as f:\n",
    "                        if generated_text.strip() != \"\":\n",
    "                            f.write(f\"\\n[Inference at step {step_count}]: {generated_text}\\n\")  \n",
    "                            f.write(\"-\"*50 + \"\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"\\n[Inference at step {step_count}]: [No output generated]\\n\")\n",
    "                            f.write(\"-\"*50 + \"\\n\")\n",
    "                model.train()\n",
    "\n",
    "            if step_count % saving_interval == 0:\n",
    "                # Save model checkpoint\n",
    "                date_hour = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                # Create models directory if it doesn't exist\n",
    "                Path(f\"models/{date_hour}\").mkdir(parents=True, exist_ok=True)\n",
    "                checkpoint_path = f\"models/{date_hour}/weights_step{step_count}.pt\"\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"\\nModel checkpoint saved at step {step_count} to {checkpoint_path}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First training on combined train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/2184115 [00:00<?, ?it/s]/home/rob/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Epoch 1/1:   0%|          | 501/2184115 [01:45<146:10:01,  4.15it/s, loss=7.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 1000/2184115 [03:25<138:05:52,  4.39it/s, loss=6.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 1501/2184115 [05:08<136:23:39,  4.45it/s, loss=6.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 1500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 2000/2184115 [06:52<144:55:01,  4.18it/s, loss=6.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 2000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 2500/2184115 [08:36<144:40:57,  4.19it/s, loss=6.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 2500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 3000/2184115 [10:20<143:54:17,  4.21it/s, loss=6.37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 3000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 3501/2184115 [12:03<127:32:01,  4.75it/s, loss=6.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 3500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 4001/2184115 [13:39<126:52:30,  4.77it/s, loss=6.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 4000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 4501/2184115 [15:14<127:12:28,  4.76it/s, loss=6.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 4500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 4999/2184115 [16:49<122:17:21,  4.95it/s, loss=6.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 5000]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models/20260202_181015 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m torch.cuda.empty_cache()\n\u001b[32m      3\u001b[39m torch.cuda.synchronize()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(model, dataloader, optimizer, scheduler, device, num_epochs, accumulation_steps, question_interval, saving_interval)\u001b[39m\n\u001b[32m     49\u001b[39m         date_hour = datetime.now().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m         checkpoint_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_hour\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/weights_step\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mModel checkpoint saved at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m avg_loss = epoch_loss / \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/serialization.py:651\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    648\u001b[39m _check_save_filelike(f)\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    652\u001b[39m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[32m    653\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/serialization.py:525\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    524\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/serialization.py:496\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    494\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(torch._C.PyTorchFileWriter(\u001b[38;5;28mself\u001b[39m.file_stream))\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Parent directory models/20260202_181015 does not exist."
     ]
    }
   ],
   "source": [
    "#empty gpu memory \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    " \n",
    "\n",
    "train_loop(model, train_dataloader, optimizer, scheduler, device, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m torch.cuda.synchronize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/cuda/memory.py:170\u001b[39m, in \u001b[36mempty_cache\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[33;03m`nvidia-smi`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m \u001b[33;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources \n",
    "\n",
    "### Principal references: \n",
    "- https://arxiv.org/abs/2005.14165 (GPT-3 paper)\n",
    "- https://arxiv.org/abs/2002.05709 (Attention is all you need paper)\n",
    "- Build a Large Language Model (from scratch) by Sebastian Raschka\n",
    "\n",
    "### About Padding tokens in Language Modeling\n",
    "- https://arxiv.org/html/2510.01238v1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
