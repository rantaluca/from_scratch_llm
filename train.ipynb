{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet GPT - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used to train a small language model using PyTorch from scratch. The model is inspired by the GPT architecture.\n",
    "\n",
    "\n",
    "#### Hardware\n",
    "- RTX3060 12GB VRAM\n",
    "- AMD Ryzen 7 5800X 8-Core\n",
    "- 32GB RAM\n",
    "- Ubuntu 22.04 LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CACHE_DIR = \"/media/rob/RobsDisk/cache_data_llm\"\n",
    "os.environ['HF_HOME'] = CACHE_DIR\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_DIR, \"datasets\")\n",
    "os.environ['HF_METRICS_CACHE'] = os.path.join(CACHE_DIR, \"metrics\")\n",
    "os.environ['HF_MODULES_CACHE'] = os.path.join(CACHE_DIR, \"modules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/micromamba/envs/mlenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import random, math\n",
    "\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Common knowledge datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### English Wikipedia crawled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Wikipedia dataset loaded.\n",
      "dataset size in gb: 18.812774107791483\n",
      "Number of entries: 6407814\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Stephen James \"Steve\" McArthur (born 17 November 1951) is a former Australian politician. He was the Liberal member for Monbulk in the Victorian Legislative Assembly from 1992 to 2002.\n",
      "\n",
      "McArthur was born in Mildura to Harold Alexander and Gloria McArthur. He graduated from Ballarat Church of England Grammar School in 1969, and became a farmer. On 7 November 1980 he married Sophie Wajsman. His son, Jacob Wajsman McArthur, was born 14 April 1982. In 1990 he began to be active in the Liberal Party, and in 1992 he was selected as the Liberal candidate for the state seat of Monbulk.\n",
      "\n",
      "McArthur was elected to Monbulk, defeating Labor MP Neil Pope, and although he did not attain ministerial office during the Kennett Government, he was promoted to the front bench after Kennett's defeat in 1999 and became Shadow Minister for Water Resources and Manager of Opposition Business. In 2000 he was also given the Agriculture portfolio, but in 2002 he was defeated by James Merlino of the Australian Labor Party.\n",
      "\n",
      "References\n",
      "\n",
      "1951 births\n",
      "Living people\n",
      "Liberal Party of Australia members of the Parliament of Victoria\n",
      "Members of the Victorian Legislative Assembly\n",
      "21st-century Australian politicians\n"
     ]
    }
   ],
   "source": [
    "# English Wikipedia crawled dataset\n",
    "# path to store the dataset cache: /Volumes/RobertsDisk\n",
    "wiki_en = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split='train', cache_dir=CACHE_DIR) \n",
    "print(\"English Wikipedia dataset loaded.\")\n",
    "print(\"dataset size in gb:\", wiki_en.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(wiki_en))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(wiki_en[random.randint(0, len(wiki_en)-1)]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple stories dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple stories dataset loaded.\n",
      "dataset size in mb: 3030.012650489807\n",
      "Number of entries: 2115696\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Tremendously, the sky darkened as the giant beast roared. The villagers ran to hide. It was a massive creature, with scales that shone like stars and eyes that glowed like fire. They called it the Sky Dragon. But there was one who did not run. This one was not a hero; he was an anti-hero. He wore a dark cloak and carried a rusty sword. He wanted to take the dragon's treasure for himself.\n",
      "\n",
      "Silently, he crept to the mountain where the dragon lived. The air was cold, and the wind howled. As he climbed, he saw the dragon sleeping on a pile of gold and gems. He grinned. \"This will be easy,\" he thought. But the ground shook. The dragon woke up and let out a loud growl. The man froze. \"Maybe I should have thought this through,\" he whispered.\n",
      "\n",
      "Unexpectedly, the dragon did not attack. Instead, it spoke. \"Why are you here, small man?\" The anti-hero was surprised. \"I want your gold!\" he shouted. The dragon laughed, a deep and booming sound. \"You think you can take it from me?\" it asked. The man felt fear creep into his heart but pushed it away. \"I am brave!\" he declared.\n",
      "\n",
      "Quickly, he grabbed his sword and charged at the dragon. But the dragon moved like lightning. It blew fire, and the man jumped back. \"You are not a hero,\" the dragon said. \"You are selfish.\" The man paused. He realized the truth in the dragon's words. Maybe he did not want to be the villain. \"What if I help you instead?\" he asked.\n",
      "\n",
      "The dragon blinked in surprise. \"Help me?\" it asked. \"Why would you do that?\" The man thought hard. \"I am tired of being alone,\" he replied. \"I want friends.\" The dragon nodded slowly. \"If you help me guard my treasure, you will have friends. But if you betray me, I will take what is mine.\" \n",
      "\n",
      "Together, they worked to keep the treasure safe from thieves and other monsters. The man learned to care for the dragon. They shared stories under the stars, and soon they were friends. The villagers watched in wonder as the man changed. He became a protector, not a thief. \n",
      "\n",
      "Eventually, the man became known as the Keeper of the Sky Dragon. People came from far away to see them together. They saw a giant creature and a small man, once enemies, now friends. The man smiled, knowing he was no longer alone. He had found a place where he belonged.\n"
     ]
    }
   ],
   "source": [
    "# Simple stories dataset\n",
    "stories = load_dataset(\"SimpleStories/SimpleStories\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Simple stories dataset loaded.\")\n",
    "print(\"dataset size in mb:\", stories.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(stories))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(stories[random.randint(0, len(stories)-1)]['story'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FineWeb-Edu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineWeb-Edu is ready.\n",
      "dataset size in gb: 45.730818568728864\n",
      "Number of entries: 9672101\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "sock toe prior to grafting, courtesy Silver’s Sock Class\n",
      "the process of grafting the toe of a sock, courtesy FaveCrafts.com\n",
      "I came across the first article in a series of articles on grafting. Grafting in knitting is the process whereby the knitter uses a tapestry needle to create new knit stitches to “graft” together two live ends. For example, when knitting top-down socks, you start with the cuff and end with the toe, but you have two “live” edges (see picture left)–that is, two edges of free stitches that will unravel unless stabilized. You can either bind off those edges, in which case you will have a huge hole in the toe of your sock, or you can graft those edges together. The process of grafting traditionally utilizes a kitchener stitch placed via a tapestry needle through the loops on back and front edges of free stitches. In other words, you basically take a huge, blunt needle (“tapestry” needle) and sew the ends together (see picture at right) with the remaining yarn you used to knit the rest of the sock. If it is done correctly, you sometimes can’t even distinguish the stitches made with the tapestry needle from the stitches made with the knitting needles.\n",
      "Outside of knitting, the term “grafting” generally just means to insert or fix or join one thing to something else, such as a tissue (e.g. hair or skin) transplant. I’m told that one problem with learning Japanese from English is that the grammatical structures of the two languages are so different that you can’t simply “graft” English onto Japanese. For example, the Japanese language uses tiny words called “particles” which identify what you’re talking about, as in: “わたしわ日本語がはなせません。” (“Watashi-wa Nihongo-ga hanasemasen.”) In this case, the literal translation is “I [subject particle] Japanese language [direct object particle] am not able to speak.” But the particles have such different roles in Japanese than they do in English that simply calling the first (わ) the subject particle and the second (が) the direct object particle doesn’t really fully encompass their roles. You basically would be better off starting from scratch, learning what each particle means without trying to find a translation for it in English.\n",
      "a grafted tree branch, courtesy Now Arise Ministries\n",
      "One of the most common uses of the word “graft” is for the grafting of a tree branch onto an unrelated tree (see picture at left). Derek and I recently went for a long walk around two adjoining parks, one of which had historic pecan trees, a few of the remains from a pecan grove created by a pioneer in the late 1800s. This particular man grafted the branches of a different species of pecan trees that produce sweeter pecans onto the native pecan trees he found in the area. Thereby, the local pecan trees produced the sweeter pecans found on the different species of pecan tree. Grafting involves cutting into the new, “recipient” tree and placing the cut end of another tree’s branch into the cut area of the recipient tree, then wrapping it securely so that it stays put. Over time, the recipient tree and the new branch will grow into each other, and the new branch will receive nourishment from the recipient tree and produce fruit of its own kind.\n",
      "In Romans 11, the Apostle Paul uses this example to explain a theological concept. He explains in chapter 10 that the Messiah came first to the Jews, but they rejected Him.\n",
      "But to Israel He saith , All day long I have stretched forth my hands unto a disobedient and gainsaying people. (Romans 10:21)\n",
      "So now He has offered His hand to the Gentiles. Using the image of grafting, Paul (a Jew) explains that the original branches–the Jews–were cast off because of their unbelief and the new branches–the Gentiles–grafted onto the root (i.e., God) because of their faith. But he warns the Gentiles not to boast of their status, reminding them that the branches do not bear the root, but the root bears the branches, and that if God cast away the original branches, He can certainly cast away the grafted branches. He concludes with hope: if the Jews will restore their belief, God will graft them back in.\n",
      "To what are you grafted?\n"
     ]
    }
   ],
   "source": [
    "fineweb_edu = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\",  split='train', cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"FineWeb-Edu is ready.\")\n",
    "print(\"dataset size in gb:\", fineweb_edu.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(fineweb_edu))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(fineweb_edu[random.randint(0, len(fineweb_edu)-1)]['text']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenWebText2 dataset loaded.\n",
      "Dataset size in GB: 37.03822539001703\n",
      "Number of entries: 8013769\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "A grand jury in Brooklyn indicted a man on hate crime charges for the assault of a lesbian subway rider that left her with a concussion, fractured nose, a broken eye socket and multiple deep cuts which required eight stitches.\n",
      "\n",
      "Prosecutors say when Antoine Thomas, 27, boarded the downtown Q train at Union Station in Manhattan on May 21st, he sat next to the victim and her girlfriend, and started shouting homophobic slurs at them. PinkNews reported he called them dykes and faggots.\n",
      "\n",
      "“Brooklyn is a model of inclusion and diversity where bias and bigotry will not be tolerated,” acting District Attorney Eric Gonzalez said in a statement. “This defendant’s alleged actions were an attack on everyone’s right to be who they are, and to love whom they choose.”\n",
      "\n",
      "The women attempted to get the man to calm down by talking with him but prosecutors say his behavior only became worse. He allegedly aggressively bumped into their bodies when the train came to a stop, then punched one in the face, knocking her unconscious.\n",
      "\n",
      "Thomas allegedly ran from the train but was apprehended by New York City Transit Police at the DeKalb Avenue station. He was arraigned Monday, reported WPIX-TV, and if convicted of a hate crime, he could be sentenced to 15 years behind bars.\n",
      "\n",
      "The victim, 24, was treated at Methodist Hospital. Her condition is not known but she is said to be recovering.\n",
      "\n",
      "Thomas is due back in court Sept. 8th.\n",
      "\n",
      "This Story Filed Under\n"
     ]
    }
   ],
   "source": [
    "# OpenWebText2 dataset\n",
    "owt2 = load_dataset(\"Skylion007/openwebtext\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "print(\"OpenWebText2 dataset loaded.\")\n",
    "print(\"Dataset size in GB:\", owt2.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(owt2))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(owt2[random.randint(0, len(owt2)-1)]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Q&A data to improve the model's ability to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A dataset loaded.\n",
      "dataset size in mb: 46.480509757995605\n",
      "Number of entries: 120959\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "What are some key aspects of scene management for multi-vehicle collisions, and why are they important? \n",
      " Scene management techniques for multi-vehicle collisions are vital because they help ensure responders address immediate threats promptly while minimizing risks associated with extrication efforts, evidence preservation, and ongoing traffic flow. Proper scene management ensures that responders can assess the situation, address critical needs, and implement safe measures to prevent further damage and injuries.\n"
     ]
    }
   ],
   "source": [
    "q_a1 = load_dataset(\"agentlans/text-sft-questions-answers-only\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Q&A dataset loaded.\")\n",
    "print(\"dataset size in mb:\", q_a1.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(q_a1))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(q_a1)-1)\n",
    "print(q_a1[index]['question'][:500], \"\\n\", q_a1[index]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit Instruct dataset loaded.\n",
      "dataset size in gb: 0.09901080373674631\n",
      "Number of entries: 84784\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "[General] Why are magical societies so backwards? By backwards I refer mostly to how almost all societies with magic are more or less stuck in the medieval age.\n",
      "\n",
      "While some of such societies are placed in a general medieval world, others don't have such excuses. Others are excused by saying that magic mix badly with technology. (but the Weasley car was awesome!) \n",
      "\n",
      "The most jarring example I can think of is Thundercats. The tech savvy lizards kicked the lions ass so hard they went from apex predator to near extinction in one day. All because the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " '\\n',\n",
       " \"Lack of selective pressure. Magic pretty quickly leads to post-scarcity, or nearly so, and also makes you the biggest, baddest power around.\\n\\nIf you don't have any credible competition for a few generations, you forget how to compete. Tech-based societies are built on continual, gradual advancement, however. Eventually they reach a point where they're competitive with magic, and perhaps even superior.\\n\\nIf we consider *Harry Potter*, magic has allowed the Wizarding community to exist more or less\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#euclaise/reddit-instruct\n",
    "reddit_instruct = load_dataset(\"euclaise/reddit-instruct\", split='train', cache_dir=CACHE_DIR)\n",
    "# reddit_instruct = load_dataset(\"Felladrin/ChatML-reddit-instruct-curated\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Reddit Instruct dataset loaded.\")\n",
    "print(\"dataset size in gb:\", reddit_instruct.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(reddit_instruct))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(reddit_instruct)-1)\n",
    "print(reddit_instruct[index]['post_title'][:500], reddit_instruct[index]['post_text'][:500]), \"\\n\", reddit_instruct[index]['comment_text'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca dataset loaded.\n",
      "dataset size in mb: 44.06797695159912\n",
      "Number of entries: 52002\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Provide a real-life example of an artificial neural network. \n",
      " An example of a real-life artificial neural network is the speech recognition technology used by many voice recognition devices, such as Amazon Alexa and Google Home. This technology uses multiple layers of neurons to learn how to identify and respond to certain spoken commands.\n"
     ]
    }
   ],
   "source": [
    "# tatsu-lab/alpaca ( for Q&A fine-tuning )\n",
    "alpaca = load_dataset(\"tatsu-lab/alpaca\", split='train')\n",
    "print(\"Alpaca dataset loaded.\")\n",
    "print(\"dataset size in mb:\", alpaca.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(alpaca))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(alpaca)-1)\n",
    "print(alpaca[index]['instruction'][:500], \"\\n\", alpaca[index]['output'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project i use tiktoken for the tokenizer, as it is the same tokenizer used by OpenAI for their models.\n",
    "\n",
    "I use the \"gpt2\" encoding which is a byte pair encoding (BPE) tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"rob-tokenizer\",\n",
    "    pat_str=tokenizer_base._pat_str,\n",
    "    mergeable_ranks=tokenizer_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **tokenizer_base._special_tokens,\n",
    "        \"<|im_start|>\": 50257,\n",
    "        \"<|im_end|>\": 50258,\n",
    "        \"<|pad|>\": 50259,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the byte pair encoding tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2061, 318, 24207, 1616, 2587, 30, 314, 2342, 257, 7684, 286, 1097, 5861, 290, 484, 1561, 546, 275, 32512, 7021, 290, 884, 11, 1312, 373, 11263, 644, 275, 32512, 318, 290, 1312, 18548, 1064, 597, 2562, 7468, 284, 644, 340, 318, 24207, 1616, 318, 655, 262, 1438, 329, 257, 16058, 286, 6147, 13, 554, 262, 29393, 995, 340, 338, 1690, 973, 355, 257, 1790, 1021, 329, 3354, 326, 547, 3235, 1389, 503, 286, 257, 1263, 2512, 286, 2587, 11, 355, 6886, 284, 11721, 3350, 654, 810, 44030, 6147, 318, 19036, 656, 257, 15936, 12070, 503, 286, 9629, 6147, 13, 7080, 3191, 318, 517, 5789, 329, 1588, 17794, 475, 340, 460, 779, 1365, 3081, 286, 21782, 290, 318, 4577, 284, 787, 329, 4833, 17794, 588, 3234, 3354, 13]\n",
      "Decoded text:\n",
      "What is Billet material? I watch a bunch of car videos and they talk about billet blocks and such, i was wondering what billet is and i cant find any easy explanation to what it is Billet is just the name for a chunk of metal. In the automotive world it's often used as a short hand for parts that were machined out of a big block of material, as opposed to cheaper castings where molten metal is poured into a mold pressed out of sheet metal. Machining is more expensive for large quantities but it can use better quality of metals and is easier to make for smaller quantities like race parts.\n",
      "Sample text length in characters: 594\n",
      "Sample text length in tokens: 127\n"
     ]
    }
   ],
   "source": [
    "# test of tokenizer on reddit_instruct\n",
    "sample_text = reddit_instruct[0]['post_title'] + \" \" + reddit_instruct[0]['post_text'] + \" \" + reddit_instruct[0]['comment_text']\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(tokens)\n",
    "print(\"Decoded text:\")\n",
    "print(tokenizer.decode(tokens)) \n",
    "print(f\"Sample text length in characters: {len(sample_text)}\")\n",
    "print(f\"Sample text length in tokens: {len(tokens)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting datasets functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 26209380\n",
      "Finetune dataset size: 257745\n",
      "Example entry from train dataset:\n",
      "{'id': None, 'url': None, 'title': None, 'text': None, 'story': 'In the light of dawn, a woman opened her window and sighed. \"What would I change if I could?\" she thought. As she wondered, a bird flew into her room. \"Follow me!\" it chirped. \"I will show you.\"\\n\\n\"Show me what?\" she asked, confused. \"Your past,\" the bird said. \"You must see the roots to understand your wings.\" They flew together, landing in memories of joy and sadness. \"I remember,\" she said softly. \"I have grown.\"\\n\\n\"Now, choose your path,\" the bird encouraged. \"You can create a new future.\" The woman smiled, feeling a spark of hope. \"I will be brave and follow my dreams,\" she said. With that, the bird took off, and she watched it soar, ready to embrace her new beginning.', 'topic': 'time travel', 'theme': 'Transformation', 'style': 'philosophical', 'feature': 'dialogue', 'grammar': 'perfect aspect', 'persona': '', 'initial_word_type': 'preposition', 'initial_letter': 'T', 'word_count': 178, 'character_count': 680, 'num_paragraphs': 3, 'avg_word_length': 3.24, 'avg_sentence_length': 9.37, 'flesch_reading_ease': 97.4, 'flesch_kincaid_grade': 1.6, 'dale_chall_readability_score': 6.82, 'num_stories_in_completion': 9, 'expected_num_stories_in_completion': 12, 'generation_id': '83302479b3f6649422f308aa74cccd79-8', 'model': 'gpt-4o-mini-2024-07-18', 'dump': None, 'file_path': None, 'language': None, 'language_score': None, 'token_count': None, 'score': None, 'int_score': None}\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_train_dataset = concatenate_datasets([  \n",
    "    wiki_en,\n",
    "    stories,\n",
    "    fineweb_edu,\n",
    "    owt2,  \n",
    "])  \n",
    "\n",
    "combined_finetune_dataset = concatenate_datasets([\n",
    "    q_a1,\n",
    "    reddit_instruct,\n",
    "    alpaca,\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_dataset = combined_train_dataset.shuffle(seed=42)\n",
    "finetune_dataset = combined_finetune_dataset.shuffle(seed=42)\n",
    "print(f\"Train dataset size: {len(combined_train_dataset)}\")\n",
    "print(f\"Finetune dataset size: {len(combined_finetune_dataset)}\")\n",
    "\n",
    "# Exemple \n",
    "\n",
    "print(\"Example entry from train dataset:\")\n",
    "index = random.randint(0, len(train_dataset)-1)\n",
    "print(train_dataset[index])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the dataloader from the \"LLMs from scratch\" repository. But adapted for multi-row text arrays.\n",
    "\n",
    "https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/dataloader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Dataset of the combined hugginface entries\n",
    "            tokenizer: the initiatokenizer to process text\n",
    "            max_length: Context window size\n",
    "        \"\"\"\n",
    "        self.data = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.input_tokens = []\n",
    "        self.target_tokens = []\n",
    "\n",
    "        self.pad_token_id = 50259         # <|pad|>\n",
    "        self.bos_token_id = 50257    # <|im_start|>\n",
    "        self.eos_token_id = 50258    # <|im_end|>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Raw text\n",
    "        \n",
    "        # Data format handling\n",
    "        entry = self.data[idx]\n",
    "        if 'text' in entry:\n",
    "            text = entry['text']\n",
    "        elif 'story' in entry:\n",
    "            text = entry['story']\n",
    "        elif 'question' in entry and 'answer' in entry: \n",
    "            text = \"User: \" + entry['question'] + \" Assistant:\" + entry['answer']\n",
    "        elif 'post_title' in entry and 'post_text' in entry and 'comment_text' in entry:\n",
    "            text = \"User: \" + entry['post_title'] + \" Assistant:\" + entry['post_text'] + \" \" + entry['comment_text']\n",
    "        elif 'instruction' in entry and 'output' in entry:\n",
    "            text = \"User: \" + entry['instruction'] + \" Assistant:\" + entry['output']\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data entry format\")\n",
    "        \n",
    "        text = str(text) # Ensure text is a string\n",
    "        #print(text)\n",
    "\n",
    "        # Adding Start and End tokens\n",
    "        text = \"<|im_start|>\" + text + \"<|im_end|>\" \n",
    "\n",
    "        # Tokenization\n",
    "        tokens = self.tokenizer.encode(text, allowed_special=\"all\")\n",
    "\n",
    "        # Truncation\n",
    "        tokens = tokens[:self.max_length] #Data is loost here ( fix later with sliding window )\n",
    "\n",
    "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)  # All tokens except last\n",
    "        labels = torch.tensor(tokens[1:], dtype=torch.long)      # All tokens except first\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        attention_mask = (input_ids != self.pad_token_id).long() # 1 for real tokens, 0 for padding\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTstreamingDataset(IterableDataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Dataset of the combined hugginface entries\n",
    "            tokenizer: the initiatokenizer to process text\n",
    "            max_length: Context window size\n",
    "        \"\"\"\n",
    "        self.data = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.bos_token_id = 50257    # <|im_start|>\n",
    "        self.eos_token_id = 50258    # <|im_end|>\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        for entry in self.data:\n",
    "            # Data format \n",
    "            if entry.get('text') is not None:\n",
    "                text = entry['text']\n",
    "            elif entry.get('story') is not None:\n",
    "                text = entry['story']\n",
    "            elif entry.get('question') is not None and entry.get('answer') is not None: \n",
    "                text = f\"User: {entry['question']} Assistant: {entry['answer']}\"\n",
    "            elif entry.get('post_title') is not None:\n",
    "                title = entry.get('post_title', \"\")\n",
    "                post = entry.get('post_text', \"\")\n",
    "                comment = entry.get('comment_text', \"\")\n",
    "                text = f\"User: {title} Assistant: {post} {comment}\"\n",
    "            elif entry.get('instruction') is not None and entry.get('output') is not None:\n",
    "                text = f\"User: {entry['instruction']} Assistant: {entry['output']}\"\n",
    "            if text is None:\n",
    "                continue\n",
    "                \n",
    "            text = str(text) \n",
    "\n",
    "            # Start and End tokens\n",
    "            text = \"<|im_start|>\" + text + \"<|im_end|>\" \n",
    "\n",
    "            # Tokenization\n",
    "            tokens = self.tokenizer.encode(text, allowed_special=\"all\")\n",
    "\n",
    "            buffer.extend(tokens) #pile tokens into buffer\n",
    "\n",
    "\n",
    "            while len(buffer) >= self.max_length +1:\n",
    "                chunk = buffer[:self.max_length + 1]\n",
    "                buffer = buffer[self.max_length+ 1:] # remove used tokens\n",
    "\n",
    "                input_ids = torch.tensor(chunk[:-1], dtype=torch.long)  \n",
    "                labels = torch.tensor(chunk[1:], dtype=torch.long)      \n",
    "\n",
    "                yield {\n",
    "                    'input_ids': input_ids,\n",
    "                    'labels': labels\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the configuration for the GPT model i am going to train. It is a smaller version of the GPT-2 model. \n",
    "\n",
    "- Context length: 512 tokens\n",
    "- Embedding dimension: 512\n",
    "- Number of attention heads: 8\n",
    "- Number of layers: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50260,\n",
    "    \"context_length\": 256, # max i could fit on my gpu\n",
    "    \"emb_dim\": 128,\n",
    "    \"number_heads\": 4,\n",
    "    \"number_layers\": 4,\n",
    "    \"drop_rate\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test of a entry from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cuda cache and memory management\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_dataset = GPTstreamingDataset(train_dataset, tokenizer, GPT_CONFIG['context_length'])\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, pin_memory=True, prefetch_factor=2, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Test of a entry from dataloader\n",
      "{'input_ids': tensor([[50257,   818,   257,  ...,  4403,  5866, 10062],\n",
      "        [  290,  7103, 28717,  ..., 23312, 33408,   341],\n",
      "        [  262,  4403,  5866,  ..., 24337,  1080,   373],\n",
      "        ...,\n",
      "        [ 2482,    13,   198,  ...,   286,   262, 38999],\n",
      "        [ 2428,   287,   262,  ...,  4911,  3194,  7170],\n",
      "        [ 3000,  5694,  3195,  ..., 13226,   262,  6594]]), 'labels': tensor([[  818,   257,  6016,  ...,  5866, 10062,  2838],\n",
      "        [ 7103, 28717,  1989,  ..., 33408,   341,   287],\n",
      "        [ 4403,  5866, 10062,  ...,  1080,   373,   973],\n",
      "        ...,\n",
      "        [   13,   198,   198,  ...,   262, 38999,  6386],\n",
      "        [  287,   262,  6130,  ...,  3194,  7170,   422],\n",
      "        [ 5694,  3195,    13,  ...,   262,  6594,   510]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Test of a entry from dataloader\")\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytroch model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first implementation, i am using the transformer and embedding modules from PyTorch. Later i will try to implement the attention mechanism from scratch for better understanding.\n",
    "\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Gpt model class using transformer library\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Network components \n",
    "        ## Embedding layers\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.positional_encoding = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        ## Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['emb_dim'],\n",
    "            nhead=config['number_heads'],\n",
    "            dim_feedforward=4 * config['emb_dim'],\n",
    "            dropout=config['drop_rate'],\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True # stabilityy \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config['number_layers'])\n",
    "        ## Output layer\n",
    "        self.output_layer = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label_ids=None):   \n",
    "        batch_size, seq_length = input_ids.shape\n",
    "\n",
    "        # Embedding\n",
    "        token_embeddings = self.embedding(input_ids)  \n",
    "        pos_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        position_embeddings = self.positional_encoding(pos_ids)  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        embeddings = token_embeddings + position_embeddings  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        # Prevent attending to future tokens\n",
    "        causal_mask = torch.triu(torch.full((seq_length, seq_length), float('-inf'), device=input_ids.device), diagonal=1)\n",
    "\n",
    "        # voiding to pay attatention padding tokens\n",
    "        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        x = self.transformer(embeddings, mask=causal_mask, src_key_padding_mask=key_padding_mask, is_causal=True)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        # Computing loss \n",
    "        if label_ids is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(logits.view(-1, self.config['vocab_size']), label_ids.view(-1)) # Applies loss to predictions\n",
    "            return logits, loss\n",
    "\n",
    "        return logits, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModelNoPad(nn.Module):\n",
    "    \"\"\"\n",
    "    Gpt model class using transformer library adapted for streaming dataset without padding mask\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Network components \n",
    "        ## Embedding layers\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.positional_encoding = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        ## Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['emb_dim'],\n",
    "            nhead=config['number_heads'],\n",
    "            dim_feedforward=4 * config['emb_dim'],\n",
    "            dropout=config['drop_rate'],\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True # stabilityy \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config['number_layers'])\n",
    "        ## Output layer\n",
    "        self.output_layer = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "        # Weight Tying ( input and output embeddings share weights)\n",
    "        self.output_layer.weight = self.embedding.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, label_ids=None):   \n",
    "        batch_size, seq_length = input_ids.shape\n",
    "\n",
    "        # Embedding\n",
    "        token_embeddings = self.embedding(input_ids)  \n",
    "        pos_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        position_embeddings = self.positional_encoding(pos_ids)  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        embeddings = token_embeddings + position_embeddings  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        # Prevent attending to future tokens\n",
    "        causal_mask = torch.triu(torch.full((seq_length, seq_length), float('-inf'), device=input_ids.device), diagonal=1)\n",
    "        \n",
    "        x = self.transformer(embeddings, mask=causal_mask, is_causal=True)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        # Computing loss \n",
    "        if label_ids is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.config['vocab_size']), label_ids.view(-1)) # Applies loss to predictions\n",
    "            return logits, loss\n",
    "\n",
    "        return logits, None \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model instantiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModelNoPad(\n",
      "  (embedding): Embedding(50260, 128)\n",
      "  (positional_encoding): Embedding(256, 128)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=128, out_features=50260, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/micromamba/envs/mlenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/tmp/ipykernel_63492/2518131499.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Gradient scaler for mixed precision\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTModelNoPad(GPT_CONFIG).to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=10000) # Linear learning rate scheduler\n",
    "scaler = torch.cuda.amp.GradScaler()  # Gradient scaler for mixed precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of parameters calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 7259136\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)    \n",
    "\n",
    "count_params = count_parameters(model)  \n",
    "print(f\"Number of trainable parameters: {count_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, prompt, max_length=256, device='cpu'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_length)\n",
    "\n",
    "    generated_ids = input_ids\n",
    "    max_length = max_length - input_ids.shape[1]  # Remaining length for generation\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            #attention_mask = torch.ones_like(generated_ids)  # All tokens are real (no padding)\n",
    "            logits, _ = model(generated_ids) # model(generated_ids, attention_mask)\n",
    "            next_token_logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # (1, 1)\n",
    "\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)  # Append to sequence\n",
    "\n",
    "            if next_token_id.item() == 50258:  # Stop if eot token is generated\n",
    "                break\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_ids.squeeze().tolist())\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps= 1  # Number of steps to accumulate gradients\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, scheduler, device, num_epochs=3, accumulation_steps = 4, question_interval=500, saving_interval=5000):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        step_count = 0\n",
    "\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            #attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):  # bf16 optimized for ampere archi\n",
    "                logits, loss = model(input_ids, labels) # model(input_ids, attention_mask, labels)\n",
    "                loss = loss / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()*accumulation_steps\n",
    "            progress_bar.set_postfix(loss=loss.item() * accumulation_steps)\n",
    "\n",
    "            # Inference check\n",
    "            step_count += 1\n",
    "            if step_count % question_interval == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    prompt = \"AI is technology that enables computers and machines to\"\n",
    "                    generated_text = inference(model, tokenizer, prompt, max_length=50, device=device)\n",
    "                    print(f\"\\n[Inference at step {step_count}]\")\n",
    "                    #save generated text to file train_ouput.txt\n",
    "                    with open(\"outputs/train_output.txt\", \"a\") as f:\n",
    "                        if generated_text.strip() != \"\":\n",
    "                            f.write(f\"\\n[Inference at step {step_count}]: {generated_text}\\n\")  \n",
    "                            f.write(\"-\"*50 + \"\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"\\n[Inference at step {step_count}]: [No output generated]\\n\")\n",
    "                            f.write(\"-\"*50 + \"\\n\")\n",
    "                model.train()\n",
    "\n",
    "            if step_count % saving_interval == 0:\n",
    "                # Save model checkpoint\n",
    "                date_hour = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                # Create models directory if it doesn't exist\n",
    "                Path(f\"models/{date_hour}\").mkdir(parents=True, exist_ok=True)\n",
    "                checkpoint_path = f\"models/{date_hour}/weights_step{step_count}.pt\"\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"\\nModel checkpoint saved at step {step_count} to {checkpoint_path}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First training on combined train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 501it [00:52,  8.16it/s, loss=7.67]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inference at step 500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 582it [01:00,  9.57it/s, loss=7.91]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m torch.cuda.empty_cache()\n\u001b[32m      3\u001b[39m torch.cuda.synchronize()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(model, dataloader, optimizer, scheduler, device, num_epochs, accumulation_steps, question_interval, saving_interval)\u001b[39m\n\u001b[32m     23\u001b[39m     optimizer.zero_grad()\n\u001b[32m     24\u001b[39m     scheduler.step()\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m epoch_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m*accumulation_steps\n\u001b[32m     27\u001b[39m progress_bar.set_postfix(loss=loss.item() * accumulation_steps)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Inference check\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#empty gpu memory \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "train_loop(model, train_dataloader, optimizer, scheduler, device, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources \n",
    "\n",
    "### Principal references: \n",
    "- https://arxiv.org/abs/2005.14165 (GPT-3 paper)\n",
    "- https://arxiv.org/abs/2002.05709 (Attention is all you need paper)\n",
    "- Build a Large Language Model (from scratch) by Sebastian Raschka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
