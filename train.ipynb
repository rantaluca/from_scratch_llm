{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet GPT - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used to train a small language model using PyTorch from scratch. The model is inspired by the GPT architecture.\n",
    "\n",
    "\n",
    "#### Hardware\n",
    "- RTX3060 12GB VRAM\n",
    "- AMD Ryzen 7 5800X 8-Core\n",
    "- 32GB RAM\n",
    "- Ubuntu 22.04 LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CACHE_DIR = \"/media/rob/RobsDisk/cache_data_llm\"\n",
    "os.environ['HF_HOME'] = CACHE_DIR\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_DIR, \"datasets\")\n",
    "os.environ['HF_METRICS_CACHE'] = os.path.join(CACHE_DIR, \"metrics\")\n",
    "os.environ['HF_MODULES_CACHE'] = os.path.join(CACHE_DIR, \"modules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/micromamba/envs/mlenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random, math\n",
    "\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "import tiktoken\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Common knowledge datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### English Wikipedia crawled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Wikipedia dataset loaded.\n",
      "dataset size in gb: 18.812774107791483\n",
      "Number of entries: 6407814\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Kopi John (10 October 1993 – 27 August 2019) was a Papua New Guinean cricketer. In July 2018, she was named in Papua New Guinea's squad for the 2018 ICC Women's World Twenty20 Qualifier tournament. She made her Women's Twenty20 International (WT20I) for Papua New Guinea against Bangladesh in the World Twenty20 Qualifier on 7 July 2018. In April 2019, she was named in Papua New Guinea's squad for the 2019 ICC Women's Qualifier EAP tournament in Vanuatu.\n",
      "\n",
      "John died on 27 August 2019 following a sh\n"
     ]
    }
   ],
   "source": [
    "# English Wikipedia crawled dataset\n",
    "# path to store the dataset cache: /Volumes/RobertsDisk\n",
    "wiki_en = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split='train', cache_dir=CACHE_DIR) \n",
    "print(\"English Wikipedia dataset loaded.\")\n",
    "print(\"dataset size in gb:\", wiki_en.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(wiki_en))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(wiki_en[random.randint(0, len(wiki_en)-1)]['text'][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple stories dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple stories dataset loaded.\n",
      "dataset size in mb: 3030.012650489807\n",
      "Number of entries: 2115696\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Down by the river, a boy named Alex loved to fish. One sunny day, he sat on the bank with his fishing pole. While waiting for a bite, he noticed something shiny in the water. He leaned in closer and saw a treasure chest! Alex couldn't believe his luck. He quickly pulled out his fishing net to catch the chest. \n",
      "\n",
      "After a bit of splashing, he managed to get it out. The chest was heavy, and Alex struggled to lift it. \"This better be worth it,\" he said, panting. He found a spot to sit and examined th\n"
     ]
    }
   ],
   "source": [
    "# Simple stories dataset\n",
    "stories = load_dataset(\"SimpleStories/SimpleStories\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Simple stories dataset loaded.\")\n",
    "print(\"dataset size in mb:\", stories.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(stories))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(stories[random.randint(0, len(stories)-1)]['story'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FineWeb-Edu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineWeb-Edu is ready.\n",
      "dataset size in gb: 45.730818568728864\n",
      "Number of entries: 9672101\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Images of Corn (Clavus)\n",
      "Corns are thickenings of the skin composed of keratin that are typically found on the toes caused by repeated friction or pressure to the area. The base of the corn is seen on the surface of the skin while the top points inward, causing discomfort.\n",
      "Corns are classified as either hard or soft, depending upon their location and appearance. Hard corns typically affect the tops of the toes and are composed of a dense core that presses on sensory nerves, causing extreme pain. \n"
     ]
    }
   ],
   "source": [
    "fineweb_edu = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\",  split='train', cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"FineWeb-Edu is ready.\")\n",
    "print(\"dataset size in gb:\", fineweb_edu.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(fineweb_edu))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(fineweb_edu[random.randint(0, len(fineweb_edu)-1)]['text'][:500]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Q&A data to improve the model's ability to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A dataset loaded.\n",
      "dataset size in mb: 46.480509757995605\n",
      "Number of entries: 120959\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "What is the value of exploring the intersection between sound and visual art through Chunity? \n",
      " The combination of audio processing and 3D graphics allows developers to explore the intersection between sound and visual art, creating immersive environments where objects emit sound, instruments react visually, and scenes come alive through synchronized audio-visual elements.\n"
     ]
    }
   ],
   "source": [
    "q_a1 = load_dataset(\"agentlans/text-sft-questions-answers-only\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Q&A dataset loaded.\")\n",
    "print(\"dataset size in mb:\", q_a1.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(q_a1))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(q_a1)-1)\n",
    "print(q_a1[index]['question'][:500], \"\\n\", q_a1[index]['answer'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit Instruct dataset loaded.\n",
      "dataset size in gb: 0.09901080373674631\n",
      "Number of entries: 84784\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "ELI5: How exactly did people kill soldiers wearing heavy armour(the type from and medieval movies among others) with only swords and arrows throughout history? Plus, how did the people wearing these armours have the stamina to fight in battles? Wouldn't they be exhausted after fighting only one person who also happened to be wearing similar armour?\n",
      "\n",
      "Assuming all these movies accurately represent how battles and wars were.\n",
      "\n",
      "Example of the armour I'm talking about:\n",
      "\n",
      "http://www.medievalcollectibles.com/images/Product/large/ED6223.png\n",
      "\n",
      "As well as other types.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " '\\n',\n",
       " 'There were usually some gaps between the armor pieces. Also you could kill these soldiers with a heavy blow (such as a mace to  the head, or falling off a moving horse). Also they could become bogged down in mud and easy to clobber. Also they could just become exhausted.\\n\\nHowever, very few soldiers actually wore heavy, thick armor. It was incredibly expensive. Incomplete armor or chain mail were more common.\\n\\nAnd then, you could just shoot them with a crossbow. This was effective against all but')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#euclaise/reddit-instruct\n",
    "reddit_instruct = load_dataset(\"euclaise/reddit-instruct\", split='train', cache_dir=CACHE_DIR)\n",
    "# reddit_instruct = load_dataset(\"Felladrin/ChatML-reddit-instruct-curated\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Reddit Instruct dataset loaded.\")\n",
    "print(\"dataset size in gb:\", reddit_instruct.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(reddit_instruct))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(reddit_instruct)-1)\n",
    "print(reddit_instruct[index]['post_title'][:500], reddit_instruct[index]['post_text'][:500]), \"\\n\", reddit_instruct[index]['comment_text'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca dataset loaded.\n",
      "dataset size in mb: 44.06797695159912\n",
      "Number of entries: 52002\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Given a real-world scenario, design a system to automate the task. \n",
      " The restaurant food delivery system will allow customers to order online or through a mobile app. Customers will be able to select items from the restaurant's menu and choose the delivery option. Drivers will receive an alert once the order has been placed, and they can navigate to the restaurant to pick up the order and deliver it to the customer's address. The system must also include payment options and order tracking features.\n"
     ]
    }
   ],
   "source": [
    "# tatsu-lab/alpaca ( for Q&A fine-tuning )\n",
    "alpaca = load_dataset(\"tatsu-lab/alpaca\", split='train')\n",
    "print(\"Alpaca dataset loaded.\")\n",
    "print(\"dataset size in mb:\", alpaca.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(alpaca))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(alpaca)-1)\n",
    "print(alpaca[index]['instruction'][:500], \"\\n\", alpaca[index]['output'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project i use tiktoken for the tokenizer, as it is the same tokenizer used by OpenAI for their models.\n",
    "\n",
    "I use the \"gpt2\" encoding which is a byte pair encoding (BPE) tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"rob-tokenizer\",\n",
    "    pat_str=tokenizer_base._pat_str,\n",
    "    mergeable_ranks=tokenizer_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **tokenizer_base._special_tokens,\n",
    "        \"<|im_start|>\": 100264,\n",
    "        \"<|im_end|>\": 100265,\n",
    "        \"<|pad|>\": 0,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the byte pair encoding tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2061, 318, 24207, 1616, 2587, 30, 314, 2342, 257, 7684, 286, 1097, 5861, 290, 484, 1561, 546, 275, 32512, 7021, 290, 884, 11, 1312, 373, 11263, 644, 275, 32512, 318, 290, 1312, 18548, 1064, 597, 2562, 7468, 284, 644, 340, 318, 24207, 1616, 318, 655, 262, 1438, 329, 257, 16058, 286, 6147, 13, 554, 262, 29393, 995, 340, 338, 1690, 973, 355, 257, 1790, 1021, 329, 3354, 326, 547, 3235, 1389, 503, 286, 257, 1263, 2512, 286, 2587, 11, 355, 6886, 284, 11721, 3350, 654, 810, 44030, 6147, 318, 19036, 656, 257, 15936, 12070, 503, 286, 9629, 6147, 13, 7080, 3191, 318, 517, 5789, 329, 1588, 17794, 475, 340, 460, 779, 1365, 3081, 286, 21782, 290, 318, 4577, 284, 787, 329, 4833, 17794, 588, 3234, 3354, 13]\n",
      "Decoded text:\n",
      "What is Billet material? I watch a bunch of car videos and they talk about billet blocks and such, i was wondering what billet is and i cant find any easy explanation to what it is Billet is just the name for a chunk of metal. In the automotive world it's often used as a short hand for parts that were machined out of a big block of material, as opposed to cheaper castings where molten metal is poured into a mold pressed out of sheet metal. Machining is more expensive for large quantities but it can use better quality of metals and is easier to make for smaller quantities like race parts.\n",
      "Sample text length in characters: 594\n",
      "Sample text length in tokens: 127\n"
     ]
    }
   ],
   "source": [
    "# test of tokenizer on reddit_instruct\n",
    "sample_text = reddit_instruct[0]['post_title'] + \" \" + reddit_instruct[0]['post_text'] + \" \" + reddit_instruct[0]['comment_text']\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(tokens)\n",
    "print(\"Decoded text:\")\n",
    "print(tokenizer.decode(tokens)) \n",
    "print(f\"Sample text length in characters: {len(sample_text)}\")\n",
    "print(f\"Sample text length in tokens: {len(tokens)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting datasets functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 18195611\n",
      "Finetune dataset size: 257745\n",
      "Example entry from train dataset:\n",
      "{'id': '<urn:uuid:b7562f79-9e71-4c40-890c-8a24f0133759>', 'url': 'http://unitedexplanations.org/english/2016/03/08/migrant-working-women-the-main-victims-of-job-insecurity/', 'title': None, 'text': 'Originally published in Spanish here.\\nIn Spain, a high percentage of female migrant workers face precarious situations of informal labor and, often, undergo the consequences of poorly or unregulated domestic work. This situation exacerbates gender inequalities, not only for native female workers who have yet to see a real redistribution of domestic tasks, but also for the female migrant workers, who become increasingly vulnerable because of the lack of efficient migration policies including an integral gender approach.\\nMigrant women globally: Triple discrimination through gender, nationality and social class\\nAccording to recent United Nations data, women represent approximately half of the 200 million people living and working outside their country of birth, i.e. roughly 2% of the world population. The same data highlights that 93% of international migrants leave their places of birth mainly in order to improve their chances of living a decent life. In Europe, more than 52% of migrants are women, and most of them are economic migrants, meaning they come to the North in search of a job.\\nIt is no longer rare for migrant women to leave their countries alone – leaving behind dependent family members, for economic reasons rather than in order to reunite with family members. However, whether migration empowers these women, improves their families’ well-being, and ultimately boosts their native countries’ economic and social development, depends mainly on the policy and institutional responses offered to these workers. Facing triple discrimination, in their gender, nationality and social class, they are among the most vulnerable groups in society.\\nThe term “feminization” of migration\\nEven though international migration has increased considerably over the past decades, the weight of women’s migration participation would not be so relevant if it were not explained and influenced by gender relationships. As sociologist Denise Paiewonsky underlines:\\n“Although in some regions there has been a net feminization of migration, what has truly changed over the last few years is the fact that women increasingly migrate independently, looking for a job, rather than for other reasons, such as dependence on their husbands”.\\nIn Spain, as in many other northern countries, the feminization of migration is directly related to increased participation of migrant women in the labor market. This is due to changes in the destination countries and the so-called “care-crisis”:\\n“Meanwhile, labor demand is still increasing, given an ageing population, the high rate of women’s participation in the labor market and the receding role of the welfare state in the northern countries. Migrant women from the south replace independent northern women in caregiving tasks”.\\nThese female migrant workers end up in the labor niches that are the least appreciated and badly paid. They often suffer – especially in Spain, where household service is generally informal and poorly regulated – from severe isolation, exploitation, and lack of access to the social benefits every worker has a right to by international law.\\nGlobal care chains\\nThe concept of care is understood as “a set of tasks and personal provisions that seeks to enhance people’s wellbeing and is therefore related to maintaining life.” Global care chains are “a set of links through which care flows, in which the woman who migrates and provides care in the country of destination becomes the first link in a chain.”\\nBecause domestic and reproductive work happens at home and is excluded from the formal market given its perceived lack of economic value, and because governmental authorities often marginalize caregiving tasks– relegating them to the private sphere, the gray market takes hold. In this context, social inequalities generate the ideal conditions for these poorly paid and socially unappreciated jobs, which are taken on by migrant women from countries with limited labor opportunities. These circumstances create the so-called care chains:\\n“Care is transferred from one set of households to others, thereby deepening inequalities between families from the North and the South and strengthening the sexist social pact of unequal care and household responsibilities between men and women.”\\nIn other words, household service liberates the northern working lower and middle class woman from caregiving tasks and from the effects of the double day’s labor, but it also strengthens patriarchal structures by blocking the redefinition of family roles: the domestic worker substitutes the woman but the man does not take on an additional share of the caregiving and reproductive tasks.\\nThe Spanish case\\nAccording to a recent study, the labor situation of migrant women in Spain is characterized by precarious conditions in terms of type of occupational field, over-qualification, instability, etc. But above all, these precarious conditions are shaped by their nationality.\\nEven though female migrants in Spain have hugely diverse life stories, almost half of employed migrant women in 2011 held unskilled jobs in hotels, retail or household service; were more likely to hold temporary employment contracts; and faced working hours which were incompatible with other facets of life.\\nThis study, based on the 2011 Spanish Labor Force Survey (LFS), concludes that the origin of the female migrant worker, particularly if she is from Central or South America, constitutes the most relevant variable in determining the labor situation of these workers – more so than age, education level or family burdens. Here, global care chains come into play:\\n“The demand for employment in certain sectors which is not covered by the local Spanish population creates an occupation niche for foreigners from countries with poor labor expectations. The singular sectorial concentration leaves these women cornered in domestic and caregiving services”.\\nIn fact, as the last UN Women report on Global Care Chains in Spain reported:\\n“The regulation of household service, the Disability law, equality policies and immigration are key, since they limit the access of female migrants to gender equality and constitute one of the main causes of the current unequal organization of caregiving.”\\nGender inequality: A social priority\\nAlthough the growth of the female migrant labor force in Spain could be confused with an improved distribution of the burden of domestic tasks among local women, and although employment (albeit precarious) could have an emancipatory effect on female migrant workers, the reality is that women are still the chief caregivers. Ignoring this fact not only denies the social reality of gender inequality in the current labor market in Spain, it also overlooks the social inequalities faced by one of the most vulnerable groups – migrant workers.\\nRaising awareness about the social importance of caregiving is vital. The fight for an equal redistribution of domestic tasks and the improvement of work-life balance policies, must continue. None of this can be achieved without moving to end migrant discrimination and without proactively encouraging the labor mobility of these women to other labor sectors. This will require the gender dimension to be clearly included in immigration policies.\\n For instance, migrant women are more vulnerable to labor discrimination, harassment and violence and are not considered possible beneficiaries of the Law on Equality if their situation is considered irregular.\\nThis is a non-profit explanation', 'story': None, 'topic': None, 'theme': None, 'style': None, 'feature': None, 'grammar': None, 'persona': None, 'initial_word_type': None, 'initial_letter': None, 'word_count': None, 'character_count': None, 'num_paragraphs': None, 'avg_word_length': None, 'avg_sentence_length': None, 'flesch_reading_ease': None, 'flesch_kincaid_grade': None, 'dale_chall_readability_score': None, 'num_stories_in_completion': None, 'expected_num_stories_in_completion': None, 'generation_id': None, 'model': None, 'dump': 'CC-MAIN-2017-30', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2017-30/segments/1500549425751.38/warc/CC-MAIN-20170726022311-20170726042311-00153.warc.gz', 'language': 'en', 'language_score': 0.9562097787857056, 'token_count': 1410, 'score': 3.046875, 'int_score': 3}\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_train_dataset = concatenate_datasets([  \n",
    "    wiki_en,\n",
    "    stories,\n",
    "    fineweb_edu,\n",
    "])  \n",
    "\n",
    "combined_finetune_dataset = concatenate_datasets([\n",
    "    q_a1,\n",
    "    reddit_instruct,\n",
    "    alpaca,\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_dataset = combined_train_dataset.shuffle(seed=42)\n",
    "finetune_dataset = combined_finetune_dataset.shuffle(seed=42)\n",
    "print(f\"Train dataset size: {len(combined_train_dataset)}\")\n",
    "print(f\"Finetune dataset size: {len(combined_finetune_dataset)}\")\n",
    "\n",
    "# Exemple \n",
    "\n",
    "print(\"Example entry from train dataset:\")\n",
    "index = random.randint(0, len(train_dataset)-1)\n",
    "print(train_dataset[index])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the dataloader from the \"LLMs from scratch\" repository. But adapted for multi-row text arrays.\n",
    "\n",
    "https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/dataloader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mGPTDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset, tokenizer, max_length=\u001b[32m512\u001b[39m):\n\u001b[32m      3\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m        Args:\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m            dataset: Dataset of the combined hugginface entries\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m            tokenizer: The tokenizer to process text\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m            max_length: Context window size\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Dataset of the combined hugginface entries\n",
    "            tokenizer: the initiatokenizer to process text\n",
    "            max_length: Context window size\n",
    "        \"\"\"\n",
    "        self.data = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.input_tokens = []\n",
    "        self.target_tokens = []\n",
    "\n",
    "        self.pad_token_id = 0         # <|pad|>\n",
    "        self.bos_token_id = 100264    # <|im_start|>\n",
    "        self.eos_token_id = 100265    # <|im_end|>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Raw text\n",
    "        \n",
    "        # Data format handling\n",
    "        entry = self.data[idx]\n",
    "        if 'text' in entry:\n",
    "            text = entry['text']\n",
    "        elif 'story' in entry:\n",
    "            text = entry['story']\n",
    "        elif 'question' in entry and 'answer' in entry: \n",
    "            text = \"User: \" + entry['question'] + \" Assistant:\" + entry['answer']\n",
    "        elif 'post_title' in entry and 'post_text' in entry and 'comment_text' in entry:\n",
    "            text = \"User: \" + entry['post_title'] + \" Assistant:\" + entry['post_text'] + \" \" + entry['comment_text']\n",
    "        elif 'instruction' in entry and 'output' in entry:\n",
    "            text = \"User: \" + entry['instruction'] + \" Assistant:\" + entry['output']\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data entry format\")\n",
    "        \n",
    "\n",
    "        # Adding Start and End tokens\n",
    "        text = \"<|im_start|>\" + text + \"<|im_end|>\" \n",
    "\n",
    "        # Tokenization\n",
    "        tokens = self.tokenizer.encode(text, allowed_special=\"all\")\n",
    "\n",
    "        # Truncation and padding\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "            tokens[-1] = self.eos_token_id  # ensure last token is eos\n",
    "\n",
    "        input_ids = torch.tensor(tokens, dtype=torch.long)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "\n",
    "        #Padding \n",
    "        padding_length = self.max_length - input_ids.size(0)\n",
    "        if padding_length > 0:\n",
    "            # Create padding tensors\n",
    "            pad_ids = torch.full((padding_length,), self.pad_token_id, dtype=torch.long)\n",
    "            pad_mask = torch.zeros((padding_length,), dtype=torch.long) # 0 = ignore\n",
    "            \n",
    "            # Concatenate\n",
    "            input_ids = torch.cat([input_ids, pad_ids])\n",
    "            attention_mask = torch.cat([attention_mask, pad_mask])\n",
    "\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        #adding the ignore index for padding tokens\n",
    "        if padding_length > 0:\n",
    "            # We know the padding is at the end\n",
    "            labels[-padding_length:] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_dataset = \u001b[43mTextDataset\u001b[49m(combined_hf_dataset, tokenizer, max_length=\u001b[32m512\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m batch_size = \u001b[32m16\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'TextDataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(combined_hf_dataset, tokenizer, max_length=512)\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 512,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources \n",
    "\n",
    "### Principal references: \n",
    "- https://arxiv.org/abs/2005.14165 (GPT-3 paper)\n",
    "- https://arxiv.org/abs/2002.05709 (Attention is all you need paper)\n",
    "- Build a Large Language Model (from scratch) by Sebastian Raschka\n",
    "\n",
    "### About Padding tokens in Language Modeling\n",
    "- https://arxiv.org/html/2510.01238v1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
