{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet GPT - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used to train a small language model using PyTorch from scratch. The model is inspired by the GPT architecture.\n",
    "\n",
    "\n",
    "#### Hardware\n",
    "- RTX3060 12GB VRAM\n",
    "- AMD Ryzen 7 5800X 8-Core\n",
    "- 32GB RAM\n",
    "- Ubuntu 22.04 LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CACHE_DIR = \"/media/rob/RobsDisk/cache_data_llm\"\n",
    "os.environ['HF_HOME'] = CACHE_DIR\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_DIR, \"datasets\")\n",
    "os.environ['HF_METRICS_CACHE'] = os.path.join(CACHE_DIR, \"metrics\")\n",
    "os.environ['HF_MODULES_CACHE'] = os.path.join(CACHE_DIR, \"modules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import random, math\n",
    "\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Common knowledge datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### English Wikipedia crawled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Wikipedia dataset loaded.\n",
      "dataset size in gb: 18.812774107791483\n",
      "Number of entries: 6407814\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "\"Dope Boys\" is the second single from The Game's third studio album, LAX. The song features Travis Barker playing the drums. The song samples \"Eleanor Rigby\" By The Beatles. The song was slated to appear in the 2008 driving game Midnight Club: Los Angeles but was omitted from The Game for unknown reasons.\n",
      "\n",
      "Music video\n",
      "\n",
      "The official video was directed and edited by Matt Alonzo and features Travis Barker, Omar Cruz, Black Wall Street, Mr. Capone-E and BYI members. The video is interspersed with shots of The Game standing on a Los Angeles rooftop with the skyline behind him. In other shots he is seen to be wearing Beats by Dr. Dre headphones.\n",
      "\n",
      "Covers and remixes \n",
      "The song has been freestyled by rappers Bow Wow, Rick Ross, Wiz Khalifa, and Dynasty of L.$.G. Royce da 5'9\" also has a freestyle called Flow Boy on his Bar Exam 2 mixtape. There is also an unofficial remix featuring Trae on his mixtape \"Streets Advocate\" and Young Buck featured this song (non-album) in April 2010.\n",
      "\n",
      "Chart position\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      "2008 singles\n",
      "The Game (rapper) songs\n",
      "Travis Barker songs\n",
      "Song recordings produced by DJ Quik\n",
      "Songs written by The Game (rapper)\n",
      "Songs written by Travis Barker\n",
      "2008 songs\n",
      "Geffen Records singles\n",
      "Rap rock songs\n"
     ]
    }
   ],
   "source": [
    "# English Wikipedia crawled dataset\n",
    "# path to store the dataset cache: /Volumes/RobertsDisk\n",
    "wiki_en = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split='train', cache_dir=CACHE_DIR) \n",
    "print(\"English Wikipedia dataset loaded.\")\n",
    "print(\"dataset size in gb:\", wiki_en.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(wiki_en))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(wiki_en[random.randint(0, len(wiki_en)-1)]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple stories dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple stories dataset loaded.\n",
      "dataset size in mb: 3030.012650489807\n",
      "Number of entries: 2115696\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Inexplicably, she felt the cold wind on her face. She had been waiting by the old oak tree, just like they used to. It had been years since he left. The memories came rushing back, but they were bitter. She should have kept in touch, but she had let time slip away. Now, she was alone, holding onto fading dreams.\n",
      "\n",
      "Before he went away, she had promised to write. But days turned to weeks, and weeks turned to months. \"I will send a letter soon,\" she had thought. Now, with a heavy heart, she realized he had moved on. The weight of her choices made her feel invisible, a shadow of her former self, lost in a sea of what-ifs.\n"
     ]
    }
   ],
   "source": [
    "# Simple stories dataset\n",
    "stories = load_dataset(\"SimpleStories/SimpleStories\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Simple stories dataset loaded.\")\n",
    "print(\"dataset size in mb:\", stories.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(stories))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(stories[random.randint(0, len(stories)-1)]['story'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FineWeb-Edu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineWeb-Edu is ready.\n",
      "dataset size in gb: 45.730818568728864\n",
      "Number of entries: 9672101\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "- Aug 06, 2007 4:40 PM EST\n",
      "- [num] Comments\n",
      "University of Delaware researchers just achieved an all-time record of solar cell efficiency, with an efficiency of 42.8%. (The previous record was 40.7%, set by Boeing in 2006; the U of D team is working towards 50%.) The 42.8% is notable not only for the relatively high efficiency, but also for its practicality: the device was developed in less than two years, is only one centimeter thick, and has no moving parts. (In contrast, the machine that set the 40.7% record was a foot thick--hardly portable!)\n",
      "The device is called the VHESC (Very High Efficiency Solar Cell), and its creators are hoping to see it used commercially. First, though, they're looking at military implementation: the U.S. soldier apparently carries 20 pounds of batteries in a standard pack.\n",
      "For comparison, ecotality blog points out that usual solar cells--the rooftop variety--peak out at about 17%.\n",
      "- Solar Energy\n"
     ]
    }
   ],
   "source": [
    "fineweb_edu = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\",  split='train', cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"FineWeb-Edu is ready.\")\n",
    "print(\"dataset size in gb:\", fineweb_edu.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(fineweb_edu))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(fineweb_edu[random.randint(0, len(fineweb_edu)-1)]['text']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenWebText2 dataset loaded.\n",
      "Dataset size in GB: 37.03822539001703\n",
      "Number of entries: 8013769\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Screen grab from Channel 4’s Sri Lanka’s Killing Fields\n",
      "\n",
      "(New York) – The Sri Lankan government continues its diplomatic offensive, denying and dismissing the growing evidence of war crimes during the final bloody battles between the Sri Lankan armed forces and the separatist Liberation Tigers of Tamil Eelam (LTTE) that ended in May 2009.\n",
      "\n",
      "Last week, at a panel presentation of the Channel 4 film, the ‘Killing Fields of Sri Lanka’, Sri Lanka’s United Nations Ambassador Palitha Kohona said, “To suggest that the Sri Lankan military was so foolhardy as to deliberately target the civilians, I think is a blatant lie… We had no intention of creating martyrs, we had no intention of creating more volunteers for the LTTE.”\n",
      "\n",
      "If the killings of civilians were not deliberate, the Sri Lankan army attacks were clearly indiscriminate, which is no less a war crime. The recent findings of the panel of experts set up to advise UN Secretary General Ban Ki-moon concluded that up to 40,000 civilians were killed in the final stages of the conflict, many as the result of indiscriminate shelling by government forces. The report also concluded that both government forces and the Tigers conducted military operations “with flagrant disregard for the protection, rights, welfare and lives of civilians and failed to respect the norms of international law.”\n",
      "\n",
      "The Channel 4 film adds even more weight to the UN report, providing devastating and graphic footage of possible war crimes by Sri Lankan soldiers. It shows summary executions of prisoners by soldiers in uniform, half-naked corpses of women that raise questions about sexual abuse and includes revealing interviews with ethnic Tamils who described indiscriminate shelling that killed many civilians.\n",
      "\n",
      "It is true that the LTTE committed horrific abuses against the civilian population by using them as human shields, forcibly conscripting children, and deploying artillery close to civilians. Human Rights Watch documented abuses by the LTTE for years.\n",
      "\n",
      "It is also true that in the final stages of the war it was difficult to verify facts and corroborate evidence, especially when the government deliberately shut out foreign media, the United Nations, and humanitarian and human rights groups from the battle zone.\n",
      "\n",
      "But it is wrong for the Sri Lankan government to dismiss this compelling footage as “fake.” The most vehement dismissals have been directed against a clip of several executions of naked, bound and blindfolded men by men in military uniforms. But the executions footage has been authenticated by four independent experts who have no connection to Sri Lanka.\n",
      "\n",
      "Sri Lanka’s own examination of the video, by contrast, carries no credibility. All the experts commissioned by the government were either Sri Lankan military experts or Sri Lankan nationals living abroad. All of the government’s objections had been addressed by the UN-commissioned experts in their recent reports.\n",
      "\n",
      "Since the screening of the film for diplomats in Geneva and New York, Sri Lankan government officials have said that they will investigate any credible allegations of wrongdoing. But it is hard to imagine more concrete evidence of war crimes than this execution video. In addition to the video, several photographs of the same bodies, all publicly available, add information that should allow the government to find those responsible for these crimes if it wants to.\n",
      "\n",
      "First, we know the identity of one of the victims. Human Rights Watch interviewed several people who identified one of the female victims as Isaipiriya, a LTTE media worker.\n",
      "\n",
      "Second, there are strong indications that the incident took place on the evening of May 18– the final night of the war. The army said on the Defense Ministry’s website that Isaipiriya was killed on May 18; this information is consistent with meta-data on many of the photographs.\n",
      "\n",
      "Third, regarding the identity of the perpetrators, the army itself said that its 53rd division killed Isaipiriya. Yet, the government has not taken even the most rudimentary steps for a criminal investigation, such as questioning those in the 53rd division who reported Isaipiriya’s killing.\n",
      "\n",
      "After initially dismissing the footage as “fake” and “made with LTTE money,” the government is now, perhaps with the growing body of evidence, asserting that it is waiting for its domestic truth mechanism, the Lessons Learnt and Reconciliation Commission, to conclude its investigations. The government now asserts that the Commission will look into some of these incidents if it deems them credible.\n",
      "\n",
      "This is nothing but a delaying tactic. As noted by the UN panel of experts, the LLRC is not an accountability mechanism, is “deeply flawed” and its mandate is “not tailored to investigating allegations of serious violations of international humanitarian law.”\n",
      "\n",
      "Only a prompt, thorough and objective criminal investigation will fulfill the government’s obligations under international law. The government’s failure to open such an investigation, almost two years after the footage surfaced, is a clear indication that the government has no intention to meet its international obligations.\n",
      "\n",
      "The government’s shallow show-and-tell exercise with the LLRC reflects a long history in Sri Lanka of setting up government commissions in response to serious allegations of abuses such as enforced disappearances and extrajudicial killings that ultimately fail to adequately investigate or to hold those responsible to account.\n",
      "\n",
      "Since 1977, Sri Lanka has set up at least 15 commissions in response to international criticism of its human rights record. The work of many of these commissions has been tainted with political interference and mainly served to exonerate the government security forces. But even in cases in which the commissions conducted thorough investigations, established numerous cases of abuses, and identified the perpetrators, the authorities failed to act on the commissions’ recommendations or to establish a meaningful accountability process.\n",
      "\n",
      "Justice for Sri Lanka’s war victims is crucial. If Ambassador Kohona really wanted to avoid making martyrs of the LTTE, then he would support an independent international investigation into the allegations of war crimes. Based on the government’s track record, anything it produces is likely to be just another whitewash. What is really needed is to establish a full international investigation of the executions in the video and other numerous credible allegations of war crimes.\n",
      "\n",
      "Elaine Pearson is the deputy Asia director at Human Rights Watch.\n"
     ]
    }
   ],
   "source": [
    "# OpenWebText2 dataset\n",
    "owt2 = load_dataset(\"Skylion007/openwebtext\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "print(\"OpenWebText2 dataset loaded.\")\n",
    "print(\"Dataset size in GB:\", owt2.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(owt2))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "print(owt2[random.randint(0, len(owt2)-1)]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Q&A data to improve the model's ability to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A dataset loaded.\n",
      "dataset size in mb: 46.480509757995605\n",
      "Number of entries: 120959\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "How do you handle CRUD operations involving drools_package and drools_rule entities in Java application? \n",
      " CRUD operations involving drools_package and drools_rule entities in Java applications are performed using Spring Data JPA, along with standard interfaces JpaRepository for mapping different entities. Custom methods can be implemented for 'drools_package' and 'drools_rule' tables to handle the CRUD operations.\n"
     ]
    }
   ],
   "source": [
    "q_a1 = load_dataset(\"agentlans/text-sft-questions-answers-only\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Q&A dataset loaded.\")\n",
    "print(\"dataset size in mb:\", q_a1.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(q_a1))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(q_a1)-1)\n",
    "print(q_a1[index]['question'][:500], \"\\n\", q_a1[index]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit Instruct dataset loaded.\n",
      "dataset size in gb: 0.09901080373674631\n",
      "Number of entries: 84784\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Is there a limit to how high a person can fly a kite? As the string gets longer, the weight of the kite increases. But also there tends to be more wind higher up. A larger kite would lift more string, but at some point a single person wouldn't be able to hold a very large kite in high winds. So, is there a limit to how high a person can fly a kite?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " '\\n',\n",
       " \"This is really a question about the limits of materials science. In some ways it's similar to the problem posed by a theoretical space elevator. How thin and light can you get the string without it breaking? How high can you get the surface area of the kite while keeping the weight down?\")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#euclaise/reddit-instruct\n",
    "reddit_instruct = load_dataset(\"euclaise/reddit-instruct\", split='train', cache_dir=CACHE_DIR)\n",
    "# reddit_instruct = load_dataset(\"Felladrin/ChatML-reddit-instruct-curated\", split='train', cache_dir=CACHE_DIR)\n",
    "print(\"Reddit Instruct dataset loaded.\")\n",
    "print(\"dataset size in gb:\", reddit_instruct.dataset_size / (1024**3))\n",
    "print(\"Number of entries:\", len(reddit_instruct))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(reddit_instruct)-1)\n",
    "print(reddit_instruct[index]['post_title'][:500], reddit_instruct[index]['post_text'][:500]), \"\\n\", reddit_instruct[index]['comment_text'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca dataset loaded.\n",
      "dataset size in mb: 44.06797695159912\n",
      "Number of entries: 52002\n",
      "--------------------------------------------------\n",
      "Example entry:\n",
      "Reword the following sentence, preserving the same structure and meaning: \n",
      " I am highly displeased with your behavior.\n"
     ]
    }
   ],
   "source": [
    "# tatsu-lab/alpaca ( for Q&A fine-tuning )\n",
    "alpaca = load_dataset(\"tatsu-lab/alpaca\", split='train')\n",
    "print(\"Alpaca dataset loaded.\")\n",
    "print(\"dataset size in mb:\", alpaca.dataset_size / (1024**2))\n",
    "print(\"Number of entries:\", len(alpaca))\n",
    "print(\"-\"*50)\n",
    "print(\"Example entry:\")\n",
    "index = random.randint(0, len(alpaca)-1)\n",
    "print(alpaca[index]['instruction'][:500], \"\\n\", alpaca[index]['output'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project i use tiktoken for the tokenizer, as it is the same tokenizer used by OpenAI for their models.\n",
    "\n",
    "I use the \"gpt2\" encoding which is a byte pair encoding (BPE) tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"rob-tokenizer\",\n",
    "    pat_str=tokenizer_base._pat_str,\n",
    "    mergeable_ranks=tokenizer_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **tokenizer_base._special_tokens,\n",
    "        \"<|im_start|>\": 50257,\n",
    "        \"<|im_end|>\": 50258,\n",
    "        \"<|pad|>\": 50259,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the byte pair encoding tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2061, 318, 24207, 1616, 2587, 30, 314, 2342, 257, 7684, 286, 1097, 5861, 290, 484, 1561, 546, 275, 32512, 7021, 290, 884, 11, 1312, 373, 11263, 644, 275, 32512, 318, 290, 1312, 18548, 1064, 597, 2562, 7468, 284, 644, 340, 318, 24207, 1616, 318, 655, 262, 1438, 329, 257, 16058, 286, 6147, 13, 554, 262, 29393, 995, 340, 338, 1690, 973, 355, 257, 1790, 1021, 329, 3354, 326, 547, 3235, 1389, 503, 286, 257, 1263, 2512, 286, 2587, 11, 355, 6886, 284, 11721, 3350, 654, 810, 44030, 6147, 318, 19036, 656, 257, 15936, 12070, 503, 286, 9629, 6147, 13, 7080, 3191, 318, 517, 5789, 329, 1588, 17794, 475, 340, 460, 779, 1365, 3081, 286, 21782, 290, 318, 4577, 284, 787, 329, 4833, 17794, 588, 3234, 3354, 13]\n",
      "Decoded text:\n",
      "What is Billet material? I watch a bunch of car videos and they talk about billet blocks and such, i was wondering what billet is and i cant find any easy explanation to what it is Billet is just the name for a chunk of metal. In the automotive world it's often used as a short hand for parts that were machined out of a big block of material, as opposed to cheaper castings where molten metal is poured into a mold pressed out of sheet metal. Machining is more expensive for large quantities but it can use better quality of metals and is easier to make for smaller quantities like race parts.\n",
      "Sample text length in characters: 594\n",
      "Sample text length in tokens: 127\n"
     ]
    }
   ],
   "source": [
    "# test of tokenizer on reddit_instruct\n",
    "sample_text = reddit_instruct[0]['post_title'] + \" \" + reddit_instruct[0]['post_text'] + \" \" + reddit_instruct[0]['comment_text']\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(tokens)\n",
    "print(\"Decoded text:\")\n",
    "print(tokenizer.decode(tokens)) \n",
    "print(f\"Sample text length in characters: {len(sample_text)}\")\n",
    "print(f\"Sample text length in tokens: {len(tokens)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting datasets functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 26209380\n",
      "Finetune dataset size: 257745\n",
      "Example entry from train dataset:\n",
      "{'id': '<urn:uuid:a1f4e114-7644-436a-b30d-f60882448cbc>', 'url': 'http://cryptidz.wikia.com/wiki/Giant_Catfish', 'title': None, 'text': 'Note: While it is quite obvious this animal already exists. This page identifies possibly larger specimens.\\nGiant catfish are catfish that grow record-breakingly large. Specimens have been reported all throughout the world. Sometimes they are reported devouring humans. Specimens have been reported 6-14 feet in length and up to 1,000 pounds in weight.\\n- There are many stories and reports of divers going to the bottom of dams and being scared out of the water by large Catfish. Here\\'s one that was collected on this Wiki. \"In Oklahoma a team of divers went down to the bottom of a dam. When they came back up they said it was dangerous to go back down. They said there was a catfish the size of a greyhound bus! Yet nobody knows for sure.\" how do we know they were telling the truth\\n- The current world record for largest catfish is a 646 pound, nine foot long Mekong Giant Catfish.\\n- The two largest catfish in the United States are the Flathead Catfish and the Blue Catfish.\\n- The Goonch Catfish (Bagarius yarelli) in India, Nepal, and Bangladesh has been reported eating humans.\\n- Another giant Indian catfish is the Sareng (Wallago attu). It has also been reported devouring small children.\\n- One particular catfish (the wels) has been reported devouring humans since medieval times.\\n- Noodling is a popular sport. When you noodle you are basically sticking your hand in a hole and waiting for a catfish to bite it then you pull it out. It takes lots of courage to noodle and you need to be in good condition. People who have did it say it hurts like crazy.\\n- There are three South American catfish that can grow alarming sizes, the Red-Tailed Catfish, the Jau Catfish, and the largest of them all, the mighty Pirahiba.\\n- The Vundu catfish of Africa can also grow very large and have been reported to drag people who try to catch them into the water.', 'story': None, 'topic': None, 'theme': None, 'style': None, 'feature': None, 'grammar': None, 'persona': None, 'initial_word_type': None, 'initial_letter': None, 'word_count': None, 'character_count': None, 'num_paragraphs': None, 'avg_word_length': None, 'avg_sentence_length': None, 'flesch_reading_ease': None, 'flesch_kincaid_grade': None, 'dale_chall_readability_score': None, 'num_stories_in_completion': None, 'expected_num_stories_in_completion': None, 'generation_id': None, 'model': None, 'dump': 'CC-MAIN-2016-36', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2016-36/segments/1471982957972.74/warc/CC-MAIN-20160823200917-00134-ip-10-153-172-175.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.9695829153060913, 'token_count': 438, 'score': 2.546875, 'int_score': 3}\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_train_dataset = concatenate_datasets([  \n",
    "    wiki_en,\n",
    "    stories,\n",
    "    fineweb_edu,\n",
    "    owt2,  \n",
    "])  \n",
    "\n",
    "combined_finetune_dataset = concatenate_datasets([\n",
    "    q_a1,\n",
    "    reddit_instruct,\n",
    "    alpaca,\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_dataset = combined_train_dataset.shuffle(seed=42)\n",
    "finetune_dataset = combined_finetune_dataset.shuffle(seed=42)\n",
    "print(f\"Train dataset size: {len(combined_train_dataset)}\")\n",
    "print(f\"Finetune dataset size: {len(combined_finetune_dataset)}\")\n",
    "\n",
    "# Exemple \n",
    "\n",
    "print(\"Example entry from train dataset:\")\n",
    "index = random.randint(0, len(train_dataset)-1)\n",
    "print(train_dataset[index])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the dataloader from the \"LLMs from scratch\" repository. But adapted for multi-row text arrays.\n",
    "\n",
    "https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/dataloader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Dataset of the combined hugginface entries\n",
    "            tokenizer: the initiatokenizer to process text\n",
    "            max_length: Context window size\n",
    "        \"\"\"\n",
    "        self.data = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.input_tokens = []\n",
    "        self.target_tokens = []\n",
    "\n",
    "        self.pad_token_id = 50259         # <|pad|>\n",
    "        self.bos_token_id = 50257    # <|im_start|>\n",
    "        self.eos_token_id = 50258    # <|im_end|>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Raw text\n",
    "        \n",
    "        # Data format handling\n",
    "        entry = self.data[idx]\n",
    "        if 'text' in entry:\n",
    "            text = entry['text']\n",
    "        elif 'story' in entry:\n",
    "            text = entry['story']\n",
    "        elif 'question' in entry and 'answer' in entry: \n",
    "            text = \"User: \" + entry['question'] + \" Assistant:\" + entry['answer']\n",
    "        elif 'post_title' in entry and 'post_text' in entry and 'comment_text' in entry:\n",
    "            text = \"User: \" + entry['post_title'] + \" Assistant:\" + entry['post_text'] + \" \" + entry['comment_text']\n",
    "        elif 'instruction' in entry and 'output' in entry:\n",
    "            text = \"User: \" + entry['instruction'] + \" Assistant:\" + entry['output']\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data entry format\")\n",
    "        \n",
    "        text = str(text) # Ensure text is a string\n",
    "        #print(text)\n",
    "\n",
    "        # Adding Start and End tokens\n",
    "        text = \"<|im_start|>\" + text + \"<|im_end|>\" \n",
    "\n",
    "        # Tokenization\n",
    "        tokens = self.tokenizer.encode(text, allowed_special=\"all\")\n",
    "\n",
    "        # Truncation\n",
    "        tokens = tokens[:self.max_length] #Data is loost here ( fix later with sliding window )\n",
    "\n",
    "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)  # All tokens except last\n",
    "        labels = torch.tensor(tokens[1:], dtype=torch.long)      # All tokens except first\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        attention_mask = (input_ids != self.pad_token_id).long() # 1 for real tokens, 0 for padding\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTstreamingDataset(IterableDataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Dataset of the combined hugginface entries\n",
    "            tokenizer: the initiatokenizer to process text\n",
    "            max_length: Context window size\n",
    "        \"\"\"\n",
    "        self.data = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.bos_token_id = 50257    # <|im_start|>\n",
    "        self.eos_token_id = 50258    # <|im_end|>\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        for entry in self.data:\n",
    "            # Data format \n",
    "            if entry.get('text') is not None:\n",
    "                text = entry['text']\n",
    "            elif entry.get('story') is not None:\n",
    "                text = entry['story']\n",
    "            elif entry.get('question') is not None and entry.get('answer') is not None: \n",
    "                text = f\"User: {entry['question']} Assistant: {entry['answer']}\"\n",
    "            elif entry.get('post_title') is not None:\n",
    "                title = entry.get('post_title', \"\")\n",
    "                post = entry.get('post_text', \"\")\n",
    "                comment = entry.get('comment_text', \"\")\n",
    "                text = f\"User: {title} Assistant: {post} {comment}\"\n",
    "            elif entry.get('instruction') is not None and entry.get('output') is not None:\n",
    "                text = f\"User: {entry['instruction']} Assistant: {entry['output']}\"\n",
    "            if text is None:\n",
    "                continue\n",
    "                \n",
    "            text = str(text) \n",
    "\n",
    "            # Start and End tokens\n",
    "            text = \"<|im_start|>\" + text + \"<|im_end|>\" \n",
    "\n",
    "            # Tokenization\n",
    "            tokens = self.tokenizer.encode(text, allowed_special=\"all\")\n",
    "\n",
    "            buffer.extend(tokens) #pile tokens into buffer\n",
    "\n",
    "\n",
    "            while len(buffer) >= self.max_length +1:\n",
    "                chunk = buffer[:self.max_length + 1]\n",
    "                buffer = buffer[self.max_length+ 1:] # remove used tokens\n",
    "\n",
    "                input_ids = torch.tensor(chunk[:-1], dtype=torch.long)  \n",
    "                labels = torch.tensor(chunk[1:], dtype=torch.long)      \n",
    "\n",
    "                yield {\n",
    "                    'input_ids': input_ids,\n",
    "                    'labels': labels\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the configuration for the GPT model i am going to train. It is a smaller version of the GPT-2 model. \n",
    "\n",
    "- Context length: 512 tokens\n",
    "- Embedding dimension: 512\n",
    "- Number of attention heads: 8\n",
    "- Number of layers: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50260,\n",
    "    \"context_length\": 256, # max i could fit on my gpu\n",
    "    \"emb_dim\": 384,\n",
    "    \"number_heads\": 6,\n",
    "    \"number_layers\": 6,\n",
    "    \"drop_rate\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test of a entry from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cuda cache and memory management\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_dataset = GPTstreamingDataset(train_dataset, tokenizer, GPT_CONFIG['context_length'])\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, pin_memory=True, prefetch_factor=2, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Test of a entry from dataloader\n",
      "{'input_ids': tensor([[50257,   818,   257,  ...,  4403,  5866, 10062],\n",
      "        [  290,  7103, 28717,  ..., 23312, 33408,   341],\n",
      "        [  262,  4403,  5866,  ..., 24337,  1080,   373],\n",
      "        ...,\n",
      "        [  897,   276,  4034,  ...,  7184,   290,   262],\n",
      "        [  286,  3292,    13,  ...,   991,  9389,  1917],\n",
      "        [ 4525,  4890,    11,  ...,   284,  2074, 48837]]), 'labels': tensor([[  818,   257,  6016,  ...,  5866, 10062,  2838],\n",
      "        [ 7103, 28717,  1989,  ..., 33408,   341,   287],\n",
      "        [ 4403,  5866, 10062,  ...,  1080,   373,   973],\n",
      "        ...,\n",
      "        [  276,  4034,    13,  ...,   290,   262,  5236],\n",
      "        [ 3292,    13,   198,  ...,  9389,  1917,    13],\n",
      "        [ 4890,    11,  8098,  ...,  2074, 48837, 14317]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Test of a entry from dataloader\")\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytroch model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first implementation, i am using the transformer and embedding modules from PyTorch. Later i will try to implement the attention mechanism from scratch for better understanding.\n",
    "\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Gpt model class using transformer library\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Network components \n",
    "        ## Embedding layers\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.positional_encoding = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        ## Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['emb_dim'],\n",
    "            nhead=config['number_heads'],\n",
    "            dim_feedforward=4 * config['emb_dim'],\n",
    "            dropout=config['drop_rate'],\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True # stabilityy \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config['number_layers'])\n",
    "        ## Output layer\n",
    "        self.output_layer = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.01)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.01)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label_ids=None):   \n",
    "        batch_size, seq_length = input_ids.shape\n",
    "\n",
    "        # Embedding\n",
    "        token_embeddings = self.embedding(input_ids)  \n",
    "        pos_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        position_embeddings = self.positional_encoding(pos_ids)  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        embeddings = token_embeddings + position_embeddings  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        # Prevent attending to future tokens\n",
    "        causal_mask = torch.triu(torch.full((seq_length, seq_length), float('-inf'), device=input_ids.device), diagonal=1)\n",
    "\n",
    "        # voiding to pay attatention padding tokens\n",
    "        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        x = self.transformer(embeddings, mask=causal_mask, src_key_padding_mask=key_padding_mask, is_causal=True)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        # Computing loss \n",
    "        if label_ids is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(logits.view(-1, self.config['vocab_size']), label_ids.view(-1)) # Applies loss to predictions\n",
    "            return logits, loss\n",
    "\n",
    "        return logits, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModelNoPad(nn.Module):\n",
    "    \"\"\"\n",
    "    Gpt model class using transformer library adapted for streaming dataset without padding mask\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Network components \n",
    "        ## Embedding layers\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.positional_encoding = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        ## Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['emb_dim'],\n",
    "            nhead=config['number_heads'],\n",
    "            dim_feedforward=4 * config['emb_dim'],\n",
    "            dropout=config['drop_rate'],\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True # stabilityy \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config['number_layers'])\n",
    "        ## Output layer\n",
    "        self.output_layer = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "        # Weight Tying ( input and output embeddings share weights)\n",
    "        self.output_layer.weight = self.embedding.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, label_ids=None):   \n",
    "        batch_size, seq_length = input_ids.shape\n",
    "\n",
    "        # Embedding\n",
    "        token_embeddings = self.embedding(input_ids)  \n",
    "        pos_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        position_embeddings = self.positional_encoding(pos_ids)  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        embeddings = token_embeddings + position_embeddings  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "        # Prevent attending to future tokens\n",
    "        causal_mask = torch.triu(torch.full((seq_length, seq_length), float('-inf'), device=input_ids.device), diagonal=1)\n",
    "        \n",
    "        x = self.transformer(embeddings, mask=causal_mask, is_causal=True)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        # Computing loss \n",
    "        if label_ids is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.config['vocab_size']), label_ids.view(-1)) # Applies loss to predictions\n",
    "            return logits, loss\n",
    "\n",
    "        return logits, None \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model instantiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModelNoPad(\n",
      "  (embedding): Embedding(50260, 384)\n",
      "  (positional_encoding): Embedding(256, 384)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=384, out_features=50260, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64315/2518131499.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Gradient scaler for mixed precision\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTModelNoPad(GPT_CONFIG).to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=10000) # Linear learning rate scheduler\n",
    "scaler = torch.cuda.amp.GradScaler()  # Gradient scaler for mixed precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of parameters calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 30044928\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)    \n",
    "\n",
    "count_params = count_parameters(model)  \n",
    "print(f\"Number of trainable parameters: {count_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, prompt, max_length=256, device='cpu'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_length)\n",
    "\n",
    "    generated_ids = input_ids\n",
    "    max_length = max_length - input_ids.shape[1]  # Remaining length for generation\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            #attention_mask = torch.ones_like(generated_ids)  # All tokens are real (no padding)\n",
    "            logits, _ = model(generated_ids) # model(generated_ids, attention_mask)\n",
    "            next_token_logits = logits[:, -1, :]  #next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # (1, 1) #\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            #print(next_token_logits)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)  # mult\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)  # Append to sequence\n",
    "\n",
    "            if next_token_id.item() == 50258:  # Stop if eot token is generated\n",
    "                break\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_ids.squeeze().tolist())\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps= 1  # Number of steps to accumulate gradients\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, scheduler, device, num_epochs=3, accumulation_steps = 4, question_interval=500, saving_interval=5000):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        step_count = 0\n",
    "\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            #attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):  # bf16 optimized for ampere archi\n",
    "                logits, loss = model(input_ids, labels) # model(input_ids, attention_mask, labels)\n",
    "                loss = loss / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()*accumulation_steps\n",
    "            progress_bar.set_postfix(loss=loss.item() * accumulation_steps)\n",
    "\n",
    "            # Inference check\n",
    "            step_count += 1\n",
    "            if step_count % question_interval == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    prompt = \"AI is technology that enables computers and machines to\"\n",
    "                    generated_text = inference(model, tokenizer, prompt, max_length=50, device=device)\n",
    "                    print(f\"\\n[Inference at step {step_count}]\")\n",
    "                    #save generated text to file train_ouput.txt\n",
    "                    with open(\"outputs/train_output.txt\", \"a\") as f:\n",
    "                        if generated_text.strip() != \"\":\n",
    "                            f.write(f\"\\n[Inference at step {step_count}]: {generated_text}\\n\")  \n",
    "                            f.write(\"-\"*50 + \"\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"\\n[Inference at step {step_count}]: [No output generated]\\n\")\n",
    "                            f.write(\"-\"*50 + \"\\n\")\n",
    "                model.train()\n",
    "\n",
    "            if step_count % saving_interval == 0:\n",
    "                # Save model checkpoint\n",
    "                date_hour = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                # Create models directory if it doesn't exist\n",
    "                Path(f\"models/{date_hour}\").mkdir(parents=True, exist_ok=True)\n",
    "                checkpoint_path = f\"models/{date_hour}/weights_step{step_count}.pt\"\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"\\nModel checkpoint saved at step {step_count} to {checkpoint_path}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First training on combined train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 499it [01:13,  6.76it/s, loss=6.87]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "softmax() received an invalid combination of arguments - got (Tensor, num_samples=int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype = None, *, Tensor out = None)\n * (Tensor input, name dim, *, torch.dtype dtype = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m torch.cuda.empty_cache()\n\u001b[32m      3\u001b[39m torch.cuda.synchronize()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(model, dataloader, optimizer, scheduler, device, num_epochs, accumulation_steps, question_interval, saving_interval)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     34\u001b[39m     prompt = \u001b[33m\"\u001b[39m\u001b[33mAI is technology that enables computers and machines to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     generated_text = \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[Inference at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m#save generated text to file train_ouput.txt\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36minference\u001b[39m\u001b[34m(model, tokenizer, prompt, max_length, device)\u001b[39m\n\u001b[32m     13\u001b[39m probs = torch.softmax(next_token_logits, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#print(next_token_logits)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m next_token_id = torch.multinomial(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# mult\u001b[39;00m\n\u001b[32m     16\u001b[39m generated_ids = torch.cat([generated_ids, next_token_id], dim=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Append to sequence\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_token_id.item() == \u001b[32m50258\u001b[39m:  \u001b[38;5;66;03m# Stop if eot token is generated\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: softmax() received an invalid combination of arguments - got (Tensor, num_samples=int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype = None, *, Tensor out = None)\n * (Tensor input, name dim, *, torch.dtype dtype = None)\n"
     ]
    }
   ],
   "source": [
    "#empty gpu memory \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "train_loop(model, train_dataloader, optimizer, scheduler, device, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources \n",
    "\n",
    "### Principal references: \n",
    "- https://arxiv.org/abs/2005.14165 (GPT-3 paper)\n",
    "- https://arxiv.org/abs/2002.05709 (Attention is all you need paper)\n",
    "- Build a Large Language Model (from scratch) by Sebastian Raschka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
